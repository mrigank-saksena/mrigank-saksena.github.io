<!DOCTYPE HTML>
<html lang="en-US">
    <head>
            
        <title>TWD - Thinking With Data</title>
        <meta https-equiv="Content-Type" content="text/html; charset=UTF-8">
        <meta name="description" content="Template by Colorlib" />
        <meta name="keywords" content="HTML, CSS, JavaScript, PHP" />
        <meta name="author" content="Colorlib" />
        <!--<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">-->
        <meta name="viewport" content="width=1024">
        <link rel="shortcut icon" href="images/favicon.png" />
        <link href='https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700,700i,900|Montserrat:400,700|PT+Serif' rel='stylesheet' type='text/css'>
        <link rel="stylesheet" type="text/css"  href='css/clear.css' />
        <link rel="stylesheet" type="text/css"  href='css/common.css' />
        <link rel="stylesheet" type="text/css"  href='css/font-awesome.min.css' />
        <link rel="stylesheet" type="text/css"  href='css/carouFredSel.css' />
        <link rel="stylesheet" type="text/css"  href='css/prettyPhoto.css' />
        <link rel="stylesheet" type="text/css"  href='css/sm-clean.css' />
        <link rel="stylesheet" type="text/css"  href='style.css' />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.10/styles/agate.min.css"/>

        <!--[if lt IE 9]>
                <script src="js/html5.js"></script>
        <![endif]-->

        <style>
            body {
                background-color: #19134E;
                
            }
            
        </style>
       
        <script type="text/javascript">
            function toggleLessonbar(){
             document.getElementById("lessonbar").classList.toggle('active');
            }
        </script>    
    </head>

        <title>TWD - Thinking With Data</title>
        <body> 
           
           
            <div class="menu-wrapper center-relative">
                <nav id="header-main-menu">
                    <div class="mob-menu">MENU</div>
                    <ul class="main-menu sm sm-clean">
                        <li><a href="index.html#home">Home</a></li>
                        <li><a href="#lessonbanner">Lessons</a></li>                    
                    </ul>
                </nav>
            </div>
           
                
            
            <div id="lessonbanner" class="section lessons-intro-page">
                <div class="block content-1170 center-relative center-text">
                    <br><br><br><br><br><br>
                    <h1 class="big-title">Lessons</h1>
                    <p class="title-desc">Scroll down to start learning...</p>
                    <!--<button class="lesson-button" onclick="toggleLessonbar()"> View Lessons </button>-->
                </div>
            </div>
            <div></div>
            <div id="lessonbar">
                <div class="menu-button" onclick="toggleLessonbar()">
                    <a>
                    <span></span>
                    <span></span>
                    <span></span>
                    </a>
                </div>    
                <ul class="toc">
                        <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>
                    <li><h1 class="lesson-topic"><a class="lesson-item" href="#queries">SQL Queries</a></h1></li>
                    <li><a class="lesson-item" href="#SQL-Setup">Setup and Introduction</a></li>
                    <li><a class="lesson-item" href="#Creating-Tables">Creating Tables</a></li>
                    <li><a class="lesson-item" href="#SQL-Queries">Writing SQL Queries</a></li>
                    <li><a class="lesson-item" href="#SQL-Queries">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Select Statements</a></li>
                    <li><a class="lesson-item" href="#Functions & Wildcards">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Functions and Wildcards</a></li>
                    <li><a class="lesson-item" href="#GroupBy">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Group By</a></li>
                    <li><a class="lesson-item" href="#Joins">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Unions and Joins</a></li>
                    <li><a class="lesson-item" href="#Nested">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Nested Queries</a></li>
                    <li><a class="lesson-item" href="#SQL-Resources">Useful Resources</a></li>
                    <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>
                    
                    <li><h1 class="lesson-topic"><a class="lesson-item" href="#DP">Data Pre-Processing</a></h1></li>
                    <li><a class="lesson-item" href="#Configuring-Pandas">Setup and Introduction</a></li>
                    <li><a class="lesson-item" href="#Structuring-Data">Structuring Data</a></li>
                    <li><a class="lesson-item" href="#Missing-Values">Interpolating Missing Data</a></li>
                    <li><a class="lesson-item" href="#Pandas-Resources">Useful Resources</a></li>
                    <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>
                    <li><h1 class="lesson-topic"><a class="lesson-item" href="#DE">Feature Exploration</a></h1></li>
                    <li><a class="lesson-item" href="#Explore-Setup">Setup and Introduction</a></li>
                    <li><a class="lesson-item" href="#Univariate">Univariate Analysis</a></li>
                    <li><a class="lesson-item" href="#Out">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Removing Outliers</a></li>
                    <li><a class="lesson-item" href="#Bivariate">Bivariate Exploration</a></li>
                    <li><a class="lesson-item" href="#Feature-Engineering">Feature Engineering</a></li>
                    <li><a class="lesson-item" href="#PCA">Principal Components</a></li>
                    <li><a class="lesson-item" href="#Explore-Resources">Useful Resources</a></li>
                    <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>

                    <li><h1 class="lesson-topic"><a class="lesson-item" href="#DM">Data Modeling</a></h1></li>
                    <li><a class="lesson-item" href="#Model-Setup">Setup and Introduction</a></li>
                    <li><a class="lesson-item" href="#Train-Test">Train-Test </a></li>
                    <li><a class="lesson-item" href="#Cross-Validation">Cross-Validation</a></li>
                    <li><a class="lesson-item" href="#Bias-Variance">Bias-Variance Tradeoff</a></li>   
                    <li><a class="lesson-item" href="#Loss-Functions">Loss Functions</a></li>
                    <li><a class="lesson-item" href="#MAE">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MAE (L1) and MSE (L2)</a></li>  
                    <li><a class="lesson-item" href="#Huber">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Huber</a></li> 
                    <li><a class="lesson-item" href="#Log">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Binary Cross-Entropy</a></li>  
                    <li><a class="lesson-item" href="#Optimizers">Optimization Algorithms</a></li> 
                    <li><a class="lesson-item" href="#GD">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Gradient Descent (GD)</a></li>
                    <li><a class="lesson-item" href="#SGD">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Stochastic GD (SGD)</a></li>
                    <li><a class="lesson-item" href="#Momentum">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Momentum</a></li>
                    <li><a class="lesson-item" href="#AdaGrad">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;AdaGrad</a></li> 
                    <li><a class="lesson-item" href="#RMS">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;RMSProp</a></li> 
                    <li><a class="lesson-item" href="#Adam">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Adam</a></li> 
                    <li><a class="lesson-item" href="#MLE">MLE for Parameterization</a></li>  
                    <li><a class="lesson-item" href="#Linear-Regression">Machine Learning Models</a></li>  
                    <li><a class="lesson-item" href="#Linear-Regression">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Linear Regression</a></li>
                    <li><a class="lesson-item" href="#Ridge">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Ridge, LASSO, Elastic Net</a></li> 
                    <li><a class="lesson-item" href="#Logit">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Logistic Regression</a></li>
                    <li><a class="lesson-item" href="#Confusion">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Confusion Matrix</li>
                    <li><a class="lesson-item" href="#AUC-ROC">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;AUC and ROC</li>
                    <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Decision Trees</li>
                    <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Random Forest</li>
                    <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;K-Nearest Neighbors</li>             
                    <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Support Vector Machines</li>
                    <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Naive Bayes</li>
                    <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;K-Means Clustering</li>
                    <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Neural Networks</li>
                    <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bagging and Boosting</li>
                    <li><a class="lesson-item" href="#ARMA">Time-Series Models</a></li>
                    <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Seasonality</li>
                    <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Stationarity</li>
                    <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Smoothing</li>
                    <li><a class="lesson-item" href="#ARMA">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;AR, MA, ARMA</a></li>
                    <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ARIMA and SARIMA</li>
                    <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Cointegration and Causality</li>
                    <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;VAR and VECM</li>
                    <li><a class="lesson-item" href="#Model-Resources">Useful Resources</a></li>
                    <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>

                </ul>
            </div>



















            <!-- POSTS -->

            <div id="lesson-post" class="post_background">
                
                <!-- QUERIES TITLE -->
                <h1 id = "queries" class="topic_title">SQL Queries </h1>
                <!-- QUERIES TITLE -->


                <div id="SQL-Setup">
                <!-- SETUP AND INSTRUCTIONS -->
                <h1 class="post_title"> Setup and Introduction </h1>
                <p class="lesson-text">    In order to get started, make sure you have your environment set up. To do this, I recommend referencing these links:</p>
                
                <ul class = "bullets">
                    <li>  You can download MySQL <a href="https://dev.mysql.com/downloads/mysql/"><u>here</u></a>.</li>

                    <li>  One free interface I find very useful in writing SQL commands is PopSQL. Which you can download 

                        <a href="https://popsql.com/"><u>here</u></a>.</li>
                </ul>
                <p class="lesson-text">     Before we dive into writing SQL queries, it's important to understand what SQL and MySQL are.
                        SQL (Structured Query Language) represents the syntax used in manipulating relational databases, whereas MySQL is a RDBMS (Relational Database Management System).
                        In other words, MySQL uses Structured Query Language to manipulate databases.
                </p>
                </div>
                    
                <div id="RD">
               
                <p class="lesson-text">  
                    Shown below are two tables within a database. The table on the left shows the players trying out for sports teams
                    while the table on the right shows the coaches for each sport.<br><br>
                    <img  style="display: inline-block" src="images/rd1.png" alt="Tables" />
                    
                </p>
                <p class="lesson-text"> 
                    The database shown above is a <i> relational database</i> because the two tables 'Players', and 'Coaches' are related
                    by a common field 'Sport'. Often times databases will have tables that are related to each other and through SQL queries,
                    we can retrieve information from these tables. 
                    </p>
                </div>
                <div id="Creating-Tables">
                     <!-- Basic SQL Commands -->
                    <h1 class="post_title"> Creating Tables </h1>
                    <p class="lesson-text"> 
                            Let's start off by creating the two tables we'll be querying in this lesson. The two tables will be
                            called 'Players' and 'Coaches'. We can make use of the <code>CREATE TABLE</code> and <code>INSERT</code> commands
                            to do this. <br><br>
                            <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-keyword" style="font-weight: 700; color: navy;">CREATE</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">TABLE</span> Players(
<span class="hljs-keyword" style="font-weight: 700; color: navy;">Name</span> <span class="hljs-built_in" style="font-weight: 700; color: navy;">VARCHAR</span>(<span class="hljs-number" style="color: rgb(136, 0, 0);">20</span>) PRIMARY <span class="hljs-keyword" style="font-weight: 700; color: navy;">KEY</span>,
Age <span class="hljs-built_in" style="font-weight: 700; color: navy;">INT</span>,
Experience <span class="hljs-built_in" style="font-weight: 700; color: navy;">INT</span>,
Sport <span class="hljs-built_in" style="font-weight: 700; color: navy;">VARCHAR</span>(<span class="hljs-number" style="color: rgb(136, 0, 0);">20</span>)
);
                            
<span class="hljs-keyword" style="font-weight: 700; color: navy;">CREATE</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">TABLE</span> Coaches(
<span class="hljs-keyword" style="font-weight: 700; color: navy;">Name</span> <span class="hljs-built_in" style="font-weight: 700; color: navy;">VARCHAR</span>(<span class="hljs-number" style="color: rgb(136, 0, 0);">20</span>) PRIMARY <span class="hljs-keyword" style="font-weight: 700; color: navy;">KEY</span>,
Age <span class="hljs-built_in" style="font-weight: 700; color: navy;">INT</span>,
Sport <span class="hljs-built_in" style="font-weight: 700; color: navy;">VARCHAR</span>(<span class="hljs-number" style="color: rgb(136, 0, 0);">20</span>)
);
                            
<span class="hljs-keyword" style="font-weight: 700; color: navy;">INSERT</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">INTO</span> Players <span class="hljs-keyword" style="font-weight: 700; color: navy;">VALUES</span>(<span class="hljs-string" style="color: rgb(0, 85, 0);">'Derek'</span>,<span class="hljs-number" style="color: rgb(136, 0, 0);">18</span>,<span class="hljs-number" style="color: rgb(136, 0, 0);">2</span>,<span class="hljs-string" style="color: rgb(0, 85, 0);">'Soccer'</span>);
<span class="hljs-keyword" style="font-weight: 700; color: navy;">INSERT</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">INTO</span> Players <span class="hljs-keyword" style="font-weight: 700; color: navy;">VALUES</span>(<span class="hljs-string" style="color: rgb(0, 85, 0);">'Bob'</span>,<span class="hljs-number" style="color: rgb(136, 0, 0);">19</span>,<span class="hljs-number" style="color: rgb(136, 0, 0);">1</span>,<span class="hljs-string" style="color: rgb(0, 85, 0);">'Basketball'</span>);
<span class="hljs-keyword" style="font-weight: 700; color: navy;">INSERT</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">INTO</span> Players <span class="hljs-keyword" style="font-weight: 700; color: navy;">VALUES</span>(<span class="hljs-string" style="color: rgb(0, 85, 0);">'Michael'</span>,<span class="hljs-number" style="color: rgb(136, 0, 0);">18</span>,<span class="hljs-number" style="color: rgb(136, 0, 0);">4</span>,<span class="hljs-string" style="color: rgb(0, 85, 0);">'Basketball'</span>);
<span class="hljs-keyword" style="font-weight: 700; color: navy;">INSERT</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">INTO</span> Players <span class="hljs-keyword" style="font-weight: 700; color: navy;">VALUES</span>(<span class="hljs-string" style="color: rgb(0, 85, 0);">'Tom'</span>,<span class="hljs-number" style="color: rgb(136, 0, 0);">16</span>,<span class="hljs-number" style="color: rgb(136, 0, 0);">3</span>,<span class="hljs-string" style="color: rgb(0, 85, 0);">'Golf'</span>);
<span class="hljs-keyword" style="font-weight: 700; color: navy;">INSERT</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">INTO</span> Players <span class="hljs-keyword" style="font-weight: 700; color: navy;">VALUES</span>(<span class="hljs-string" style="color: rgb(0, 85, 0);">'Fred'</span>,<span class="hljs-number" style="color: rgb(136, 0, 0);">14</span>,<span class="hljs-number" style="color: rgb(136, 0, 0);">1</span>,<span class="hljs-string" style="color: rgb(0, 85, 0);">'Soccer'</span>);
<span class="hljs-keyword" style="font-weight: 700; color: navy;">INSERT</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">INTO</span> Players <span class="hljs-keyword" style="font-weight: 700; color: navy;">VALUES</span>(<span class="hljs-string" style="color: rgb(0, 85, 0);">'Carl'</span>,<span class="hljs-number" style="color: rgb(136, 0, 0);">18</span>,<span class="hljs-number" style="color: rgb(136, 0, 0);">0</span>,<span class="hljs-string" style="color: rgb(0, 85, 0);">'Golf'</span>);
<span class="hljs-keyword" style="font-weight: 700; color: navy;">INSERT</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">INTO</span> Players <span class="hljs-keyword" style="font-weight: 700; color: navy;">VALUES</span>(<span class="hljs-string" style="color: rgb(0, 85, 0);">'Ivan'</span>,<span class="hljs-number" style="color: rgb(136, 0, 0);">21</span>,<span class="hljs-number" style="color: rgb(136, 0, 0);">0</span>,<span class="hljs-string" style="color: rgb(0, 85, 0);">'Basketball'</span>);
<span class="hljs-keyword" style="font-weight: 700; color: navy;">INSERT</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">INTO</span> Players <span class="hljs-keyword" style="font-weight: 700; color: navy;">VALUES</span>(<span class="hljs-string" style="color: rgb(0, 85, 0);">'Brian'</span>,<span class="hljs-number" style="color: rgb(136, 0, 0);">13</span>,<span class="hljs-number" style="color: rgb(136, 0, 0);">1</span>,<span class="hljs-string" style="color: rgb(0, 85, 0);">'Basketball'</span>);
                            
<span class="hljs-keyword" style="font-weight: 700; color: navy;">INSERT</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">INTO</span> Coaches <span class="hljs-keyword" style="font-weight: 700; color: navy;">VALUES</span>(<span class="hljs-string" style="color: rgb(0, 85, 0);">"Leo"</span>,<span class="hljs-number" style="color: rgb(136, 0, 0);">34</span>,<span class="hljs-string" style="color: rgb(0, 85, 0);">'Soccer'</span>);
<span class="hljs-keyword" style="font-weight: 700; color: navy;">INSERT</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">INTO</span> Coaches <span class="hljs-keyword" style="font-weight: 700; color: navy;">VALUES</span>(<span class="hljs-string" style="color: rgb(0, 85, 0);">"Brian"</span>,<span class="hljs-number" style="color: rgb(136, 0, 0);">36</span>, <span class="hljs-string" style="color: rgb(0, 85, 0);">'Basketball'</span>);
<span class="hljs-keyword" style="font-weight: 700; color: navy;">INSERT</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">INTO</span> Coaches <span class="hljs-keyword" style="font-weight: 700; color: navy;">VALUES</span>(<span class="hljs-string" style="color: rgb(0, 85, 0);">"Gary"</span>,<span class="hljs-number" style="color: rgb(136, 0, 0);">49</span>,<span class="hljs-string" style="color: rgb(0, 85, 0);">'Golf'</span>);</pre>
                    </p>
                    <p class="lesson-text"> 
                    
                    We first created two tabels 'Players' and 'Coaches'. Inside each <code>CREATE TABLE</code> block
                    are the names and datatypes of each column of each table. The <code>PRIMARY KEY</code> signifies the
                    ID of each entry, similar to an index. Relational databases may also have a <code>FOREIGN KEY</code>  
                    that signifies a column or attribute that is the <code>PRIMARY KEY</code> of <i>another</i> table. After creating
                    the tables we then <code>INSERT INTO</code> each table specific <code>VALUES</code>, this is how you populate
                    a table using SQL.<br>
                    There are many other commands we can use to manipulate our tables before querying them. I've outlined some of them below.<br><br>
                    <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-keyword" style="font-weight: 700; color: navy;">DESCRIBE</span> Players; <span class="hljs-keyword" style="color: rgb(119, 119, 119);"># Summarizes the Players</span>
<span class="hljs-keyword" style="font-weight: 700; color: navy;">DROP TABLE</span> Players; <span class="hljs-comment" style="color: rgb(119, 119, 119);"># Deletes the Players table</span>
<span class="hljs-keyword" style="font-weight: 700; color: navy;">ALTER TABLE</span> Players <span class="hljs-comment" style="color: rgb(0, 0, 0);"><span class="hljs-keyword" style="font-weight: 700; color: navy;">ADD</span> Height <span class="hljs-keyword" style="font-weight: 700; color: navy;">INT</span></span>;<span class="hljs-comment" style="color: rgb(119, 119, 119);"> # Adds a</span> <span class="hljs-comment" style="color: rgb(119, 119, 119);">"Height"</span><span class="hljs-comment" style="color: rgb(119, 119, 119);"> attribute to Players</span>
<span class="hljs-keyword" style="font-weight: 700; color: navy;">ALTER TABLE</span> Players <span class="hljs-comment" style="color: rgb(0, 0, 0);"><span class="hljs-keyword" style="font-weight: 700; color: navy;">DROP COLUMN</span> Height</span>;<span class="hljs-comment" style="color: rgb(119, 119, 119);"> # Deletes the</span> <span class="hljs-comment" style="color: rgb(119, 119, 119);">"Height"</span><span class="hljs-comment" style="color: rgb(119, 119, 119);"> attribute from Players</span>
<span class="hljs-keyword" style="font-weight: 700; color: navy;">UPDATE</span> Coaches <span class="hljs-keyword" style="font-weight: 700; color: navy;">SET</span> Age = <span class="hljs-number" style="color: rgb(136, 0, 0);">40</span>; <span class="hljs-number" style="color: rgb(119, 119, 119);"># Updates the ages of coaches to 40</span>
<span class="hljs-keyword" style="font-weight: 700; color: navy;">DELETE FROM</span> Players <span class="hljs-keyword" style="font-weight: 700; color: navy;">WHERE</span> Name = <span class="hljs-string" style="color: rgb(0, 85, 0);">'Tom'</span>; <span class="hljs-keyword" style="color: rgb(119, 119, 119);"># Deletes Tom from Players</span></pre>    
                    </p>
                </div>

                <div id="SQL-Queries">
                        <!-- SQL QUERIES -->
                        <h1 class="post_title"> Writing SQL Queries </h1>
                        
                        <p class="lesson-text"> <a href="https://dev.mysql.com/doc/refman/8.0/en/select.html"><u><strong><code>SELECT</code></strong></u></a><br>"Querying" a database is another way of saying you want to "retrieve information
                            from" a database. SQL queries make use of a powerful command called <code>SELECT</code>, it allows
                        us to retrieve specific information from our database. Let's first start off by selecting all the data from
                        our 'Players' table. We can do this using <code>SELECT * FROM Players;</code>. After running this, we should 
                        see the 'Players' table. The <code>SELECT</code> command is where we put the attributes we're hoping to retrieve,
                        the <code>FROM</code> command is used to signify which table we plan on selecting our attributes from, the * means "all", and a <code>WHERE</code> command
                        is used to provide a condition to determine which specific entries from those columns we hope to retrieve. Here's an example of one of these
                        queries. <br><br>
                        <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-keyword" style="font-weight: 700; color: navy;">SELECT</span> Name, Sport <span class="hljs-keyword" style="font-weight: 700; color: navy;">FROM</span> Players <span class="hljs-keyword" style="font-weight: 700; color: navy;">WHERE</span> Age &gt;= <span class="hljs-number" style="color: rgb(136, 0, 0);">18</span>;</pre>
                        <br> 
                        <img  src="images/query1.png" alt="Query #1 Output" /> 
                    </p>   
                    <p class="lesson-text">
                        In the above query, we selected the 'Name' and 'Sport' of each person in the 'Players' table whose age 
                        is at least 18. We could also add a <code>ORDER BY Sport</code> after the <code>WHERE</code> command
                        in order to see our derived table sorted by 'Sport'. <br>
                        If we were instead only interested in the sports that have players over 18, we could take out the <code>Name</code> attribute
                        from our <code>SELECT</code> command. However, when running this query, you'll notice the same sports come up multiple times.
                        We can get around this by using the <code>DISTINCT</code> keyword. <br><br>
                        <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-keyword" style="font-weight: 700; color: navy;">SELECT</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">DISTINCT</span> Sport <span class="hljs-keyword" style="font-weight: 700; color: navy;">FROM</span> Players <span class="hljs-keyword" style="font-weight: 700; color: navy;">WHERE</span> Age &gt;= <span class="hljs-number" style="color: rgb(136, 0, 0);">18</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">ORDER</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">BY</span> Sport;</pre><br>
                        <img  src="images/query2.png" alt="Query #2 Output" />

                    </p>
                    <p class="lesson-text">
                        Let's do one more example. What if we wanted to see all the players who play either Soccer or Golf who are
                        between 14 and 17? We can do this using the following query. <br><br>
                        <pre id = "Functions & Wildcards" class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-keyword" style="font-weight: 700; color: navy;">SELECT</span> Name
<span class="hljs-keyword" style="font-weight: 700; color: navy;">FROM</span> Players 
<span class="hljs-keyword" style="font-weight: 700; color: navy;">WHERE</span> (age <span class="hljs-keyword" style="font-weight: 700; color: navy;">BETWEEN</span> <span class="hljs-number" style="color: rgb(136, 0, 0);">14</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">AND</span> <span class="hljs-number" style="color: rgb(136, 0, 0);">17</span>) <span class="hljs-keyword" style="font-weight: 700; color: navy;">AND</span> (Sport <span class="hljs-keyword" style="font-weight: 700; color: navy;">IN</span> (<span class="hljs-string" style="color: rgb(0, 85, 0);">'Soccer'</span>, <span class="hljs-string" style="color: rgb(0, 85, 0);">'Golf'</span>));</pre>
                    </p>
                    <p  class="lesson-text"> <a href="https://dev.mysql.com/doc/refman/8.0/en/select.html"><u><strong><code>Functions</code></strong></u></a><br>
                    <br>
                    Although it's useful to be able to retrieve entries from our database, sometimes we may be interested in 
                    characteristics of our data. SQL keywords such as <code>AVG, COUNT, SUM, MAX,</code> and <code> MIN</code> can be used to 
                    return characteristics of a column. Here we find some summary statistics of the 
                    basketball team's age distribution. We use the <code>AS</code> keyword to rename the columns in our derived table.
                    <br><br>
                    <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-keyword" style="font-weight: 700; color: navy;">SELECT</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">AVG</span>(Age) <span class="hljs-keyword" style="font-weight: 700; color: navy;">AS</span> Average, <span class="hljs-keyword" style="font-weight: 700; color: navy;">MAX</span>(Age) <span class="hljs-keyword" style="font-weight: 700; color: navy;">AS</span> Maximum, <span class="hljs-keyword" style="font-weight: 700; color: navy;">MIN</span>(Age) <span class="hljs-keyword" style="font-weight: 700; color: navy;">AS</span> Minimum, <span class="hljs-keyword" style="font-weight: 700; color: navy;">COUNT</span>(AGE) <span class="hljs-keyword" style="font-weight: 700; color: navy;">AS</span> <span class="hljs-string" style="color: rgb(0, 85, 0);">'Count'</span>, <span class="hljs-keyword" style="font-weight: 700; color: navy;">SUM</span>(AGE) <span class="hljs-keyword" style="font-weight: 700; color: navy;">AS</span> <span class="hljs-string" style="color: rgb(0, 85, 0);">'Sum'</span> 
<span class="hljs-keyword" style="font-weight: 700; color: navy;">FROM</span> Players 
<span class="hljs-keyword" style="font-weight: 700; color: navy;">WHERE</span> Sport = <span class="hljs-string" style="color: rgb(0, 85, 0);">'Basketball'</span>;</pre>
                    </p>   
                    <p  class="lesson-text">
                        What if we wanted to filter our data based on the letters within non-numerical data? For instance, if we wanted
                        to find the average age of the players whose names end with the letter 'l' we can use SQL Wildcards which
                        make use of the <code>LIKE</code> keyword.<br><br>
                        <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-keyword" style="font-weight: 700; color: navy;">SELECT</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">AVG</span>(Age) <span class="hljs-keyword" style="font-weight: 700; color: navy;">AS</span> Average <span class="hljs-keyword" style="font-weight: 700; color: navy;">FROM</span> Players <span class="hljs-keyword" style="font-weight: 700; color: navy;">WHERE</span> Name <span class="hljs-keyword" style="font-weight: 700; color: navy;">LIKE</span> <span class="hljs-string" style="color: rgb(0, 85, 0);">"%l"</span>;</pre>
                    </p>
                    <p  class="lesson-text">
                        The <code>LIKE</code> keyword compares a <code>VARCHAR</code> (a word/string) to a specified pattern. The <code>%</code>
                        in <code>"%l"</code> signifies a sequence of characters that ends with l. The <code>%</code> is what's called
                        a <i>Wildcard Character</i>. These are often helpful when looking for specific patters within non-numerical
                        data. A name starting with M would use the wildcard <code>"M%"</code>, while a name whose second letter is r would
                        use <code>"_r%"</code>. You can read more about wildcards <a id = "GroupBy" href="https://www.w3schools.com/sql/sql_wildcards.asp"><u>here</u></a>.
                    </p>
                    <p  class="lesson-text"> <a href="https://dev.mysql.com/doc/refman/8.0/en/select.html"><u><strong><code>GROUP BY</code></strong></u></a><br>
                        <br>
                        Sometimes we're interested in querying data by categories or <i>groups</i>. To do this, we can make use
                        of the <code>GROUP BY</code> command. Let's say we were interested in the number of players for each sport.
                        We would have to count the number of players <i>grouped</i> by Sport. In this example, we ordered
                        the derived table in reverse-alphabetical order.<br><br>
                        <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-keyword" style="font-weight: 700; color: navy;">SELECT</span> Sport, <span class="hljs-keyword" style="font-weight: 700; color: navy;">COUNT</span>(Name) <span class="hljs-keyword" style="font-weight: 700; color: navy;">AS</span> Total 
<span class="hljs-keyword" style="font-weight: 700; color: navy;">FROM</span> Players 
<span class="hljs-keyword" style="font-weight: 700; color: navy;">GROUP</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">BY</span> Sport 
<span class="hljs-keyword" style="font-weight: 700; color: navy;">ORDER</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">BY</span> Sport <span class="hljs-keyword" style="font-weight: 700; color: navy;">DESC</span>;</pre>
                        <br><img  src="images/groupby.png" alt="GROUP BY" />
                        </p>
                        <p  class="lesson-text">
                            One confusing part of the <code>GROUP BY</code> command is knowing when to use <code>WHERE</code> versus
                            <code>HAVING</code> for conditional queries. <code>WHERE</code> is used <i>before</i> the <code>GROUP BY</code>
                            command whereas <code>HAVING</code> is used <i>after</i> the GROUP BY clause. Here are a couple of examples to help out.<br><br>
                            <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-keyword" style="font-weight: 700; color: navy;">SELECT</span> Sport, <span class="hljs-keyword" style="font-weight: 700; color: navy;">COUNT</span>(Name) <span class="hljs-keyword" style="font-weight: 700; color: navy;">AS</span> Total 
<span class="hljs-keyword" style="font-weight: 700; color: navy;">FROM</span> Players 
<span class="hljs-keyword" style="font-weight: 700; color: navy;">WHERE</span> Age &gt;= <span class="hljs-number" style="color: rgb(136, 0, 0);">18</span> 
<span class="hljs-keyword" style="font-weight: 700; color: navy;">GROUP</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">BY</span> Sport 
<span class="hljs-keyword" style="font-weight: 700; color: navy;">ORDER</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">BY</span> Sport <span class="hljs-keyword" style="font-weight: 700; color: navy;">DESC</span>;
                                
<span class="hljs-keyword" style="font-weight: 700; color: navy;">SELECT</span> Sport, <span class="hljs-keyword" style="font-weight: 700; color: navy;">AVG</span>(Age) <span class="hljs-keyword" style="font-weight: 700; color: navy;">AS</span> <span class="hljs-string" style="color: rgb(0, 85, 0);">'Average Age'</span>
<span class="hljs-keyword" style="font-weight: 700; color: navy;">FROM</span> Players 
<span class="hljs-keyword" style="font-weight: 700; color: navy;">GROUP</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">BY</span> Sport 
<span class="hljs-keyword" style="font-weight: 700; color: navy;">HAVING</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">COUNT</span>(Name) &gt; <span class="hljs-number" style="color: rgb(136, 0, 0);">2</span>
<span class="hljs-keyword" style="font-weight: 700; color: navy;">ORDER</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">BY</span> Sport <span class="hljs-keyword" style="font-weight: 700; color: navy;">DESC</span>;</pre>

                        </p>
                        <p  class="lesson-text">
                                In our first query, we're filtering entries based on the number of players who are at least 18.
                                This means we're filtering out players <i>before</i> they're grouped. Whether or not a player
                                is 18 years old is irrelevant to what sport he/she plays and thus we can make use of the
                                <code>WHERE</code> keyword. However, in our second query, we're filtering our results based on
                                how many players there are in each sport, or in other words, <i>after</i> they're grouped, which is why we use
                                the <code id = "Joins">HAVING</code> clause.
                        </p>
                        <p  class="lesson-text"><a href="https://dev.mysql.com/doc/refman/8.0/en/select.html"><u><strong><code>UNIONS & JOINS</code></strong></u></a><br>
                            Unions and joins are incredibly useful in querying data from tables that
                            can be combined. A <code>UNION</code> is an easy way to combine multiple <code>SELECT</code> statements.
                            Here's an example. <br><br>
                            <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-keyword" style="font-weight: 700; color: navy;">SELECT</span> Name, Age
<span class="hljs-keyword" style="font-weight: 700; color: navy;">FROM</span> Players
<span class="hljs-keyword" style="font-weight: 700; color: navy;">WHERE</span> Age &gt; <span class="hljs-number" style="color: rgb(136, 0, 0);">18</span>
<span class="hljs-keyword" style="font-weight: 700; color: navy;">UNION</span>
<span class="hljs-keyword" style="font-weight: 700; color: navy;">SELECT</span> Name, Age
<span class="hljs-keyword" style="font-weight: 700; color: navy;">FROM</span> Coaches
<span class="hljs-keyword" style="font-weight: 700; color: navy;">WHERE</span> Age &gt; <span class="hljs-number" style="color: rgb(136, 0, 0);">40</span>;</pre>
                            <br><img  src="images/union.png" alt="Union Output" />
                        </p>
                        <p  class="lesson-text">
                            In this example, we selected both the players who are over 18 <i>and</i> the coaches who are over 40.
                            Unions are convenient when you want to create new rows in your table. The query result is a derived table
                            that simply stacks the table with players over 18 on top of the table of coaches over 40. If instead of this
                            we were interested in combining <i>columns</i> of data. We could make use of the <code>JOIN</code> command.<br><br>
                            <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-keyword" style="font-weight: 700; color: navy;">SELECT</span> Players.Name <span class="hljs-keyword" style="font-weight: 700; color: navy;">as</span> <span class="hljs-string" style="color: rgb(0, 85, 0);">'Player Name'</span>, Coaches.Name <span class="hljs-keyword" style="font-weight: 700; color: navy;">as</span> <span class="hljs-string" style="color: rgb(0, 85, 0);">'Coach Name'</span>
<span class="hljs-keyword" style="font-weight: 700; color: navy;">FROM</span> Players
<span class="hljs-keyword" style="font-weight: 700; color: navy;">LEFT</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">JOIN</span> Coaches
<span class="hljs-keyword" style="font-weight: 700; color: navy;">ON</span> Players.Sport = Coaches.Sport;</pre><br>
                            <img src="images/leftjoin.png" alt="Left Join" />
                        </p>
                        <p  class="lesson-text"> In the query above, we're retrieving the names of each player along with their coaches.
                            Their coaches are determined by their sport which is why we joined <code>ON Players.Sport = Coaches.Sport</code>.
                            However, I included a <code>LEFT JOIN</code> in my query when combining both tables, what is this? A <code>LEFT JOIN</code>
                            is a method of joining two tables. It starts with each entry in the left table (Players) and then <i>matches</i> it to a value on the right
                            table (Coaches) based on their shared attribute (Sport), a <code>LEFT JOIN</code> is the same as as
                            <code>LEFT OUTER JOIN</code>. There are other joins like right joins, inner joins, full-outer joins, and others. The best way to understand the various joins is by visualizing the joins using venn diagrams. Here's a helpful
                            and easy diagram I found on <a href="https://www.reddit.com/r/SQL/comments/108xpy/visual_guide_to_sql_joins/"><u>Reddit</u></a> that shows the different types of joins. <br><br>
                            
                        </p>
                        <p  class="lesson-text">
                            To know when to use each type of <code>join</code>, it's important you understand how they work conceptually. The Venn Diagram will help
                            but the best practice is to try each <code>join</code> out yourself.
                        </p>
                </div>

                <div id="Nested">
                        <!-- NESTED QUERIES -->
                        <h1 class="post_title"> Nested Queries </h1>
                        <p class="lesson-text">
                            Sometimes we may need to write more complex queries and make use of <i>nested</i> queries. A nested query, sometimes called a subquery, is a
                            <code>SELECT</code> statement <i>within</i> another <code>SELECT</code> clause. You can
                            think of it as a query that's based on another query. We'll start off with a simple example. Let's say we wanted
                            a list of players with the same name as a coach. We could this simply by: <br><br>
                            <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-keyword" style="font-weight: 700; color: navy;">SELECT</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">DISTINCT </span> Name 
<span class="hljs-keyword" style="font-weight: 700; color: navy;">FROM</span> Players 
<span class="hljs-keyword" style="font-weight: 700; color: navy;">WHERE</span> Name <span class="hljs-keyword" style="font-weight: 700; color: navy;">IN</span> (<span class="hljs-keyword" style="font-weight: 700; color: navy;">SELECT</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">DISTINCT</span> Name <span class="hljs-keyword" style="font-weight: 700; color: navy;">FROM</span> Coaches);</pre>
                        </p>
                        <p class="lesson-text">
                            After running this query, we get <code>Brian</code> as an output, the only common name between the two tables. In this example 
                            the subquery is <code>"SELECT DISTINCT Name FROM Coaches"</code> within the <code>"SELECT DISTINCT Name FROM Players WHERE..."</code> query.
                            Now let's do a slightly more complicated nested query. How could we query the 'Names' and 'Sports' of the coaches who coach teams
                            whose players' average age is at least 17, or, who coach a team that has more than two players. Although this sounds complex, it's often easiest
                            to break down the problem into smaller parts. The first part being how to query the 'Name' and 'Sport' of 'Coaches' within the 'Coach' table. 
                            The next part is figuring out how to query the sports whose players meet the conditions. The last part requires the use of the
                            <code>GROUP BY</code> clause, as illustrated below.<br><br>
                            <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-keyword" style="font-weight: 700; color: navy;">SELECT</span> Name, Sport
<span class="hljs-keyword" style="font-weight: 700; color: navy;">FROM</span> Coaches
<span class="hljs-keyword" style="font-weight: 700; color: navy;">WHERE</span> Sport <span class="hljs-keyword" style="font-weight: 700; color: navy;">IN</span> 
   (<span class="hljs-keyword" style="font-weight: 700; color: navy;">SELECT</span> Sport
    <span class="hljs-keyword" style="font-weight: 700; color: navy;">FROM</span> Players 
    <span class="hljs-keyword" style="font-weight: 700; color: navy;">GROUP</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">BY</span> Sport 
    <span class="hljs-keyword" style="font-weight: 700; color: navy;">HAVING</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">COUNT</span>(Name) &gt; <span class="hljs-number" style="color: rgb(136, 0, 0);">2</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">OR</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">AVG</span>(Age)&gt;=<span class="hljs-number" style="color: rgb(136, 0, 0);">17</span>);</pre>
                        </p>
                        <p class="lesson-text">
                            Subqueries are extremely useful when using SQL and can get complicated very quickly. A good strategy is to take the query
                            you're trying to write, and break it down into its queries and subqueries individually if possible. From there, you can
                            combine your queries and subqueries to reach your final query. Remember, practice is key.
                        </p>
                </div>


                <div id="SQL-Resources">
                        <!-- SQL RESOURCES -->
                        <h1 class="post_title"> Useful Resources </h1>
                        <p class="lesson-text">    Even though we've learned the most important and "fundamental" SQL commands, we haven't
                            worked with large datasets, complex nested queries, or certain topics such as table aliases and triggers. It's helpful to make your own tables and try to create queries yourself
                            in order to practice. Nevertheless, here are some references I think are particularly useful.
                        </p>
                        
                        <ul class = "bullets">
                            <li>  Here are a couple of places you can practice SQL commands. I personally recommend
                                Hackerrankl, though have found all three helpful. <a href="https://www.hackerrank.com/domains/sql"><u>Hackerrank</u></a>, 
                                <a href="https://www.w3schools.com/sql/exercise.asp"><u>W3Schools</u></a>, <a href="https://sqlzoo.net/"><u>SQL Zoo</u></a>.
                            
                            </li>
          
                            <li>  Here's a downloadable cheatsheet that goes over the main SQL commands.  
                                <a href="https://www.sqltutorial.org/sql-cheat-sheet/"><u>Cheatsheet</u></a>.
                            </li>
                        </ul>
                            </p>   
                </div>























                <!-- DATA PREPERATION TITLE -->
                <h1 id = "DP" class="topic_title">Data Pre-Processing </h1>
                <!-- DATA PREPERATION TITLE -->


                <div id="Configuring-Pandas">
                <!-- SETUP AND INSTRUCTIONS -->
                <h1 class="post_title"> Setup and Introduction </h1>
                <p class="lesson-text">    In order to get started, make sure you have your environment set up. In order to do this, I recommend referencing these links:</p>
                
                <ul class = "bullets">
                    <li>  You can download Python <a href="https://www.python.org/downloads/"><u>here</u></a>.</li>

                    <li>  Most Python code written here will be written on a Jupyter Notebook. You can reference 
                        <a href="https://jupyter.org/install"><u>this guide</u></a> on Jupyter's site for installation. Google Colab works as well.</li>
  
                    <li>  Some important libraries we'll be using for data preprocessing include pandas, numpy, matplotlib, and seaborn. All of which can be installed
                        following these <a href="https://docs.python.org/3/installing/index.html"><u>guidelines</u></a>.
                    </li>
                    <li> You can download the CSV file we'll be referencing <a href="supplements/Sample_Stocks.csv" download><u>here</u></a>.</li>
                </ul>
                </div>


                <!-- Structuring Data -->
                <div id="Structuring-Data">
                <h1 class="post_title"> Structuring Data </h1>
                <p class="lesson-text">    The first thing we have to do is load our data into our Jupyter Notebook. To open up your Jupyter Notebook, 
                    simply open up your command prompt/terminal and type <code>jupyter notebook</code>. A full guide on the interface can be found 
                    <a href="https://jupyter-notebook.readthedocs.io/en/latest/notebook.html"><u>here</u></a>.
                    Let's first import pandas into our notebook. We'll reference it as "pd" for simplicity.
                    <br><br>
                    <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-keyword" style="font-weight: 700; color: navy;">import</span> pandas <span class="hljs-keyword" style="font-weight: 700; color: navy;">as</span> pd</pre>
                </p>
                <p class="lesson-text"> Pandas is a powerful tool used for manipulating data by providing functionalities to make working with out data easier. The CSV file referenced in this part of the tutorial can be downloaded 
                    <a href="supplements/Sample_Stocks.csv" download><u>here</u></a>. The first powerful tool in pandas we'll make use of is a DataFrame.
                    You can think of a DataFrame like a spreadsheet or a data table. It's a data structure representing a two-dimensional table. Let's read in our data from our CSV file and parse it into a DataFrame object to look at the first few rows of our data.
                    <br><br>
                    <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-attribute" style="color: rgb(0, 85, 0);">stock_data</span> = pd.read_csv(<span class="hljs-string" style="color: rgb(0, 85, 0);">'Sample_Stocks.csv'</span>) <span class="hljs-comment" style="color: rgb(136, 0, 0);"></span>
stock_data.head()   </pre>
                    <br>
                    <img  src="images/headfunction.png" alt="Head Output" />
                </p>
                <p class="lesson-text"> 
                    
                    The <code>head()</code> function displays the first few rows of our table. You can specify the last few rows using tail(). Specifying an integer parameter (<code>head(n) or tail(n)</code>) displays the first or last <code>n</code> rows. We can get summary statistics of our data as well through the <code>describe()</code> function. 
                    <br><br>
                    <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);">stock_data[<span class="hljs-string" style="color: rgb(0, 85, 0);">'HINDEX'</span>].describe() <span class="hljs-comment" style="color: rgb(136, 0, 0);"># Describes the HINDEX attribute</span>
stock_data.groupby(<span class="hljs-keyword" style="font-weight: 700; color: navy;">by</span>=<span class="hljs-string" style="color: rgb(0, 85, 0);">'Stock'</span>).describe() <span class="hljs-comment" style="color: rgb(136, 0, 0);"># Describes all attributes grouped by 'Stock'</span>
stock_data.groupby(<span class="hljs-keyword" style="font-weight: 700; color: navy;">by</span>=<span class="hljs-string" style="color: rgb(0, 85, 0);">'Stock'</span>)[<span class="hljs-string" style="color: rgb(0, 85, 0);">'Volume'</span>].describe() <span class="hljs-comment" style="color: rgb(136, 0, 0);"># Output shown below</span></pre>    
                    <br>
                    <img  src="images/volumedescribe.png" alt="Volume describe() Output" />        
                </p>
                <p class="lesson-text"> 
                    After looking through the columns, you may realize "HINDEX" is relatively useless as it's always zero. We can delete 
                    this column using: 
                    <br><br><pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);">stock_data.drop(columns = [<span class="hljs-string" style="color: rgb(0, 85, 0);">'HINDEX'</span>])</pre> 
                </p>
                <p class="lesson-text"> 
                    Using SQL, we were able to query a database in order to retrieve specific information. Thankfully, pandas provides a similar functionality. Let's
                    say we were interested in finding all the entries of stock A with a price of over $46.00. We can do this by using conditionals within
                    the pandas <code>loc</code> (locate) function. Let's try this out. 
                        <br><br><pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);">stock_data.loc[(stock_data[<span class="hljs-string" style="color: rgb(0, 85, 0);">'Stock'</span>]==<span class="hljs-string" style="color: rgb(0, 85, 0);">'A'</span>) &amp; (stock_data[<span class="hljs-string" style="color: rgb(0, 85, 0);">'Price'</span>]&gt;<span class="hljs-number" style="color: rgb(136, 0, 0);">46</span>)]</pre>
                        
                </p>
                <p class="lesson-text"> 
                    When we run this, we get an error message saying <code>TypeError: '>' not supported between instances of 'str' and 'int'</code>
                    What this message is telling us is we're trying to compare a string type variable to an integer. We'll have to 
                    convert the 'Price' column into floating point numbers before running our query. We can make use of the <code>replace()</code> function to remove the dollar signs and <code>astype()</code> function to convert the column.<br><br><pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);">stock_data[<span class="hljs-string" style="color: rgb(0, 85, 0);">'Price'</span>] = stock_data[<span class="hljs-string" style="color: rgb(0, 85, 0);">'Price'</span>].replace(<span class="hljs-string" style="color: rgb(0, 85, 0);">'[\$,]'</span>, <span class="hljs-string" style="color: rgb(0, 85, 0);">''</span>, regex=<span class="hljs-symbol" style="color: rgb(0, 85, 0);">True</span>).astype(float)
stock_data.loc[(stock_data[<span class="hljs-string" style="color: rgb(0, 85, 0);">'Stock'</span>]==<span class="hljs-string" style="color: rgb(0, 85, 0);">'A'</span>) &amp; (stock_data[<span class="hljs-string" style="color: rgb(0, 85, 0);">'Price'</span>]&gt;<span class="hljs-number" style="color: rgb(136, 0, 0);">46</span>)]</pre>      
                <br>
                <img  src="images/pandasquery.png" alt="Query Result" />
                </p>
                <p class="lesson-text"> 
                        So far, we've looked into the very basics of pandas. We'll continue to make use of this library
                        in future lessons which is why I highly recommend getting more comfortable with the pandas library and its
                        various functionalities. This will make understanding future lessons much easier.
                        Feel free to utilize the references provided in the <a href="#1Resources"><u>Useful Resources</u></a> section.
                    </p>

                </div>
               

                <div id="Missing-Values">
                        <h1 class="post_title"> Missing Data </h1>
                        <p class="lesson-text">    Part of our data cleaning process is dealing with missing values. One way to detect if your dataset has missing values
                            is to use the <code>.info()</code> function. Using the dataset from the <a href="#Structuring-Data"><u>previous lesson</u></a>,  <code>stock_data.info()</code> outputs the following:
                            <br><br>
                            <img  src="images/nulldetection.png" alt="info() output" />
                            <br><br>
                            <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html"><u><strong><code>dropna()</code></strong></u></a><br>
                            We can see our dataset only has 35 non-null price values and 37 non-null volume values out of 40 total values.
                            One way to deal with missing values is to delete the rows that have at least one value missing. Using pandas,
                            this can be done by calling the <code>dropna()</code> function on our DataFrame, returning a DataFrame with all rows containing
                            missing values deleted. <br>
                            <br>
                            <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html"><u><strong><code>fillna()</code></strong></u></a><br>
                            However, deleted entries may not always be the best option, especially when 
                            the entry has other important information we might want to keep. This leads us to our first method of handling
                            missing data using the <code>fillna()</code> function. The <code>fillna()</code> function
                            allows us to quickly find all NaN values and replace them using a specified method. You can learn more about 
                            <code>fillna()</code> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html"><u>here</u></a>.
                            One way to use <code>fillna()</code> is with the parameter <code>method = 'ffill' </code>. This parameter
                            propogates all non-null values forward, replacing each null value with the previous non-null value.
                            Conversely, <code>method = 'bfill'</code> does the opposite. <br><br>
                            <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);">stock_data.fillna(method = 'ffill', inplace = True)
stock_data</pre>
                            <br>
                            <img  src="images/fillna2.png" alt="FFill" />
                            </p>
                            <p class="lesson-text"> Previously, the values for 'Price' and 'Volume' for the entry at index 12
                                were both NaN. After using the <code>ffill</code> method of <code>fillna()</code> the values for 'Price'
                                and 'Volume' were replaced with the 'Price' and 'Volume' values for the entry at index 11. A snippet showing this change is
                                shown above. The inplace parameter in the above function call signifies that the outputted DataFrame
                                should replace our current DataFrame and make changes "in place."
                                <br>
                                <br>
                                <a href="https://pandas.pydata.org/pandas-docs/version/0.16/generated/pandas.DataFrame.interpolate.html"><u><strong><code>interpolate()</code></strong></u></a><br>
                                Another way to fill missing values, which is especially useful for time-series data, is the interpolate method. Interpolating data
                                is very helpful and there are many ways to do it. One of the simplest ways to do this that does not involve  fitting
                                a complex model to our data, is using the <code>interpolate()</code> function. It defaults to a linear method
                                which replaces all missing values with the <i>linear average</i> of the non-null values surrounding it. It can be called on
                                an entire DataFrame object like shown below, however, it can be useful for an individual series as well (i.e our 'Price' column alone). Read more about that
                                <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.interpolate.html"><u>here</u></a>.
                                <br><br>
                                <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);">stock_data.interpolate(<span class="hljs-function"><span class="hljs-keyword" style="font-weight: 700; color: navy;">method</span>='<span class="hljs-title" style="font-weight: 700; color: navy;">linear</span>', <span class="hljs-title" style="font-weight: 700; color: navy;">inplace</span>=<span class="hljs-title" style="font-weight: 700; color: navy;">True</span>) </span></pre>
                                <br>
                                <img  src="images/interpolate.png" alt="interpolate()" />

                </div>

                <div id="Pandas-Resources">
                    <h1 class="post_title"> Useful Resources </h1>
                    <p class="lesson-text">    So far, we've looked into the basics of manipulating our data with the pandas library. The following links and guidelines
                        are extremely useful and I highly recommend looking into them to get a better idea of what we've learned:</p>
                
                <ul class = "bullets">
                    <li>  The full pandas documentation that outlines the various functions and required parameters
                        within the pandas library can be found <a href="https://pandas.pydata.org/pandas-docs/stable/"><u>here</u></a>.</li>

                    <li>  Another important library to look into is the numpy library (we'll start using this soon), which makes working with arrays
                        much faster and easier. See documentation  
                        <a href="https://docs.scipy.org/doc/numpy/user/basics.html"><u>here</u></a>.</li>
  
                    <li>  Here's a useful "cheatsheet" I used to use that goes over commonly used pandas functionalities.  
                        <a href="https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html"><u>Cheatsheet</u></a>.
                    </li>
                </ul>
                    </p>
                </div>
                




















                <!-- DATA EXPLORATION TITLE -->
                <h1 id = "DE" class="topic_title">Feature Exploration </h1>
                <!-- DATA EXPLORATION TITLE -->
                <div id="Explore-Setup">
                        <!-- SETUP AND INSTRUCTIONS -->
                        <h1 class="post_title"> Setup and Introduction</h1>
                        <p class="lesson-text">
                            If you don't have your environment set up yet. Please reference <a href="#Configuring-Pandas"><u>this guide</u></a>. Throughout this lesson
                            we'll make use of multiple Python libraries. If you see a library you haven't previously used, simply install the library 
                            using <code>pip install</code>. For reference, here's the <code>pip install</code> <a href="https://pip.pypa.io/en/stable/reference/pip_install/"><u>documentation</u></a>.
                            Download the CSV file we'll be working with <a href="supplements/iris.csv" download><u>here</u></a>.
                            <br><br>
                            Before we get started, let's recap what we've gone through so far. We've learned how to query a database, import data,
                            manipulate data tables, and other techniques involved in cleaning and aggregating data. Now that we have our DataFrames
                            setup, we can begin exploring our variables. This initial "exploration" will help us choose and create a model later on as well
                            as give us insight into our data.
                        </p>
                </div>
                <div id="Univariate">
                        <!-- UNIVARIATE -->
                        <h1 class="post_title"> Univariate Exploration </h1>
                        <p class="lesson-text"> The first step to looking into our now "cleaned" data is
                            to perform a univariate analysis. A univariate analysis is when we look at the characteristics
                            and plots of each variable <i>individually</i> in order to gain insight into our variables as a whole. We'll
                            be making use of a few new libraries like Matplotlib and Seaborn, which you should install now if you plan
                            on following along. (See <a href="#Explore-Setup"><u>setup</u></a> for more information). The best way
                            to start looking into your data is to visualize your data. For this exercise we'll use the
                            following <a href="#Explore-Setup"><u>CSV</u></a> file. The dataset we're using is
                            the Iris dataset (read more <a href="https://archive.ics.uci.edu/ml/datasets/Iris"><u>here</u></a>). 
                            Even though this dataset is preloaded in the <code>sklearn</code> library, it's best we practice importing
                            our own datasets. Here is the code requried
                            to read in our data and get it ready for exploration. Feel free to copy and paste. <br><br>
                            <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-keyword" style="font-weight: 700; color: navy;">import</span> pandas <span class="hljs-keyword" style="font-weight: 700; color: navy;">as</span> pd
<span class="hljs-keyword" style="font-weight: 700; color: navy;">import</span> numpy <span class="hljs-keyword" style="font-weight: 700; color: navy;">as</span> np
<span class="hljs-keyword" style="font-weight: 700; color: navy;">import</span> matplotlib.pyplot <span class="hljs-keyword" style="font-weight: 700; color: navy;">as</span> plt
<span class="hljs-keyword" style="font-weight: 700; color: navy;">import</span> seaborn <span class="hljs-keyword" style="font-weight: 700; color: navy;">as</span> sns
                                
iris = pd.read_csv(<span class="hljs-string" style="color: rgb(0, 85, 0);">"iris.csv"</span>)
iris.columns=[<span class="hljs-string" style="color: rgb(0, 85, 0);">'Sepal Length'</span>, <span class="hljs-string" style="color: rgb(0, 85, 0);">'Sepal Width'</span>, <span class="hljs-string" style="color: rgb(0, 85, 0);">'Petal Length'</span>, <span class="hljs-string" style="color: rgb(0, 85, 0);">'Petal Width'</span>, <span class="hljs-string" style="color: rgb(0, 85, 0);">'Class'</span>]</pre>

                </p>
                <p class="lesson-text">
                    If you remember from the Data Preperation lessons, we learned that the <code>describe()</code> function
                    of dataframes outputs summary statistics of our data. We can do that now and also develop a couple
                    visualizations. <br><br>
                    <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-symbol" style="color: rgb(0, 85, 0);"># Defining subplots</span> 
fig,axes=plt.subplots(<span class="hljs-number" style="color: rgb(136, 0, 0);">2</span>,<span class="hljs-number" style="color: rgb(136, 0, 0);">2</span>, figsize=(<span class="hljs-number" style="color: rgb(136, 0, 0);">12</span>, <span class="hljs-number" style="color: rgb(136, 0, 0);">9</span>))
                        
<span class="hljs-symbol" style="color: rgb(0, 85, 0);"># Visualizing each numeric attribute</span> 
sns.distplot(iris[<span class="hljs-string" style="color: rgb(0, 85, 0);">'Sepal Length'</span>], ax=axes[<span class="hljs-number" style="color: rgb(136, 0, 0);">0</span>,<span class="hljs-number" style="color: rgb(136, 0, 0);">0</span>])
sns.distplot(iris[<span class="hljs-string" style="color: rgb(0, 85, 0);">'Sepal Width'</span>], ax=axes[<span class="hljs-number" style="color: rgb(136, 0, 0);">0</span>,<span class="hljs-number" style="color: rgb(136, 0, 0);">1</span>])
sns.distplot(iris[<span class="hljs-string" style="color: rgb(0, 85, 0);">'Petal Length'</span>], ax=axes[<span class="hljs-number" style="color: rgb(136, 0, 0);">1</span>,<span class="hljs-number" style="color: rgb(136, 0, 0);">0</span>])
sns.distplot(iris[<span class="hljs-string" style="color: rgb(0, 85, 0);">'Petal Width'</span>], ax=axes[<span class="hljs-number" style="color: rgb(136, 0, 0);">1</span>,<span class="hljs-number" style="color: rgb(136, 0, 0);">1</span>])
                        
<span class="hljs-symbol" style="color: rgb(0, 85, 0);"># Displaying figures</span> 
plt.show()</pre><br>
                    <img  src="images/histo.png" alt="Histograms" /> <br><br></p>
                    <p class="lesson-text">
                    These histograms give us insight into our data by showing us each distribution. For instance,
                    we can see that sepal length and sepal width display unimodality (one mode/peak), whereas petal length
                    and petal width seem more bimodal. We can go one step furthur and look into the kurtosis and skew of each attribute.
                    The kurtosis measures how close our values are to our mean, you can think of it as how "steep" our distribution slopes around
                     its mean. Skew on the otherhand shows us if we have more observations on the tails of our data, in other words,
                    it shows us if our distribution favors the left or right side. <br><br>
                    <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-function"><span class="hljs-title" style="font-weight: 700; color: navy;">print</span><span class="hljs-params">(<span class="hljs-string" style="color: rgb(0, 85, 0);">"Kurtosis"</span>)</span></span>
<span class="hljs-function"><span class="hljs-title" style="font-weight: 700; color: navy;">print</span><span class="hljs-params">(iris.kurtosis()</span></span>)
<span class="hljs-function"><span class="hljs-title" style="font-weight: 700; color: navy;">print</span><span class="hljs-params">(<span class="hljs-string" style="color: rgb(0, 85, 0);">"Skew"</span>)</span></span>
<span class="hljs-function"><span class="hljs-title" style="font-weight: 700; color: navy;">print</span><span class="hljs-params">(iris.skew()</span></span>)</pre> <br>
                    <img  src="images/kurtosis.png" alt="Kurtosis and Skew" /></p>
                    <p class="lesson-text">
                    Great! We now have our kurtosis and skew values, but what do they mean? A negative skew value indicates that
                    our data has more observations to the right of the mean, and thinner tails to the left, that's why say the distribution is left-skewed. If our skew is positive,
                    our data is considered right-skewed. However, the magnitude of the skewness values also matter. Our skewness
                    values are between -0.5 and 0.5 which means our data is relatively symmetric. A larger skewness value tends to mean
                    your data is more heavily skewed. In general a skewness value between -1 and 1 is considered acceptable (however
                    these values aren't set in stone). In terms of kurtosis, a positive kurtosis shows that our data is steep
                    near the mean, while a negative kurtosis means our "hump" is relatively flat. Like skewness, the magnitude of the
                    kurtosis value is important. The higher the magnitude, the more observations are clustered near the mean. In general, 
                    kurtosis values between -2 and 2 are considered acceptable. <br><br>
                    One more thing we can do is look at a box-plot of our data. A box-plot will show us our interquartile range (IQR)
                    which represents the range of values between the 25th and 75th percentile. In addition, a box-plot will show us if we have any outliers, 
                    something we'll learn how to deal with next.<br><br> 
                    <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-title" style="font-weight: 700; color: navy;">sns</span>.boxplot(<span class="hljs-class"><span class="hljs-keyword" style="font-weight: 700; color: navy;">data</span>=iris)</span></pre><br>
                    <img  id="Out" src="images/boxplot.png" alt="Box-plot" /></p>
                    <p class="lesson-text">
                    As you can see, we have outliers in our sepal width attribute. The outliers are denoted by the points
                    that fall outside the whiskers of the plot. There are many ways to deal with outliers, one of which is removing all entries where outliers
                    exist. However, determining what should be done with outliers depends on our model. We can train and test our model with
                    and without outliers to see performance. In general, it is a good idea to deal with outliers early to prevent
                    our model from being trained on observations that aren't representative of our dataset as a whole. In order to deal with
                    outliers, we'll first have to import stats from the scipy library and then remove outlier observations. <br><br>
                    <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-keyword" style="font-weight: 700; color: navy;">from</span> scipy <span class="hljs-keyword" style="font-weight: 700; color: navy;">import</span> stats
<span class="hljs-function"><span class="hljs-title" style="font-weight: 700; color: navy;">print</span><span class="hljs-params">(iris.size)</span></span>
<span class="hljs-keyword" style="font-weight: 700; color: navy;">for</span> column <span class="hljs-keyword" style="font-weight: 700; color: navy;">in</span> iris<span class="hljs-selector-class">.columns</span>:
    <span class="hljs-keyword" style="font-weight: 700; color: navy;">if</span> column != <span class="hljs-string" style="color: rgb(0, 85, 0);">'Class'</span>:
        iris = iris[(np.abs(stats.zscore(iris[column])))&lt;<span class="hljs-number" style="color: rgb(136, 0, 0);">3</span>]
<span class="hljs-function"><span class="hljs-title" style="font-weight: 700; color: navy;">print</span><span class="hljs-params">(iris.size)</span></span>   </pre></p>
                    <p class="lesson-text">    
                    What we've done here is removed all the outliers from our data. The way we did this was by looking at something called a z-score, which 
                    represents how many standard deviations an observation is from the mean. Generally, any observation with a z-score over 3 (3 standard deviations
                    from the mean), is considered an outlier. In this code snippet, we iterated through each column and updated our dataframe
                    with only observations which had a z-score of less than 3. What's interesting is that the size of our dataframe did not change,
                    no observations were deleted so our data had no outliers.<br><br>
                    Something you may be wondering is that why our box-plot displayed outliers while our z-score checker did not. The reason for the
                    difference is how outliers are calculated. The z-score looks at the standard deviations an observation is from the mean (read more about z-scores <a href="https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/z-score/"><u>here</u></a>.)
                    With box-plots, outliers are usually determined by looking at how many <i>IQRs</i> our data is from our mean. Data outside <code>k x IQR</code> (where <code>k</code> is usually 1.5)
                    from the quartiles denote outliers in a box-whisker plot. 
                </p>
                </div>


                <div id="Bivariate">
                        <!-- BIVARIATE -->
                        <h1 class="post_title"> Bivariate Exploration </h1>
                        <p class="lesson-text"> In our previous lesson, we learned how to look into the attributes of our dataset and
                            utilize visualizations to explore our data. Often times, we're interested in how variables relate to each other, leading to
                            bivariate and multivariate data exploration. <br><br>
                            To start off, we'll look at how our variables are correlated with each other through a <i>pairs plot</i>, a pairs plot
                            is a scatterplot of two specific attributes. We can use <code>seaborn</code> to create these.<br><br>
                            <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);">sns.<span class="hljs-title" style="font-weight: 700; color: navy;">pairplot</span>(iris, hue = <span class="hljs-string" style="color: rgb(0, 85, 0);">'Class'</span>);</pre><br>
                            <img  src="images/pairsplot.png" alt="Pairs Plot" /></p>
                            <p class="lesson-text">
                        I know the plot may look daunting, but it's very easy to understand. The points are colored based on the 'Class'
                        of each observation. We did this by passing in the Class attribute for the hue paramater. On either axis of the overall figure
                        are each of our attributes. Each individual figure corresponds to a scatter plot of those attributes. For instance, in the figure on the top row
                        second column, attribute 'Sepal Width' is on the x-axis while attribute 'Sepal Length' is on the y-axis. On the diagonal you'll notice
                        rather than scatterplots, distributions are shown. These are the distribution of each 
                        attribute for each Class. <br><br>
                        The next step is to see what insight we can gain from these figures. It seems like the iris-virginica Class 
                        tends to be the largest in terms of dimensions while the iris-setosa seems to be the smallest. Another thing you can notice is that on the
                        distributions along the diagonal, the peaks and steepnesses aren't the same across classes. This is due to different kurtosis and skew values. If you
                        look at our kurtosis and skew calculations from the previous lesson, you'll be able to visualize how these metrics translate to a distribution. Something else
                        you'll notice is that our plot for petal length and petal width seem to follow a strong linear correlation. With pair plots, we can look into how correlated
                        variables are with each other. 
                        <br><br>
                        However, there's an even better way to quantify and visualize the correlation between out attributes using Heatmaps and correlation matricies. 
                        Let's start off by getting the correlation matrix of our attributes.<br><br>
                        <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-name" style="font-weight: 700; color: navy;">print(</span>iris.corr())</pre><br>
                        <img  src="images/corrmat.png" alt="Correlation Matrix" /></p>    
                        <p class="lesson-text">           
                        Like in our pairs plot, the x and y axis show our attributes while the values show the correlation between those attributes. Obviously,
                        variables will be 100% correlated with themselves, which is why our diagonal is 1. After looking at the values you'll notice we were correct
                        in saying petal length and petal width are correlated, with a correlation of 0.96. We'll notice other variables are also strongly correlated with each other
                        like sepal length and petal length. 
                        As useful as the correlation matrix is, it'll be helpful to actually <i>visualize</i> the matrix. We can do this with a Heat Map. <br><br>
                        <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);">f, <span class="hljs-attr">ax</span> = plt.subplots(<span class="hljs-attr">figsize=(12,</span> <span class="hljs-number" style="color: rgb(136, 0, 0);">9</span>))
sns.heatmap(corr_mat, <span class="hljs-attr">vmax=1,</span> <span class="hljs-attr">square=True,</span> <span class="hljs-attr">annot=True);</span></pre><br>
                        <img  src="images/heatmap.png" alt="Heat Map" />   
                        </p>
                        <p class="lesson-text"> 
                        Our heat map shows us a visual version of our correlation matrix. Our <code>vmax</code> parameter sets the maximum value scale for our correlation. The <code>square</code> parameter
                        gives us both sides of our matrix rather than a "staircase" version. The <code>annot</code> parameter displays the numeric value for the correlation within each square.                     
                        </p>

                    </div>
                    <div id="Feature-Engineering">
                            <!-- feature-engineering -->
                            <h1 class="post_title"> Feature Engineering </h1>

                            <p class="lesson-text"> Now that we've learned a few techniques in exploring our data, it's time for us to develop the features for our model. We need to alter the features we'll use in our model as well as potentially transform
                                these features in order to help develop stronger predictive models. What concepts are considered part of feature engineering and what aren't is subjective. Think of feature engineering as the final step
                            in figuring out the best way to change our features to better be able to predict our target. When working with our dataset in this lesson, we'll be using a lot of the concepts from the Data Pre-Processing series. 
                            Instead of continuing our use of the Iris dataset, let's look into a dataset that has more features. This will help make understanding certain
                            feature engineering concepts easier (we'll go back to the Iris dataset for the next lesson). For this lesson, we'll use the Capital Bikeshare Service dataset, a dataset about a bikesharing service in the Metro DC area (download data <a href="supplements/bikeshare.csv" download><u>here</u></a>  from Kaggle). Our goal is to create a model
                            to predict bike traffic (the 'cnt' column).
                            Let's first take a look at our data.</p><br><br>
                            <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-keyword" style="font-weight: 700; color: navy;">import</span> pandas <span class="hljs-keyword" style="font-weight: 700; color: navy;">as</span> pd

bikedata = pd.read_csv(<span class="hljs-string" style="color: rgb(0, 85, 0);">'bikeshare.csv'</span>)
bikedata.head()</pre>
                                <br>
                                <img src="images/bikehead.png">
                            </p>
                            <p class="lesson-text"> 
                                You'll notice a couple things right off the bat. First, our column "instant" is just an indexing column and won't be a feature in our regression model. Since we already have dataframe indicies, we can get rid of this column.
                                Next, you'll notice that our 'dteday', 'season', 'yr', 'mnth', 'weekday', and 'workingday' attributes all give us information about the date. In our model, we should try to use as few features as possible while keeping our model accurate.
                                Given that the information from these attributes is similar, let's see which ones are the most useful. Since we're not doing a time-series analysis, the "dteday" feature can be removed. Likewise, 'yr' can also be removed.
                                The 'season' and 'mnth' attribute are related to each other since the season is determined by the month. Bike usage would probably change the most season to season rather than month to month, since seasons have much more distinguishable
                                weather factors. Thus, we can remove the 'mnth' feature as well. The 'weekday' column is related to the 'workingday' column by distinguishing between weekdays and weekends. Since traffic is more likely affected by weekday vs. weekend rather than
                                two arbitrary days of the week, we'l keep the 'workingday' feature and remove the 'weekday' feature. One feature we glossed over is the 'holiday' feature. The holiday feature has two values, 0 for not a holiday or 1 for a holiday. Let's quickly look into
                                how many holidays we have. We can do this by running.<br><br>
                                <code>bikedata[bikedata['holiday']==1].count()[0]</code>
                                <br><br>
                                You'll notice out of our 731 total entries, only 21 are holidays - thats roughly 2.5% of our dataset. One thing we haven't learned yet is the concept of training and testing sets. That's when we divide our dataset into two sets to fit our model and test its performance.
                                Having a binary feature with very few "true" instances could throw our model off during training/testing. This is because we're not sure that the holiday feature will show up evenly. Although we can get around this using stratified k-fold cross-validation (we'll also get into that later),
                                it's better for us to remove the feature altogether as it may introduce unnecessary variance.
                                <br><br>
                                Often times, you won't be lucky enough to have a dataset that already has the dates split into weekdays, seasons, etc. This dataset has clearly already been pre-processed. If you're only given dates and want to create new features. Simply create a new column
                                using conditional statements based on the date. 
                                Now let's look at our non-date data. To see a reference as to what each column signifies, go <a href="https://www.kaggle.com/marklvl/bike-sharing-dataset"><u>here</u></a>. Our 'atemp' and 'temp' features are obviously very correlated and will have 
                                <i>multicollinearity</i>. We can deal with this multicollinearity through dimensionality reduction (we'll get more into that in the next lecture), or pure feature selection. In this case, we'll just choose one of them. 'atemp' represents the temperature
                                <i>felt</i> rather than the actual temperature. Since people are more likely to base their judgements on their <i>perception</i> of the weather, we'll choose to only look at 'atemp'. We also have two features that split up the total count of bike traffic into
                                'casual', and 'registered'. Since at this point it's difficult to logically determine whether or not our features would be more correlated to one over the other, we'll choose to keep both. So now after pure logical feature selection our dataframe looks like this:<br><br>
                                <img src="images/updatedbikehead.png">
                                <br><br>
                                One more thing we can do is categorize our numerical data. Another term used for this is <i>encoding</i>. In school, 90-100 is an A, 80-89 is a B, 70-79 is a C, etc. This is an example of a <i>numerical mapping</i>,
                                where certain ranges correspond to certain categorical variables or bins. When looking at our data, one feature I think could benefit from this sort of encoding is windspeed. Most people split days into either windy or not windy when making decisions, so let's do the same.
                                We can now bin our 'windspeed' feature. It's important to make sure each of our bins has enough representation in our total dataset, this will help reduce variance (we'll get more into that later) within our model's results.
                                <br>
                                Let's first get a summary of our 'windspeed' feature using the <code>describe()</code> function.<br><br>
                                <img src="images/windhead.png"><br><br>
                                We can <i>bin</i> our 'windspeed' data assigning it to a category mapped to its quartile. Let's create a list of [min, 25th percentile, 50th percentile, 75th percentile, max]. Then we'll encode our 'windspeed'
                                into a categorical bin. <strong>NOTE: 'windspeed' will now become a categorical variable.</strong> <br><br>
                                <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);">bins = [bikedata[<span class="hljs-string" style="color: rgb(0, 85, 0);">'windspeed'</span>].describe()[<span class="hljs-number" style="color: rgb(136, 0, 0);">3</span>],bikedata[<span class="hljs-string" style="color: rgb(0, 85, 0);">'windspeed'</span>].describe()[<span class="hljs-number" style="color: rgb(136, 0, 0);">4</span>],
        bikedata[<span class="hljs-string" style="color: rgb(0, 85, 0);">'windspeed'</span>].describe()[<span class="hljs-number" style="color: rgb(136, 0, 0);">5</span>],bikedata[<span class="hljs-string" style="color: rgb(0, 85, 0);">'windspeed'</span>].describe()[<span class="hljs-number" style="color: rgb(136, 0, 0);">6</span>],bikedata[<span class="hljs-string" style="color: rgb(0, 85, 0);">'windspeed'</span>].describe()[<span class="hljs-number" style="color: rgb(136, 0, 0);">7</span>]]
bikedata[<span class="hljs-string" style="color: rgb(0, 85, 0);">'windspeed'</span>] = pd.cut(bikedata[<span class="hljs-string" style="color: rgb(0, 85, 0);">'windspeed'</span>], bins)
bikedata[<span class="hljs-string" style="color: rgb(0, 85, 0);">'windspeed'</span>].head(<span class="hljs-number" style="color: rgb(136, 0, 0);">10</span>)</pre><br>
                                <img src="images/windbin.png">
                            </p>
                            <p class="lesson-text"> 
                                As you can see in our 'windspeed' column, our speeds have been put into categories. This can help with categorical analysis in looking at regressions across various 'windspeed' bins. Something else we can do is engineer our own features
                                through something called <i>feature interaction</i>. This means we can combine columns. For instance, let's say 'weathersit' was not given to us as a feature. We could calculate it by creating a new column called 'weathersit', and
                                assigning values based on conditional statements around windspeed, humidity, temperature, etc. We could say if the temperature is within some range, the wind is below some value, and the humidity is between a certain range, that the 'weathersit' is "2" or "okay".
                                Like I mentioned earlier, the dataset we imported has already been preprocessed by the uploader and no features like this need to be created.
                                <br><br>
                                One important concept we skimmed over is <i>multicollinearity</i>. Multicollinearity is a fancy way of saying our features are correlated with each other. For instance, the season and temperature features may be correlated since temperatures vary according to season.
                                Likewise, the weather situation ('weathersit') could be correlated with one or more of our weather features. One method we'll go into in our <a href="#Ridge" ><u>LASSO lesson</u></a> is using L1 regularization for feature selection to address this multicollinearity. In the next lesson,
                                we'll go through a common model used for dimensionality reduction called a Principal Component Analysis (PCA).


                            </p>
                        
                        
                        </div>

                    <div id="PCA">
                                <!-- pca -->
                                <h1 class="post_title"> Principal Component Analysis with Singular Value Decomposition</h1>

                                <p class="lesson-text"> In the previous lesson, we engineered our features in our bikeshare dataset and made it more ready for modeling. In this lesson we'll use a 
                                    Principal Component Analysis (PCA) for dimensionality reduction. A PCA can help us visualize our dataset when we have many features and can also help us with dimensionality reduction. Before we get into how to implement a PCA.
                                    Let's understand how it works. Before I get started I highly recommend checking out StatQuest's video on PCA. 
                                    It's only 20 minutes and is extremely useful. Much of what I'll go over is expanded on in-depth in his video (<a href="https://www.youtube.com/watch?v=FgakZw6K1QQ"><u>here</u></a>).<br>
                                    <br>
                                    Shown below is a scatterplot of height and weight of various men (blue) and women (pink). <br>
                                    <img src="images/pca1.png" ><br>
                                    What we'll do next is find the overall average weight and height and shift our axis so that
                                    (Average Weight, Average Height) is our <i>new origin</i>. Our plot now looks something like this: <br><br>
                                    <img src="images/pca2.png" ><img src="images/pca3.png" ><br>
                                    Our next step will be to find a line of best fit for our shifted plot. This will be called the first <strong>principal component</strong> (PC1).
                                    The unit vector of this principal component is known as the <strong>eigenvector</strong> or <strong>singular vector</strong> for PC<sub>1</sub>.<br>
                                    How do we find that line? We'll use this diagram to help:<br><br>
                                    <img src="images/pca4.png" ><br>
                                    In this diagram, the blue circle represents the data point while the green line represents PC<sub>1</sub>. The
                                    red line represents the distance from the point to the origin, we'll call this distance 'c'. It's important to note that 
                                    this distance never changes, regardless of where the line is. The purple line represents the distance the point is from the 
                                    principal component. One way to find the line of best fit is to try to minimize 'a'. This is usually done by minimizing the sum of the distances
                                    squared. That value is known as the <strong>eigenvalue</strong> for that principal component and the square root of the eigenvalue is called the <strong>singular value</strong> for that 
                                    principal component.<br>
                                    One thing you'll see is that side lengths a,b, and c form a right triangle. 'c', as we mentioned earlier, is constant. According to the Pythagorean Theorem
                                    a<sup>2</sup> + b<sup>2</sup> = c <sup>2</sup>. This means that minimizing 'a' is the same thing as maximizing 'b'. A PCA maximizes 'b' by maximizing
                                    the sum squared of 'b' across all datapoints.<br><br>
                                    Once the PC<sub>1</sub> is found, PC<sub>2</sub> is simply the best-fit line perpendicular to PC<sub>1</sub>. If we had more features,
                                    we could add even more principal components, all of which would be the best-fit line orthogonal to all the others. We'll stick with two dimensions for the sake of
                                    simplicity. Our PC<sub>1</sub> and PC<sub>2</sub> look something like this.<br><br>
                                    <img src="images/pca5.png" ><br>
                                    Now we'll rotate our plot so that PC<sub>1</sub> becomes our horizontal axis. Our plot now looks something like this:<br><br>
                                    <img src="images/pca6.png" ><br>
                                    If we had more than two features, to create the 2-D plot, we simply project the points from the other principal components on to PC<sub>1</sub>
                                    and PC<sub>2</sub>. <br><br>
                                    If you take the eigenvalue and divide it by (n-1), you get the variation from that principal component. You can compare the varations between different
                                    components to see which ones account for more variability within the dataset. A graph of these variations is called a <strong>scree plot</strong>
                                    and can be useful to determine how many features to keep in our final model.
                                    <br><br>
                                    Now that we understand the process of principal component analysis using singular value decomposition (PCA using SVD), let's look into how we can implement it.
                                    First, make sure you have your iris dataset properly loaded into a dataframe using the <code>read_csv</code> function. (To see the full
                                    instructions on setting up go <a href="#Univariate"><u>here</u></a>. The CSV file
                                    can be downloaded <a href="supplements/iris.csv" download><u>here</u></a>).<br><br>
                                    We'll start by standardizing our features. This means setting the mean and variance to 0 and 1 respectively. We can do this by subtracting the mean from each point (something we did above), and then dividing by the
                                    standard deviation. Luckily, sklearn makes this easy for us. <br><br>
                                    <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);">x = iris<span class="hljs-selector-class">.loc</span>[:, [<span class="hljs-string" style="color: rgb(0, 85, 0);">'Sepal Length'</span>, <span class="hljs-string" style="color: rgb(0, 85, 0);">'Sepal Width'</span>, <span class="hljs-string" style="color: rgb(0, 85, 0);">'Petal Length'</span>, <span class="hljs-string" style="color: rgb(0, 85, 0);">'Petal Width'</span>]]<span class="hljs-selector-class">.values</span>
y = iris<span class="hljs-selector-class">.loc</span>[:,[<span class="hljs-string" style="color: rgb(0, 85, 0);">'Class'</span>]]<span class="hljs-selector-class">.values</span>
x = StandardScaler().fit_transform(x)</pre> </p>
                                        <p class="lesson-text">
                                        All we're doing here is seperating our features from our target variable ('Class'), and standardizing our features (subtracting means and dividing by standard deviations).
                                        After we do this we can start our PCA. <br><br>
                                        <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-attr">pca</span> = PCA(<span class="hljs-number" style="color: rgb(136, 0, 0);">2</span>)
<span class="hljs-attr">projected</span> = pca.fit_transform(x)
<span class="hljs-attr">pca_x</span> = projected[:, <span class="hljs-number" style="color: rgb(136, 0, 0);">0</span>]
<span class="hljs-attr">pca_y</span> = projected[:, <span class="hljs-number" style="color: rgb(136, 0, 0);">1</span>]</pre>


                                </p> 
                                <p class="lesson-text">
                                        pca_x holds our PC<sub>1</sub> values while pca_y holds our PC<sub>2</sub> values. Remember, we originally had four features and now
                                        we only have two. Now let's visualize these components. <br><br>
                                        <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);">color_dict = {<span class="hljs-string" style="color: rgb(0, 85, 0);">'Iris-setosa'</span>:<span class="hljs-string" style="color: rgb(0, 85, 0);">'red'</span>,<span class="hljs-string" style="color: rgb(0, 85, 0);">'Iris-virginica'</span>:<span class="hljs-string" style="color: rgb(0, 85, 0);">'green'</span>,<span class="hljs-string" style="color: rgb(0, 85, 0);">'Iris-versicolor'</span>:<span class="hljs-string" style="color: rgb(0, 85, 0);">'blue'</span>}
color_list = [color_dict[label] <span class="hljs-keyword" style="font-weight: 700; color: navy;">for</span> <span class="hljs-selector-tag" style="font-weight: 700; color: navy;">label</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">in</span> iris[<span class="hljs-string" style="color: rgb(0, 85, 0);">'Class'</span>]]
plt.scatter(pca_x, pca_y,c=color_list, edgecolor=<span class="hljs-string" style="color: rgb(0, 85, 0);">''</span>, alpha=<span class="hljs-number" style="color: rgb(136, 0, 0);">0.5</span>)
plt.xlabel(<span class="hljs-string" style="color: rgb(0, 85, 0);">'Principal Component 1'</span>)
plt.ylabel(<span class="hljs-string" style="color: rgb(0, 85, 0);">'Principal Component 2'</span>)
plt.show()</pre><br>
                                            <img src="images/pca7.png">
                            
                                </p>
                                <p class="lesson-text">
                                    PCA is very useful for visualizing data as well as reducing the number of features we have to explore. One more thing we can do is use the line
                                    <code>pca.explained_variance_ratio_</code>. This returns a list showing the variance ratio (as we discussed earlier), for each principal component.
                                    This can help show the importance of different components and help determine how many features to include. 

                                </p>

                            </div>
                                
                    <div id="Explore-Resources">
                                <!-- USEFUL RESOURCES -->
                                <h1 class="post_title"> Useful Resources </h1>

                                <p class="lesson-text"> In this lesson, we learned how to explore our data and understand how our variables relate
                                    to each other. We went through a few functionalities of the seaborn and pandas libraries and I highly recommend learning how to
                                    tweak paramaters for various visualizations. 
                                </p>
                                
                                <ul class = "bullets">
                                    <li>  The following are the documentations for the functionalities we learned about. <br> <a href="https://seaborn.pydata.org/api.html"><u>Seaborn API</u></a> | 
                                    <a href="https://seaborn.pydata.org/generated/seaborn.distplot.html#seaborn.distplot"><u>distplot()</u></a> | <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.kurtosis.html"><u>kurtosis()</u></a> | 
                                    <a href="https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.DataFrame.skew.html"><u>skew()</u></a> | <a href="https://pythonbasics.org/seaborn_boxplot/"><u>boxplot()</u> | <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.zscore.html"><u>zscore()</u> | 
                                    <a href="https://seaborn.pydata.org/generated/seaborn.pairplot.html"><u>pairplot()</u></a> | <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html"><u>corr()</u></a> | <a href="https://seaborn.pydata.org/generated/seaborn.heatmap.html"><u>heatmap()</u></a>.            
                                    </li>
                
                                    <li>  Here's a helpful guide provided by the University of Minnesota on covariance and correlation. 
                                        <a href="http://users.stat.umn.edu/~helwig/notes/datamat-Notes.pdf"><u>Link</u></a>.</li>
                
                                </ul>

                                    </p>

                                    </div>
                                











                                    









                <!-- DATA MODELING TITLE -->
                <h1 id = "DM" class="topic_title">Data Modeling </h1>
                <!-- DATA MODELING TITLE -->
                <div id="Model-Setup">
                        <!-- SETUP AND INSTRUCTIONS -->
                        <h1 class="post_title"> Setup and Introduction</h1>
                        <p class="lesson-text">
                            If you don't have your environment set up yet. Please reference <a href="#Configuring-Pandas"><u>this guide</u></a>. Throughout this lesson
                            we'll make use of multiple Python libraries. If you see a library you haven't previously used, simply install the library 
                            using <code>pip install</code>. For reference, here's the <code>pip install</code> <a href="https://pip.pypa.io/en/stable/reference/pip_install/"><u>documentation</u></a>.
                            <br><br>
                            The next concept we'll be going over is modeling our data. Even though modeling data is extremely important, it's important to understand
                            that a majority of your time will be spent cleaning and exploring your data and thus, should be practiced heavily. In this series, we'll go over
                            some commonly used models, how they work, how to implement them, and when to use them. Feel free to use your own datasets when following along.
                        </p>
                </div>

                        <div id="Train-Test">
                                <!-- TRAIN AND TEST -->
                                <h1 class="post_title"> Training and Testing Sets</h1>
                                <p class="lesson-text">
                                    Before we start learning how to use models, it's important to understand what it means to "train" and "test" in the context of machine learning.
                                    When you "train" your model, you are fitting your model to a part of your dataset. "Testing" your data is when you feed the other part of your dataset
                                    into your newly trained model in order to see how it performs. You can see how it performs by comparing your model's forecasts with the actual values in your dataset.
                                    We'll be going much more in-depth on how to do all this in later lectures. Figuring out your train/test ratio is important to prevent under or overfitting.
                                
                                    Training your model too much on your training set may cause the model to be <i>overfitted</i> while not training your model enough may cause the model to be <i>underfitted</i>. A technique
                                    used to avoid overfitting is using another part of your data called a <i>validation set</i>. The validation set is really part of your training set - we'll get more into this in the next lesson. <br><br>
                                    You'll hear a lot of recommended train/test ratios like 70/30, 80/20, or 75/25 and train/test/validation ratios of 70/15/15 or 60/20/20. If your dataset is smaller, 
                                    a validation set may not be helpful and a 70/30 train/test split could work. With larger datasets, using cross-validation can help with fitting the model. We'll split our iris dataset from the previous lessons below.
                                    <br><br>
                                    <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-built_in" style="font-weight: 700; color: navy;">from</span> sklearn.model_selection <span class="hljs-built_in" style="font-weight: 700; color: navy;">import</span> train_test_split
X = iris.<span class="hljs-built_in" style="font-weight: 700; color: navy;">drop(columns=['Class'])</span>
X_train, X_test, y_train, y_test = <span class="hljs-built_in" style="font-weight: 700; color: navy;">train_test_split</span>(X, iris[<span class="hljs-string" style="color: rgb(0, 85, 0);">'Class'</span>], <span class="hljs-built_in" style="font-weight: 700; color: navy;">test_size</span>=0.3)</pre>
                                    </p>
                                    <p class="lesson-text">
                                        <code>X_train</code> represents the inputs we'll be training our model on, while <code>X_test</code> 
                                        represents the inputs we'll be using for testing our model. Similarly, <code>y_train</code> and <code>y_test</code>
                                        are both the outputs we'll be using for training and testing respectively. The <code>train_test_split</code> function splits our <code>iris</code> dataset, with <code>iris['Class']</code>  being the target variable, into a training and testing set according to the ratio in the <code>test_size</code> paramater.
                                        <br><br>
                                        One of the reasons <code>train_test_split</code> is more useful than just manually splitting a Series is because it defaults to <i>shuffling</i> our data. This means instead of just taking the first 70% of your dataset as your training set, it'll shuffle your dataset first to prevent
                                        potential bias based on how the data was prepared or inputted into your database. The <code>train_test_split</code> function can also <i>stratify</i> our data. <br><br>
                                        <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);">X_train, X_test, y_train, y_test = <span class="hljs-built_in" style="font-weight: 700; color: navy;">train_test_split</span>(X, iris[<span class="hljs-string" style="color: rgb(0, 85, 0);">'Class'</span>], <span class="hljs-built_in" style="font-weight: 700; color: navy;">stratify</span>=iris[<span class="hljs-string" style="color: rgb(0, 85, 0);">'Class'</span>], <span class="hljs-built_in" style="font-weight: 700; color: navy;">test_size</span>=0.3)</pre>
                                        
                                    </p>
                                    <p class="lesson-text">
                                        Stratifying our training sets allows the model to be trained on a subset more representative of the dataset as a whole. In our case, our target variable is 'Class'. There are four iris 'Class' categories
                                        each with a certain share of the total dataset. When a stratified training set is created, it <i>maintains</i> that share of each 'Class' when making the training set. This is especially useful in smaller datasets.
                                    </p>                         
                                    
                </div>
                <div id="Cross-Validation">
                        <!-- CROSS-VALIDATION -->
                        <h1 class="post_title"> Cross Validation</h1>
                        <p class="lesson-text">
                            Cross-Validation is a method to reduce overall bias in our model's predictions. We'll be focusong on <code>k-fold cross-validation</code>. Let's say we took 70% of our data for training and 30% for testing.
                            We would only be training our model on one specific training and testing set, this may not be the best option because our model might perform better on specific subsets of our data compared to other subsets. How can we 
                            get around this? One solution is to use <i>multiple</i> training and testing sets and then looking at the <i>overall</i> result across <i>all</i> sets. This makes it much easier to compare different machine learning models
                            because we'll have a better idea of how each model would perform across multiple testing sets. Luckily, this is one of the main benefits of <code>k-fold cross-validation</code>. <br><br>
                            K-fold cross validation works by dividing out dataset into k subsets, and running our learning algorithm k times, with each bin being the testing set for a different iteration. I'll give you an example.
                            Let's say our dataset had 1000 observations. We could use a k-fold cross-validation method with k=10 that would divide our dataset into k=10 bins, each of size 100. Good so far? Instead of then training our model in one iteration,
                            we'll use 9 of those bins to train our data and the last bin left over to test our data. Going back to basic combinatorics, there are only 10 ways to choose 9 training subsets and 1 testing subset from a set of
                            10 bins, this is why it runs 10 times.
                            <br><br>
                            When k=10, like in the example above, it is known as <i>10-fold cross-validation</i>. Usually we use k=10 for cross-validation, however, like when we divided our data into training and testing sets, it all depends on how much data you have. Here's a useful visualization
                            from <a href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)#/media/File:K-fold_cross_validation_EN.svg"><u>Wikipedia</u></a> that shows how a k-fold validation would split our dataset.<br><br>
                            <img  src="images/kfoldval.png" alt="K-Fold Cross-Validation" /><br><br>
                            Now let's look into how we can implement a <code>k-fold cross-validation</code>.<br><br>
                            <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-keyword" style="font-weight: 700; color: navy;">from</span> sklearn<span class="hljs-selector-class">.model_selection</span><span class="hljs-keyword" style="font-weight: 700; color: navy;"> import </span>KFold
folds = <span class="hljs-keyword" style="font-weight: 700; color: navy;">KFold</span>(n_splits=<span class="hljs-number" style="color: rgb(136, 0, 0);">10</span>)
<span class="hljs-keyword" style="font-weight: 700; color: navy;">for</span> train_index, test_index <span class="hljs-keyword" style="font-weight: 700; color: navy;">in</span> folds.split(iris):
    X_train, X_test, y_train, y_test = X<span class="hljs-selector-class">.iloc</span>[train_index], X<span class="hljs-selector-class">.iloc</span>[test_index], iris[<span class="hljs-string" style="color: rgb(0, 85, 0);">'Class'</span>]<span class="hljs-selector-class">.iloc</span>[train_index], iris[<span class="hljs-string" style="color: rgb(0, 85, 0);">'Class'</span>]<span class="hljs-selector-class">.iloc</span>[test_index]</pre><br><br>
                                Now we'll have our training and testing sets ready. Another common type of cross-validation is Leave-One-Out Cross Validation (LOOCV). LOOCV is a type of k-fold cross-validation where k = n, or the total number of data points in our dataset. This is when <i>each</i> of our observations will
                                individually be a testing set when training our model.
                                Obviously, as k increases, the computation time increases as well, since more training and testing will have to occur. This is why LOOCV is only recommended for small datasets. When choosing your k, it's important to keep in mind the size of your dataset
                                as well as the complexity of your model. One other reason a high k may not be advisable is due to the bias-variance tradeoff. Something we'll get into in our next lesson.
                            </p>
                            </div>
                <div id="Bias-Variance">
                        <!-- Bias-Variance -->
                        <h1 class="post_title"> Bias-Variance Tradeoff</h1>
                        <p class="lesson-text">
                            When choosing your model, it's important to keep the bias-variance tradeoff in mind. Let's look into what bias and variance are.
                            <br><br>
                            <strong>Bias</strong><br>
                            Bias is how well our model's predictions compare to the expected values. One way to think of it is like throwing darts at a dartboard where the center is your expected output. 
                            If your model had low bias, the darts would be around the center - meaning we did a pretty good job <i>on average</i> in predicting our output. However, we're also interested in decreasing the <i>variability</i>
                            in those predictions. In the dart analogy, that's like making the darts we threw closer together. This brings us to <i>variance</i>.<br><br>
                            <strong>Variance</strong><br>
                            When training a model, if the model becomes overfitted on our training set, it may perform very well when fitting that specific subset of data, but may not perform well when tested on
                            a different set of data. You can think of the variation in how our model performs on these sets of data as the <i>variance</i> of our model.
                            Ideally, we want to reduce the variance in order to have more precise outputs. When throwing darts, if the darts are close together, you have low variance whereas if the darts we threw
                            are far apart, we have high variance. Here's a <a href="https://www.pinterest.com/pin/514043744937666589/?lp=true"><u>visualization</u></a> of the dart analogy.<br>
                            <img  src="images/biasvariability.png" alt="Dart Analogy" /><br><br>
                            When making our model, if we train our model too much on our training set it becomes <i>overfitted</i>. This means it will perform very well on the training set but not perform well when tested. The variation in how it performs
                            between these two sets is why we say it has <i>high variance</i>. If our model is <i>underfitted</i> that means we haven't fit our model well and we have <i>high bias</i>. Here's a figure to help out. <br><br>
                            <img  src="images/underoverfit.png" alt="Under and Overfitting" /><br><br>
                            Now that we know what bias and variance are and how they relate to our model's fit, we can look into the trade-off. When using an overly-simplistic model, we'll have high bias since our model doesn't fit our data well. To lower this bias, we can
                            try using a more complex model. However, the more complex our model is and the more it fits our training data, the more likely it is to have high variance when tested on a testing set. A popular <a id='biass' href="https://www.dataquest.io/blog/learning-curves-machine-learning/"><u>curve</u></a> you'll see that depicts this relationship is shown below. <br><br>
                            <img src="images/biascurve.png" alt="Bias-Variance Tradeoff" /><br><br>
                            Like the figure shows, the goal is to find the optimal balance between bias and variance. This may be a bit confusing since we haven't learned any models yet, but keep this in mind 
                            as you go through the series.

                            </p>
                        </div>

                <div id="Loss-Functions">
                        <!-- Loss Functions and Optimizers -->
                        <h1 class="post_title"> Loss Functions and Optimizers</h1>
                        <p class="lesson-text">
                            You might be wondering why we're learning all these concepts <i>before</i> we've even started using any models.
                            The reason is because learning how to fit various models is just one part of data analysis. The important part
                            is understanding how the models work and how to optimize and gain insight from them. So bear with me, we'll get to ML models soon. <br>
                            Let's first learn what a loss function and optimizer are, and then look into some that are commonly used. Let's start with the loss function.<br>
                            <strong id="MAE"> NOTE: We will not be going through the code required for these loss functions as it would be much easier to understand the code when implementing them into a model (we'll do this in our modeling series).</strong>
                            <br><br>
                            <strong><u>Regression Loss Functions - MAE (L1), MSE (L2) and Huber</u></strong><br><br>
                            As you remember from our train-test lesson. When we use a supervised machine learning model, we split our data into a training set and a testing set.
                            We then talked about how a model "trains" on the training set and then tests itself on the testing set. How does the model know if it guessed correctly?
                            The model determines its error using a <i>loss function</i>. Let's see some examples. <br><br>

                            <strong><u>Regression Loss Functions: MAE (L1) and MSE (L2) Loss Functions</u></strong><br><br>
                            For regression, two commonly used loss functions are Mean Squared Error (MSE) or L2 and Mean Absolute Error (MAE) or L1. MSE works by taking the average squared difference
                            between the predicted values and expected values (we kinda went over this in our PCA with SVD lesson). MAE works similarly except only looks at the average absolute value, or magnitude, of the difference between predicted
                            and expected values. One major difference between these two loss functions is that MSE tends to punish values that are far very harshly, since the
                            error is proportional to the difference squared. MAE on the otherhand does not punish outliers more since the error is proportional to the distance. We'll be revisiting these topics
                            when we get into <a id = "Huber" href="#Ridge"><u>Ridge and LASSO</u></a> regression, as they use MSE and MAE for regularization (we'll learn about that later).<br><br>
                            <strong><u>Regression Loss Functions: Huber Loss Function</u></strong><br><br>
                            <br>Another loss function is the <strong>Huber Loss Function</strong>. You can think of a Huber loss function as a sort of combination of L1 (MAE) and L2 (MSE). 
                            When using a Huber loss function, you define a &delta;. This &delta; represents a <i>tolerance</i> in terms of error in our predictions. If our error is less than this &delta;
                            we use a <i>quadratic</i> loss function. This just means our loss function has a degree of 2 - like MSE. However, like we learned earlier, MSE punishes outliers heavily since
                            loss is dependent on the error squared. Huber counters this by using a different loss function of degree 1 - or <i>linear</i>, when our error is greater than &delta;. This is called a <i>piecewise function</i>.
                            Here's the equation from <a href="https://en.wikipedia.org/wiki/Huber_loss"><u>Wikipedia</u></a>, where <code>f(x)</code> represents our expected value.<br><br>
                            <img src="images/huber.png"><br><br>
                            This concept is similar to how Elastic Net regularization builds on L1 and L2 regularization, something we'll get into in our Regression lesson series.<br>
                            Here are some popular regression loss functions:<br><br>
                            <img  id="Log" src="images/lossfunctions.png"><br><br><br>
                            <strong><u>Classification Loss Functions: Binary Cross-Entropy (Logarithmic)</u></strong><br><br>
                            When working with classification data, we have to use different loss functions that work with our "classes".
                            We'll be working with other classification loss functions like <strong>Hinge Loss</strong> later on. We'll skip it for now because we'll cover it once we learn SVMs.<br><br>
                            When we have a binary classification problem, our goal is to create a model that 
                            takes in our features and outputs either 0 or 1, representing our two classes. We'll learn more about these in our classification section, but it's a good idea to understand the basic premise.
                            When we feed our features into our model, let's say we use a logistic regression model, we can get an output between 0 and 1 for each observation that corresponds to the class. For example, when feeding in our features for a specific 
                            observation, our model may output 0.94. If our expected value was 1, we weren't too far off, but if our expected value was 0, we have some work to do. Binary Cross-Entropy is useful because it treats these two cases differently ("binary" because there are two classes).
                            When the error is large, like if our expected out was 0 in our previous example, binary cross-entropy would penalize these predictions <i>heavily</i>. To better illustrate this, take a look at the log loss function
                            <a href="https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html"><u>graph</u></a>. when the true value is 1. The x-axis represents our model's predictions, and the y-axis represents the corresponding log error associated with that prediction. <br><br>
                            <img src="images/crossentropy.png"><br><br>
                            
                            As you can see, the log loss function penalizes predictions that are way off <i>much more</i> than predictions that are slightly off. Intuitively, this should make sense. Let's take a look at the <strong>cost function</strong>. <br><br>
                            <img src="images/binaryeqn.png"><br><br>
                            An important distinction to make is the difference between a <i>loss</i> function and a <i>cost</i> function. A <strong>loss function</strong> is the the error we calculate for a <i>single observation</i>, whereas
                            a <strong>cost function</strong> is the <i>average</i> of our loss function across our entire dataset. Knowing that the average is just the sum divided by the total number of observations, we can see that the loss function for cross-entropy
                            is everything inside the summation. Let's make sense of what each variable represents. y<sub>i</sub> represents the observation's target class, either 0 or 1. The p(y<sub>i</sub>) represents our model's probability <i>prediction</i> of
                            that observation. Remember that all our probabilities are between 0 and 1 and the logarithm of a value between 0 and 1 goes from 0 to negative infinity. In practice, it's common to maximize the negative log rather minimize the log loss function.
                            Minimizing the log of a function is the same as maximizing the negative log of that function. <br><br>
                            One thing you'll notice aboutis our coefficients are y<sub>i</sub> and (1-y<sub>i</sub>), and as we mentioned earlier, y<sub>i</sub> can only have values 0 or 1. This means that only one of these terms is used when calculating the cross-entropy loss for an
                            observation. Why is that so? Imagine we have an observation whose target is 0. Our error would simply be our model's output (y<sub>i</sub>) minus zero, or, y<sub>i</sub>. If our target was 1, our error would be 1 minus our model's output (y<sub>i</sub>), or
                            , (1-y<sub>i</sub>). The cross-entropy loss function will prove extremely useful when we start using neural networks and other classification models. <br><br>
                            
                            Now that we know what a loss function is, we can look into how to <i>minimize</i> it. We can minimize our loss function by using
                            certain optimizers.
                            </p>

                            </div>
                <div id="Optimizers">
                        <!-- Optimizers -->
                        <h1 class="post_title">Optimizers</h1>
                        <p class="lesson-text">
                            Like our last lesson, this lesson will focus on the conceptual and mathematical understanding of certain optimizers rather than
                            their programatic implementation. We'll be implementing these optimizers throughout our machine learning models series.<br><br>
                            Optimization functions are commonly used in conjunction with loss functions. If you remember from our loss functions lesson, a loss function represents the error
                            of a model's prediction. Obviously, when we fit our model to our training set, we're interested in trying to minimize our error. This is where optimizers come in.
                            Optimizers are functions whose primary purpose is to minimize or maximize another function - in our case, a <i id="GD">loss function.</i> <br>
                            <br><br>
                            <strong><u>(Vanilla) Gradient Descent (GD)</u></strong><br><br>
                            An optimization algorithm you'll definitely hear about and use is gradient descent. To understand gradient descent, we'll go through
                            how it works. Let's say we're trying to use gradient descent for the linear model: <br><br>
                            <a href="https://www.codecogs.com/eqnedit.php?latex=y&space;=&space;\Theta&space;_{0}&space;&plus;&space;\Theta&space;_1x" target="_blank"><img src="https://latex.codecogs.com/png.latex?y_{i}&space;=&space;\Theta&space;_{0}&space;&plus;&space;\Theta&space;_1x_{i}" title="y = \Theta _{0} + \Theta _1x" /></a><br><br>
                            Our loss function (RSS) is:<br><br>
                            <a href="https://www.codecogs.com/eqnedit.php?latex=RSS&space;=&space;\sum_{i=1}^{n}&space;(\widehat{y}_{i}-y_{i})^2" target="_blank"><img src="https://latex.codecogs.com/png.latex?RSS&space;=&space;\sum_{i=1}^{n}&space;(\widehat{y}_{i}-y_{i})^2" title="RSS = \sum_{i=1}^{n} (\widehat{y}_{i}-y_{i})^2" /></a><br><br>
                            Let's imagine that we substituted two random values for &theta;<sub>0</sub> and &theta;<sub>1</sub> into our linear model 
                            and then calculated our RSS.
                            We could graph the point (&theta;<sub>0</sub>, &theta;<sub>1</sub>, RSS) on a 3-D plot. Here's an example of what that plot might look like:<br><br>
                            <img src="images/gd.png"><br><br>
                            You can imagine the plane the "bowl" sits on as our (&theta;<sub>0</sub>, &theta;<sub>1</sub>) plane, making out third axis (the one coming up) 
                            our RSS axis. Remember that our goal is to minimize our loss function, graphically, that means we want to find the "bottom" of our bowl. This is called a minima.
                            The two types of minima are local and absolute. A local minimum is a minimum relative to surrounding points whereas
                            an absolute minimum is the minimum for the entire plot. You can think of it like your local star basketball player, a <i>local</i> legend, and Kobe Bryant,
                            an <i>absolute</i> legend (RIP Kobe). In the diagram above, our local minimum is our absolute minimum.<br><br>
                            Finding a minimum means 
                            we have to find where our plot bottoms out, or in other words, where the slope and deriviative <i>are zero</i>. We can take our RSS loss function
                            and take the partial derivative with respect to each of our &theta;'s to see how the deviations
                            in any specific parameter affect the RSS keeping while <i>all else constant</i>. These expressions are the rates of change of our RSS function with respect to our paramaters,
                            in this case &theta;<sub>0</sub> and &theta;<sub>1</sub>. Remember that a derivative of a summation
                            is simply the derivative of each term added together, and when taking the partial with respect to &theta;<sub>0 or 1</sub>, the other &theta; can be treated as a constant. <br>
                            Okay, so now what?
                            <br><br>
                            We'll start off by giving arbitrary values for &theta;<sub>0</sub> and &theta;<sub>1</sub>. You can imagine our gradient descent algorithm plotting that point
                            on a graph (&theta;<sub>0</sub>, &theta;<sub>1</sub>, RSS) and then jumping to another &theta;<sub>0</sub>, &theta;<sub>1</sub>, calculating the model's RSS with 
                            the new paramaters, and plotting that point before repeating the process. Our gradient descent algorithm tries to take steps or jumps towards the minimum
                            by iteratively adjusting &theta;<sub>0</sub> and &theta;<sub>1</sub> until the step sizes are as close to 0 as possible, or a certain step limit 
                            has been reached. 
                            It might be hard to visualize so here's a gif from <a href="https://media.giphy.com/media/O9rcZVmRcEGqI/giphy.gif"><u>GIPHY</u></a> that might help out:<br><br>
                            <img src="https://media.giphy.com/media/O9rcZVmRcEGqI/giphy.gif" height="300px" width="750px">
                            
                            <br><br>
                            Here is how we will mathematically update our weights.<br><br>
                            <a href="https://www.codecogs.com/eqnedit.php?latex=\Theta&space;_{1}'&space;=\Theta&space;_{1}-\alpha&space;\frac{\partial&space;}{\partial&space;\Theta&space;_{1}}RSS(\Theta&space;_{1},&space;\Theta&space;_{2})" target="_blank"><img src="https://latex.codecogs.com/png.latex?\Theta&space;_{0}'&space;=\Theta&space;_{0}-\alpha&space;\frac{\partial&space;}{\partial&space;\Theta&space;_{0}}RSS(\Theta&space;_{0},&space;\Theta&space;_{1})" title="\Theta _{1}' =\Theta _{1}-\alpha \frac{\partial }{\partial \Theta _{1}}RSS(\Theta _{1}, \Theta _{2})" /></a><br>
                            <a href="https://www.codecogs.com/eqnedit.php?latex=\Theta&space;_{1}'&space;=\Theta&space;_{1}-\alpha&space;\frac{\partial&space;}{\partial&space;\Theta&space;_{1}}RSS(\Theta&space;_{1},&space;\Theta&space;_{2})" target="_blank"><img src="https://latex.codecogs.com/png.latex?\Theta&space;_{1}'&space;=\Theta&space;_{1}-\alpha&space;\frac{\partial&space;}{\partial&space;\Theta&space;_{1}}RSS(\Theta&space;_{0},&space;\Theta&space;_{1})" title="\Theta _{1}' =\Theta _{1}-\alpha \frac{\partial }{\partial \Theta _{1}}RSS(\Theta _{1}, \Theta _{2})" /></a>
                            <br><br>The equation pretty much says that our paramater's updated value should be its previous value <i>minus</i> how much a change in our parameter would affect our RSS, scaled by our learning rate &alpha;.
                            &theta;' represents our updated value and &alpha; represents our <i>learning rate</i>. Our learning rate decides how meticulous our gradient descent algorithm should be in terms of step size.
                            If our learning rate is too small, it will take a long time for our algorithm to converge, if our rate is too large, we may overshoot and end with infinite error. This is why determining the learning rate is vital. 
                            Trying out different rates and using a fixed value is a good way to start. Another common practice is to start with a larger learning rate and then decrease it as your model approaches its optimal solution. 
                            This is helpful because it allows our model to work quickly in the beginning with larger steps while being more precise around the minima.<br><br>
                            Gradient descent is very useful but has some drawbacks. One major drawback is working with Big Data. When we have a lot of features and a lot of observations, our gradient descent algorithm will have to calculate the partial deriviatves
                            for <i>each parameter</i> in <i id = "SGD">each observation</i>, this can be very time-costly. Luckily, SGD helps with this problem.
                            
                            
                            
                            <br><br>
                            <strong><u>Stochastic Gradient Descent (SGD) and Mini Batch Gradient Descent</u></strong><br><br>
                            Stochastic Gradient Descent (SGD), is a variation of the <a href="#GD"><u>Gradient Descent</u></a> algorithm. In our gradient descent algorithm, we
                            used <i>all</i> our observations when finding our errors before changing our weights. With a lot of data, this becomes too time-costly. 
                            SGD randomly chooses a <strong>single</strong> sample in each iteration to calculate our error and partial derivatives. This makes our computation n times faster, n being our
                            dataset size.<br><br>
                            SGD minimizes our loss much faster but has a lot more noise and can oscillate around the minimum. If we wanted to get the best of both worlds, we could use
                            <strong>Mini Batch Gradient Descent</strong>. Our vanilla gradient descent algorithm took our entire dataset as a "batch," while SGD took one sample as a "batch". 
                            Mini Batch Gradient Descent takes a <i>subset</i> of our dataset as a batch in each iteration to adjust our model's weights. This improves the computational complexity of vanilla gradient descent
                            while <i id="Momentum">decreasing</i> the variance from stochastic gradient descent.
                            <br><br>
                            <strong><u>Momentum</u></strong><br><br>
                            The first extension of gradient descent we'll look into is <strong>momentum</strong>. One issue with gradient descent is its ability to distinguish
                            local minima from absolute minima. SGD helps with this by introducing a random process during training to "jump" around more.
                            However, SGD does not do well in cases where one dimension is changing at a much different rate than another. To see how momentum
                            can help lessen the chance of stopping at a local minimum, let's look at the following diagram. <br><br>
                            <img src="images/momentum.png"><br><br>
                            As you can see from this diagram, there's a small hiccup at the local minimum which our SGD algorithm may confuse as the absolute minimum. One thing you'll notice is even though there's a small bump,
                            the overall trend seems to be downward, or, the overall <i>momentum</i> seems to be downward. If you don't get what I mean by this, imagine a tennis ball at the top of that chart.
                            If we were to let the ball roll, when it gets to the bottom of the first hump (local minimum), it'll already have <i>momentum</i> to carry it over to the next hump.
                            This is in essence what our momentum algorithm is doing with steps. Now the question is, how can we incorporate this into our algorithm? 
                            Well, let's first revisit at our update function from gradient descent: <br><br>
                            <img src="https://latex.codecogs.com/gif.latex?\Theta&space;_{j}'&space;=\Theta&space;_{j}-\alpha&space;\frac{\partial&space;L&space;}{\partial&space;\Theta&space;_{j}}" title="\Theta _{j}' =\Theta _{j}-\alpha \frac{\partial L }{\partial \Theta _{j}}" /><br><br>
                            As we discussed earlier, this update function works by taking our previous gradient and looking at how much it changed since our last epoch. If we wanted to see our momentum, we want to also take into account
                            how our gradient changed the epoch earlier as well, and before that, and so on. Our momentum algorithm takes our gradient descent update function one step furthur
                            by also including how much the gradient changed the epoch <i>before the last</i>. This introduces an additional term into our update function and will make it look something like this: <br><br>
                            <a href="https://www.codecogs.com/eqnedit.php?latex=\Theta&space;_{j}'&space;=\Theta&space;_{j}(t)-\alpha&space;\frac{\partial&space;L&space;}{\partial&space;\Theta&space;_{j}}(t)-\beta\cdot&space;\alpha\frac{\partial&space;L&space;}{\partial&space;\Theta&space;_{j}}(t-1)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Theta&space;_{j}'&space;=\Theta&space;_{j}(t)-\alpha&space;\frac{\partial&space;L&space;}{\partial&space;\Theta&space;_{j}}(t)-\beta\cdot&space;\alpha\frac{\partial&space;L&space;}{\partial&space;\Theta&space;_{j}}(t-1)" title="\Theta _{j}' =\Theta _{j}(t)-\alpha \frac{\partial L }{\partial \Theta _{j}}(t)-\beta\cdot \alpha\frac{\partial L }{\partial \Theta _{j}}(t-1)" /></a><br><br>
                            Each time the parameter is updated, the previous change in gradient is multipled by a factor &beta; between 0 and 1 to reduce its weight. This allows our algorithm to take recent updates into account <i id="AdaGrad">more</i> than
                            changes from older updates. 
                            <br><br>
                            <strong><u>AdaGrad</u></strong><br><br>
                            Before we jump into the Adam algorithm, we have to look into two other algorithms called AdaGrad and RMSProp.
                            We'll start with the <strong>Adaptive Gradient Algorithm (AdaGrad)</strong>. To understand AdaGrad, let's revisit our gradient descent update function. This time, instead of writing "RSS" we'll use "L" (Loss).<br><br>
                            <a href="https://www.codecogs.com/eqnedit.php?latex=\Theta&space;_{j}'&space;=\Theta&space;_{j}-\alpha&space;\frac{\partial&space;L&space;}{\partial&space;\Theta&space;_{j}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Theta&space;_{j}'&space;=\Theta&space;_{j}-\alpha&space;\frac{\partial&space;L&space;}{\partial&space;\Theta&space;_{j}}" title="\Theta _{j}' =\Theta _{j}-\alpha \frac{\partial L }{\partial \Theta _{j}}" /></a><br>
                            <br>This can be rewritten as: <br><br>
                            <a href="https://www.codecogs.com/eqnedit.php?latex=\Delta&space;\Theta&space;_{j}&space;=-\alpha&space;\frac{\partial&space;L&space;}{\partial&space;\Theta&space;_{j}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Delta&space;\Theta&space;_{j}&space;=-\alpha&space;\frac{\partial&space;L&space;}{\partial&space;\Theta&space;_{j}}" title="\Delta \Theta _{j} =-\alpha \frac{\partial L }{\partial \Theta _{j}}" /></a><br><br>
                            You'll notice that our &alpha; is constant and doesn't depend on our parameter estimates. AdaGrad improves on gradient descent by <i>decreasing</i> the learning rate as
                            the model gets closer to the solution. It does this by taking the &alpha; from the equation above and dividing it by an expression every epoch to yield: <br><br>
                            <a href="https://www.codecogs.com/eqnedit.php?latex=\Delta&space;\Theta&space;_{j}&space;=-\frac{\alpha}{\sqrt{G_{j}}&plus;\epsilon&space;}&space;\cdot&space;\frac{\partial&space;L&space;}{\partial&space;\Theta&space;_{j}}" target="_blank"><img src="https://latex.codecogs.com/png.latex?\Delta&space;\Theta&space;_{j}&space;=-\frac{\alpha}{\sqrt{G_{j}}&plus;\epsilon&space;}&space;\cdot&space;\frac{\partial&space;L&space;}{\partial&space;\Theta&space;_{j}}" title="\Delta \Theta _{j} =-\frac{\alpha}{\sqrt{G_{j}}+\epsilon } \cdot \frac{\partial L }{\partial \Theta _{j}}" /></a><br><br>
                            The only difference between this and our previous equation is the denominator of the square root of G<sub>j</sub> plus some epsilon. The epsilon isn't very important as its
                            main function is to prevent division by 0. One thing you should take note of is that each parameter will have its own adaptive learning rate, which is very useful
                            in minimizing our overall time-cost. So anyway, what is G<sub>j</sub>? You can think of G<sub>j</sub> as our scaling factor for our learning rates. The larger our value for G<sub>j</sub>,
                            the smaller our learning rate and vice versa. G<sub>j</sub> is calculated as follows: <br><br>
                            <a href="https://www.codecogs.com/eqnedit.php?latex=G_{j}'&space;=G_{j}&space;&plus;&space;(\frac{\partial&space;L&space;}{\partial&space;\Theta&space;_{j}})^2" target="_blank"><img src="https://latex.codecogs.com/png.latex?G_{j}'&space;=G_{j}&space;&plus;&space;(\frac{\partial&space;L&space;}{\partial&space;\Theta&space;_{j}})^2" title="G_{j}' =G_{j} + (\frac{\partial L }{\partial \Theta _{j}})^2" /></a><br><br>
                            When we start, our G<sub>j</sub> is 0 so our &alpha;'s denominator is simply &epsilon;. There are some important things to notice. The first thing is that since G<sub>j</sub> starts at 0 and a squared value (positive) is being
                            added, our G<sub >j</sub> value will increase every epoch as more and more positive terms are added. This means our denominator for our &alpha; will continually decrease and after a certain point, our learning rate will be <i id="RMS">too small</i> for our algorithm
                            to effectively change our parameters. <br><br>
                            <strong><u>RMSProp</u></strong><br><br>
                            RMSProp tries to build on AdaGrad by solving the issue of the learning rate continuously decreasing. To understand RMSProp, we'll compare it to AdaGrad and see how it differs. If you remember our update function for 
                            AutoGrad, it looked something like this:<br><br>
                            <img src="https://latex.codecogs.com/png.latex?\Delta&space;\Theta&space;_{j}&space;=-\frac{\alpha}{\sqrt{G_{j}}&plus;\epsilon&space;}&space;\cdot&space;\frac{\partial&space;L&space;}{\partial&space;\Theta&space;_{j}}" title="\Delta \Theta _{j} =-\frac{\alpha}{\sqrt{G_{j}}+\epsilon } \cdot \frac{\partial L }{\partial \Theta _{j}}" /><br><br>
                            For our RMSProp, we'll be using the same parameter update function. The only difference will be how we calculate our G<sub>j</sub> term. In our AutoGrad algorithm, we used
                            the following function:<br><br>
                            <img src="https://latex.codecogs.com/png.latex?G_{j}'&space;=G_{j}&space;&plus;&space;(\frac{\partial&space;L&space;}{\partial&space;\Theta&space;_{j}})^2" title="G_{j}' =G_{j} + (\frac{\partial L }{\partial \Theta _{j}})^2" /><br><br>
                            This function says that our updated G<sub>j</sub> will be its previous value plus a proportion of how much it changed. Our RMSProp update function for G<sub>j</sub>, uses
                            the <i>exponentially weighted moving average</i> (EWMA) of these two terms. It does this by multiplying each term by factors of &beta; and (1-&beta;) respectively like thus:<br><br>
                            <a href="https://www.codecogs.com/eqnedit.php?latex=G_{j}'&space;=\beta\cdot&space;G_{j}&space;&plus;&space;(1-\beta)\cdot(\frac{\partial&space;L&space;}{\partial&space;\Theta&space;_{j}})^2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?G_{j}'&space;=\beta\cdot&space;G_{j}&space;&plus;&space;(1-\beta)\cdot(\frac{\partial&space;L&space;}{\partial&space;\Theta&space;_{j}})^2" title="G_{j}' =\beta\cdot G_{j} + (1-\beta)\cdot(\frac{\partial L }{\partial \Theta _{j}})^2" /></a><br><br>
                            The beauty of adding the hyperparameter &beta; is that now the G<sub>j</sub>' function is not costantly increasing. This means that our effective learning rate, <a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\alpha}{\sqrt{G_{j}}&plus;\epsilon&space;}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\alpha}{\sqrt{G_{j}}&plus;\epsilon&space;}" title="\frac{\alpha}{\sqrt{G_{j}}+\epsilon }" /></a> will
                            not continuously decrease, offering a solution to AdaGrad's issue. Luckily, there's a way to combine these optimizers into a new optimization algorithm called
                            <strong id="Adam">Adam</strong>.

                            <br><br>
                            <strong><u>Adam</u></strong><br><br>
                            Adam, or Adaptive Moment Estimation, is an algorithm that builds on all of these optimizers. Adam is a relatively new algoritm published in 2015 and is
                            known as an extremely effective optimizer. The Adam optimizer works by combining the advantages of Momentum, AdaGrad, and RMSProp. Just a quick review, let's take a look
                            at our parameter update functions for our momentum algorithm and our AdaGrad/RMSProp algorithm. <br><br>
                            <strong>Momentum</strong><br>
                            <a href="https://www.codecogs.com/eqnedit.php?latex=\Theta&space;_{j}'&space;=\Theta&space;_{j}(t)-\alpha&space;\frac{\partial&space;L&space;}{\partial&space;\Theta&space;_{j}}(t)-\beta\cdot&space;\alpha\frac{\partial&space;L&space;}{\partial&space;\Theta&space;_{j}}(t-1)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Theta&space;_{j}'&space;=\Theta&space;_{j}(t)-\alpha&space;\frac{\partial&space;L&space;}{\partial&space;\Theta&space;_{j}}(t)-\beta\cdot&space;\alpha\frac{\partial&space;L&space;}{\partial&space;\Theta&space;_{j}}(t-1)" title="\Theta _{j}' =\Theta _{j}(t)-\alpha \frac{\partial L }{\partial \Theta _{j}}(t)-\beta\cdot \alpha\frac{\partial L }{\partial \Theta _{j}}(t-1)" /></a>
                            <br><br>
                            <strong>AdaGrad/RMSProp</strong><br>
                            <a href="https://www.codecogs.com/eqnedit.php?latex=\Delta&space;\Theta&space;_{j}&space;=-\frac{\alpha}{\sqrt{G_{j}}&plus;\epsilon&space;}&space;\cdot&space;\frac{\partial&space;L&space;}{\partial&space;\Theta&space;_{j}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Delta&space;\Theta&space;_{j}&space;=-\frac{\alpha}{\sqrt{G_{j}}&plus;\epsilon&space;}&space;\cdot&space;\frac{\partial&space;L&space;}{\partial&space;\Theta&space;_{j}}" title="\Delta \Theta _{j} =-\frac{\alpha}{\sqrt{G_{j}}+\epsilon } \cdot \frac{\partial L }{\partial \Theta _{j}}" /></a>
                            <br><br>
                             Our AdaGrad update function says the amount a paramater &theta;<sub>j</sub> should be changed is related to a modified learning rate multiplied by
                             our previous epoch's change in the loss function with respect to &theta;<sub>j</sub>. However, what if we wanted to look at more than just the previous change? Well, this is where we can add
                             momentum into our algorithm. We'll start by making our parameter update proportional to the modified learning rate multiplied by some expression for <i>momentum M<sub>j</sub></i>, like so: <br><br>
                             <a href="https://www.codecogs.com/eqnedit.php?latex=\Delta&space;\Theta&space;_{j}(t)&space;=-\frac{\alpha}{\sqrt{G_{j}(t)}&plus;\epsilon&space;}&space;M_{i}(t)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Delta&space;\Theta&space;_{j}(t)&space;=-\frac{\alpha}{\sqrt{G_{j}(t)}&plus;\epsilon&space;}&space;M_{j}(t)" title="\Delta \Theta _{j}(t) =-\frac{\alpha}{\sqrt{G_{j}(t)}+\epsilon } M_{i}(t)" /></a>
                            <br><br>
                            Just a quick reminder, the t represents our epoch. This makes it easier to understand which gradient lags we'll be using in our updates. Now that we have our updated update function, let's see how we can calculate our momentum term M<sub>j</sub>.
                            With momentum, our goal is to look at how our gradient has been changing while placing more weight on recent changes than earlier changes. Our G'<sub>j</sub> update function in RMSProp used an EWMA
                            for its gradients. It looked like this: <br><br>
                            <a href="https://www.codecogs.com/eqnedit.php?latex=G_{j}'&space;=\beta\cdot&space;G_{j}&space;&plus;&space;(1-\beta)\cdot(\frac{\partial&space;L&space;}{\partial&space;\Theta&space;_{j}})^2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?G_{j}'&space;=\beta\cdot&space;G_{j}&space;&plus;&space;(1-\beta)\cdot(\frac{\partial&space;L&space;}{\partial&space;\Theta&space;_{j}})^2" title="G_{j}' =\beta\cdot G_{j} + (1-\beta)\cdot(\frac{\partial L }{\partial \Theta _{j}})^2" /></a><br><br>
                            We can modify this to include momentum by using the update function to update M<sub>j</sub> rather than G<sub>j</sub>. Our new momentum term update function will look like this: <br><br>
                            <a href="https://www.codecogs.com/eqnedit.php?latex=M_{j}'&space;=\beta\cdot&space;M_{j}&space;&plus;&space;(1-\beta)\cdot(\frac{\partial&space;L&space;}{\partial&space;\Theta&space;_{j}})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?M_{j}'&space;=\beta\cdot&space;M_{j}&space;&plus;&space;(1-\beta)\cdot(\frac{\partial&space;L&space;}{\partial&space;\Theta&space;_{j}})" title="M_{j}' =\beta\cdot M_{j} + (1-\beta)\cdot(\frac{\partial L }{\partial \Theta _{j}})" /></a>
                            <br>
                            Note: Our M<sub>j</sub> starts at 0 for our first epoch.<br>
                            Let's look at a helpful <a href="https://ruder.io/content/images/2016/09/saddle_point_evaluation_optimizers.gif"><u>gif</u></a> that shows how these optimizers compare in a specific instance (in this case on what's called a <i>saddle point</i>):
                            <br>
                            <img src="https://ruder.io/content/images/2016/09/saddle_point_evaluation_optimizers.gif">
                            <br><br>
                            We started with gradient descent, saw how we could add <strong>Momentum</strong>, then saw how we could alter learning rates with <strong>AdaGrad</strong>, then built on AdaGrad to include moving
                            averages for updates using <strong>RMSProp</strong>, and then built on top of <i> all of those </i> to get to our <strong>Adam</strong> optimizer.
                            <strong>Data science is a field where methodologies improve on each other through advancements in research.</strong> Data science and machine learning are still in its infancy and optimization models
                            are constantly being developed and researched. Understanding basic concepts and methodologies will help us eventually be able to begin <i>improving</i> these algorithms. One step at a time.
                            </p>
                        </div>
                            
                <div id="MLE">
                        <!-- MLE -->
                        <h1 class="post_title"> Maximum Likelihood Estimation for Parameterization</h1>
                        <p class="lesson-text">
                         We've gone over quite a few algorithms and will be going through quite a few models as well. One thing all statistical models have are <i>paramaters</i>.
                         Parameters are the inputs we give our model to best fit our data. For instance, in a linear regression model in the form: <br><br>
                         <a href="https://www.codecogs.com/eqnedit.php?latex=y=a_{0}&space;&plus;&space;a_{1}x_{1}&space;&plus;&space;a_{2}x_{2}&space;...&space;&plus;&space;a_{n}x_{n}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y=a_{0}&space;&plus;&space;a_{1}x_{1}&space;&plus;&space;a_{2}x_{2}&space;...&space;&plus;&space;a_{n}x_{n}" title="y=a_{0} + a_{1}x_{1} + a_{2}x_{2} ... + a_{n}x_{n}" /></a><br><br>
                         Our parameters are our a<sub>n</sub>'s. In order to find our parameters, we looked into various loss and optimization functions. 
                         For a linear regression model, ordinary least squares (OLS) can be used to estimate our parameters. 
                         It basically tries to find what values for our parameters will minimize our total squared error. We'll learn soon that OLS is just a special case of MLE where the error is assumed to be normal.
                         <br><br>
                         MLE tries to find what values for a probabilitity distribution's parameters <i>maximize</i> the <i>likelihood</i> of us observing our data. This might sound a bit confusing so 
                         let's breakdown what likelihood is. You might think it's the same thing as probability. You're half-right. A likelihood is really just another way of phrasing a probability. When working with
                         MLE, 
                         we can think of probability as the chance of observing certain data in our model given certain paramaters.<br><br>
                         <a href="https://www.codecogs.com/eqnedit.php?latex=P(x&space;|&space;\Theta&space;_{1},\Theta&space;_{2}...\Theta&space;_{n})" target="_blank"><img src="https://latex.codecogs.com/png.latex?P(x&space;|&space;\Theta&space;_{1},\Theta&space;_{2}...\Theta&space;_{n})" title="P(x | \Theta _{1},\Theta _{2}...\Theta _{n})" /></a>
                         <br><br>Whereas with likelihoods, we look at the chance
                         of our model having certain parameters <i>given</i> our data. We can write this like so:<br><br>
                         <a href="https://www.codecogs.com/eqnedit.php?latex=L(\Theta&space;_{1},\Theta&space;_{2}...\Theta&space;_{n}&space;|&space;x)" target="_blank"><img src="https://latex.codecogs.com/png.latex?L(\Theta&space;_{1},\Theta&space;_{2}...\Theta&space;_{n}&space;|&space;x)" title="L(\Theta _{1},\Theta _{2}...\Theta _{n} | x)" /></a>
                         <br><br>
                         Now let's figure out how we can logically find a distribution's parameters. One way is by trying different values for parameters and try to maximize the 
                         likelihood of that parameter producing our data. That sounds pretty logical right? Well luckily, that's how an MLE works. The equation for likelihood should now
                         make more sense. <br><br>
                         <img src="images/mleeqn.png" height="80px" width="200px">
                         <br><br>
                         The &Pi; symbol represents the product of a set of numbers. You can think of it like a &Sigma; for multiplication.
                         Let's make sense of this equation. The equation is saying that the likelihood of observing some parameter &theta; is 
                         a product of the probabilities of observing each of our observations given that parameter. You might be wondering why we're
                         taking the product. Well, if you remember, the probability of two <i>independent</i> events, A and B, occuring is 
                         P(A) x P(B). In this case, the probability of a certain observation being produced by a certain parameter is independent of the probability
                         of a <i>different</i> observation being produced by that same parameter. Since we're assuming independence, we can say that the overall likelihood of observing a parameter
                         is just the product of the probabilities of observing each sample with that parameter. Obviously, our goal should be to maximize this value, but how do we do that? <br><br>
                         Maximizing the likelihood function is difficult to do because taking the derivative of a bunch of products would require extensive
                         chain rules. One way to go about this is by looking at the log of the likelihood, otherwise known as, wait for it, the
                         <strong>log-likelihood</strong>. This will work because a logarithm is an increasing function.
                         That means maximizing our log is equivalent to maximizing our original function, which was also an increasing function. Our log-likelihood function looks something like this:
                         <br><br>
                         <img src="images/mleeqn2.png" height="80px" width="250px">
                         <br><br>
                         Let's say we have our data and want to fit a <i>normal distribution</i> to it. Our first step would be to figure out what the parameters are.
                         A normal distribution is a distribution that is symmetric around the mean and whose mean, median and mode are the same. The parameter's we'd need then
                         would only be &mu; (our mean) and &sigma; (our standard deviation). This is what the probability density function (PDF) of
                         a normal distribution looks like given a single observation: <br><br>
                         <a href="https://www.codecogs.com/eqnedit.php?latex=F(x|\mu&space;,\sigma)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-(x-\mu)^2/2\sigma^2}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(x|\mu&space;,\sigma)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-(x-\mu)^2/2\sigma^2}" title="P(x|\mu ,\sigma)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-(x-\mu)^2/2\sigma^2}" /></a><br><br>
                         We can rewrite this as a likelihood. <br><br>
                         <a href="https://www.codecogs.com/eqnedit.php?latex=L(\mu&space;,\sigma|x)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-(x-\mu)^2/2\sigma^2}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?L(\mu&space;,\sigma|x)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-(x-\mu)^2/2\sigma^2}" title="L(\mu ,\sigma|x)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-(x-\mu)^2/2\sigma^2}" /></a><br><br>
                         Like we mentioned earlier, taking the derivative of a &Pi; operator is very difficult. Taking the logarithm of our likelihood (called <strong>log-likelihood</strong>) changes the products
                         to additions, making computation much easier. This is what our log-likelihood would look like (we'll use a natural log). <br><br>
                         <a href="https://www.codecogs.com/eqnedit.php?latex=ln(L(\mu&space;,\sigma|x_{1},x_{2},...x_{n}))=&space;\sum_{i=1}^{n}&space;ln(\frac{1}{\sqrt{2\pi\sigma^2}}e^{-(x_{i}-\mu)^2/2\sigma^2})" target="_blank"><img src="https://latex.codecogs.com/png.latex?ln(L(\mu&space;,\sigma|x_{1},x_{2},...x_{n}))=&space;\sum_{i=1}^{n}&space;ln(\frac{1}{\sqrt{2\pi\sigma^2}}e^{-(x_{i}-\mu)^2/2\sigma^2})" title="ln(L(\mu ,\sigma|x_{1},x_{2},...x_{n}))= \sum_{i=1}^{n} ln(\frac{1}{\sqrt{2\pi\sigma^2}}e^{-(x_{i}-\mu)^2/2\sigma^2})" /></a>
                         <a href="https://www.codecogs.com/eqnedit.php?latex==\sum_{i=1}^{n}&space;[ln(\frac{1}{\sqrt{2\pi\sigma^2}})&plus;-\frac{(x_{i}-\mu)^2}{2\sigma^2}]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?=\sum_{i=1}^{n}&space;[ln(\frac{1}{\sqrt{2\pi\sigma^2}})&plus;-\frac{(x_{i}-\mu)^2}{2\sigma^2}]" title="=\sum_{i=1}^{n} [ln(\frac{1}{\sqrt{2\pi\sigma^2}})+-\frac{(x_{i}-\mu)^2}{2\sigma^2}]" /></a><br><br>                     
                         We can continue to simplify the expression inside the summation to yield: <br><br>
                         <a href="https://www.codecogs.com/eqnedit.php?latex=\sum_{i=1}^{n}&space;[-\frac{1}{2}ln(2\pi&space;)-ln(\sigma&space;)-\frac{(x_{i}-\mu)^2}{2\sigma^2}]&space;=[-\frac{n}{2}ln(2\pi&space;)-nln(\sigma&space;)-\sum_{i=1}^{n}\frac{(x_{i}-\mu)^2}{2\sigma^2}]" target="_blank"><img src="https://latex.codecogs.com/png.latex?\sum_{i=1}^{n}&space;[-\frac{1}{2}ln(2\pi&space;)-ln(\sigma&space;)-\frac{(x_{i}-\mu)^2}{2\sigma^2}]&space;=[-\frac{n}{2}ln(2\pi&space;)-nln(\sigma&space;)-\sum_{i=1}^{n}\frac{(x_{i}-\mu)^2}{2\sigma^2}]" title="\sum_{i=1}^{n} [-\frac{1}{2}ln(2\pi )-ln(\sigma )-\frac{(x_{i}-\mu)^2}{2\sigma^2}] =[-\frac{n}{2}ln(2\pi )-nln(\sigma )-\sum_{i=1}^{n}\frac{(x_{i}-\mu)^2}{2\sigma^2}]" /></a><br><br>
                         Our next step is to <i>maximize</i> this expression. We can do this by setting the partial derivative of of log-likelihood function with respect to each of our parameters to zero. <br><br>
                         <a href="https://www.codecogs.com/eqnedit.php?latex={\frac{\partial&space;}{\partial&space;\mu}}{}[-\frac{n}{2}ln(2\pi&space;)-nln(\sigma&space;)-\sum_{i=1}^{n}\frac{(x_{i}-\mu)^2}{2\sigma^2}]=0" target="_blank"><img src="https://latex.codecogs.com/png.latex?{\frac{\partial&space;}{\partial&space;\mu}}{}[-\frac{n}{2}ln(2\pi&space;)-nln(\sigma&space;)-\sum_{i=1}^{n}\frac{(x_{i}-\mu)^2}{2\sigma^2}]=0" title="{\frac{\partial }{\partial \mu}}{}[-\frac{n}{2}ln(2\pi )-nln(\sigma )-\sum_{i=1}^{n}\frac{(x_{i}-\mu)^2}{2\sigma^2}]=0" /></a><br>
                         <a href="https://www.codecogs.com/eqnedit.php?latex={\frac{\partial&space;}{\partial&space;\sigma}}{}[-\frac{n}{2}ln(2\pi&space;)-nln(\sigma&space;)-\sum_{i=1}^{n}\frac{(x_{i}-\mu)^2}{2\sigma^2}]=0" target="_blank"><img src="https://latex.codecogs.com/png.latex?{\frac{\partial&space;}{\partial&space;\sigma}}{}[-\frac{n}{2}ln(2\pi&space;)-nln(\sigma&space;)-\sum_{i=1}^{n}\frac{(x_{i}-\mu)^2}{2\sigma^2}]=0" title="{\frac{\partial }{\partial \sigma}}{}[-\frac{n}{2}ln(2\pi )-nln(\sigma )-\sum_{i=1}^{n}\frac{(x_{i}-\mu)^2}{2\sigma^2}]=0" /></a><br>
                         

                         <br>
                         Simplifying our partials yield the following expressions for our parameters &mu; and &sigma;.<br><br>
                         <a href="https://www.codecogs.com/eqnedit.php?latex=\mu&space;=&space;\frac{1}{n}\sum_{i=1}^{n}x_{i}" target="_blank"><img src="https://latex.codecogs.com/png.latex?\mu&space;=&space;\frac{1}{n}\sum_{i=1}^{n}x_{i}" title="\mu = \frac{1}{n}\sum_{i=1}^{n}x_{i}" /></a><br>
                         <a href="https://www.codecogs.com/eqnedit.php?latex=\sigma&space;=&space;\sqrt{\frac{\sum_{i=1}^{n}(x_{i}-\mu)^2}{n}}" target="_blank"><img src="https://latex.codecogs.com/png.latex?\sigma&space;=&space;\sqrt{\frac{\sum_{i=1}^{n}(x_{i}-\mu)^2}{n}}" title="\sigma = \sqrt{\frac{\sum_{i=1}^{n}(x_{i}-\mu)^2}{n}}" /></a>
                         <br><br>
                         You should recognize these expressions. This shows us that the value for &mu;, our distributions mean, is the mean of our data and our
                         &sigma;, or distribution's standard deviation, should be our sample's standard deviation. We were able to derive these expressions for our mean
                         and standard deviation through MLE.
                        </p>
                </div>
                <div id="Linear-Regression">
                        <!-- Linear-Regression -->
                        <h1 class="post_title"> Linear Regression</h1>
                        <p class="lesson-text">
                            The first model we'll look into is linear regression. Linear regression works by trying to find the <i>line of best fit</i>. Or, the line that minimizes the error between
                            our predictions and what the values actually are. Here's an animation to help out.<br><br>
                            <img  src="images/linreg.gif" alt="Linear Regression Animation" /><br><br>
                            The best way to understand linear regression is to understand what a linear regression algorithm is trying to do. A linear equation takes the form: <br><br>
                            <a href="https://www.codecogs.com/eqnedit.php?latex=y=a_{0}&space;&plus;&space;a_{1}x_{1}&space;&plus;&space;a_{2}x_{2}&space;...&space;&plus;&space;a_{n}x_{n}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y=a_{0}&space;&plus;&space;a_{1}x_{1}&space;&plus;&space;a_{2}x_{2}&space;...&space;&plus;&space;a_{n}x_{n}" title="y=a_{0} + a_{1}x_{1} + a_{2}x_{2} ... + a_{n}x_{n}" /></a><br><br>
                            Where a<sub>0</sub> represents the y-intercept, or when our linear equation crosses the y-axis. This is usually refered to as the <i>constant</i> or <i>shift</i>. The coefficients
                            a<sub>1,2...,n</sub> represent the weights given to each of our features (independent variables) x<sub>1,2,...,n</sub>. The larger the weight, the more important
                            that feature is in predicting our output. Our overall goal is to minimize the <i>cost function</i>, how we go about optimizing our model, whether through OLS or gradient descent, will be covered
                            in the next lesson. Our cost function looks something like this: <br><br>
                            <a href="https://www.codecogs.com/eqnedit.php?latex=J&space;=&space;\frac{1}{n}\sum_{i=1}^{n}(p_{i}&space;-&space;y_{i})^{2}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?J&space;=&space;\frac{1}{n}\sum_{i=1}^{n}(p_{i}&space;-&space;y_{i})^{2}" title="J = \frac{1}{n}\sum_{i=1}^{n}(p_{i} - y_{i})^{2}" /></a><br><br>
                            This equation is what we call the <strong>Mean Squared Error (MSE)</strong>. <code>J</code> represents the cost, what we're trying to minimize. While <code>p<sub>i</sub></code> is our prediction and <code>y<sub>i</sub></code> is the actual value. What we're doing is looking at the
                            difference in what our model predicted and what the actual output was, squaring it, and averaging it across our <code>n</code> observations. No wonder it's called Mean Squared Error! Minimzing our cost function (<code>J</code>), is how we determine the coefficients
                            a<sub>0</sub>, a<sub>1</sub>, ..., a<sub>n</sub>, in our final model. Like I said earlier, we'll be going over how to minimize the cost-function in our next lesson.<br><br>
                            Now let's look into how we can implement a linear regression model. The dataset we'll be using is based on data collected from the Capital Bikeshare Service, a bikesharing service in the Metro DC area (download <a href="supplements/bikeshare.csv" download><u>here</u></a>). Our goal will be to build a regression
                            model that relates the features we choose to the total bikes being used on a given day. We'll start by importing our libraries and loading/preparing our training and testing sets. <br><br>
                            <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-keyword" style="font-weight: 700; color: navy;">import</span> pandas <span class="hljs-keyword" style="font-weight: 700; color: navy;">as</span> pd
from sklearn.model_selection <span class="hljs-keyword" style="font-weight: 700; color: navy;">import</span> train_test_split
from sklearn.linear_model <span class="hljs-keyword" style="font-weight: 700; color: navy;">import</span> LinearRegression 
from sklearn.metrics <span class="hljs-keyword" style="font-weight: 700; color: navy;">import</span> r2_score
                                
bikedata = pd.read_csv(<span class="hljs-string" style="color: rgb(0, 85, 0);">'bikeshare.csv'</span>)[[<span class="hljs-string" style="color: rgb(0, 85, 0);">'workingday'</span>,<span class="hljs-string" style="color: rgb(0, 85, 0);">'weathersit'</span>, <span class="hljs-string" style="color: rgb(0, 85, 0);">'temp'</span>, <span class="hljs-string" style="color: rgb(0, 85, 0);">'atemp'</span>, <span class="hljs-string" style="color: rgb(0, 85, 0);">'hum'</span>, <span class="hljs-string" style="color: rgb(0, 85, 0);">'windspeed'</span>, <span class="hljs-string" style="color: rgb(0, 85, 0);">'cnt'</span>]]
X_train, X_test, y_train, y_test = train_test_split(bikedata.iloc[:,:-<span class="hljs-number" style="color: rgb(136, 0, 0);">1</span>], bikedata[<span class="hljs-string" style="color: rgb(0, 85, 0);">'cnt'</span>], test_size=<span class="hljs-number" style="color: rgb(136, 0, 0);">0.3</span>)</pre>
                        </p>
                        <p class="lesson-text">
                            Instead of choosing all attributes, we've decided to only look at a few (this is an arbitrary decision, to know how to deal with features, I recommend looking at the
                            Feature Engineering lesson). We split out data
                            into a training and testing set. If you'd like to use cross-validation, you can look into <a href="#Cross-Validation"><u>this lesson</u></a>.
                            Next, we'll fit our linear regression model, look at how it performs, and develop our linear equation. <br><br>
                            <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);">lr = LinearRegression()
lr.fit(X_train,y_train)
y_pred = lr.predict(X_test)
                                    
<span class="hljs-function"><span class="hljs-title" style="font-weight: 700; color: navy;">print</span><span class="hljs-params">(<span class="hljs-string" style="color: rgb(0, 85, 0);">"R-Squared: "</span>, r2_score(y_test, y_pred)</span></span>)
<span class="hljs-function"><span class="hljs-title" style="font-weight: 700; color: navy;">print</span><span class="hljs-params">(<span class="hljs-string" style="color: rgb(0, 85, 0);">"Weights: "</span>,lr.coef_)</span></span>
<span class="hljs-function"><span class="hljs-title" style="font-weight: 700; color: navy;">print</span><span class="hljs-params">(<span class="hljs-string" style="color: rgb(0, 85, 0);">"Constant: "</span>,lr.intercept_)</span></span></pre><br>
                            <img  src="images/rsquared.png" alt="R-Squared" />
                        </p>
                        <p class="lesson-text">
                            The first line shows our R-Squared value. An R-Squared value is a correlation coefficient that tells us how well our model
                            performed when predicting during the testing phase. When looking at the error of our model, we can call the regression error 
                            the difference between our prediction and our expected output, and call total error the difference between our prediction and the average output.
                            When you subtract the sum of your regression error squared divided by the sum of your total error squared from 1, you arrive at your R-Squared value. Our next outputs
                            are the coefficients and intercepts of our model. Remember a linear model takes the form: <br>
                            <a href="https://www.codecogs.com/eqnedit.php?latex=y=a_{0}&space;&plus;&space;a_{1}x_{1}&space;&plus;&space;a_{2}x_{2}&space;...&space;&plus;&space;a_{n}x_{n}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y=a_{0}&space;&plus;&space;a_{1}x_{1}&space;&plus;&space;a_{2}x_{2}&space;...&space;&plus;&space;a_{n}x_{n}" title="y=a_{0} + a_{1}x_{1} + a_{2}x_{2} ... + a_{n}x_{n}" /></a><br>
                            The coefficients outputted correspond to the weights for each of our features. One thing you'll notice is that certain weights are higher than others, for instance, 'weathersit', 'temp', 'atemp', 'windspeed' seem to have the highest weights which means our model
                            predicts they have model the target variable better, or in other words have more "sway", in the outcome. If we were to run the regression with just these four variables,
                            our R-Squared wouldn't be too much different - it may even be worth it. This idea is the basis of Ridge, LASSO, and Elastic Net regression, models we'll get into
                            in the next lesson. 
                            </p>


                        </div>


                <div id="Ridge">
                        <!-- Ridge, LASSO, Elastic Net -->
                        <h1 class="post_title"> Ridge, LASSO, and Elastic Net</h1>
                        <p class="lesson-text">
                            <strong>Ridge Regression</strong><br><br>
                            When using linear regression, especially with small datasets, the chance of overfitting your model is high. This is because although there is low bias, with a smaller training set,
                            the variability in how your model performs on testing sets will increase. In order to counter overfitting, we can use various regularization techniques used by Ridge and LASSO regression.
                            They pretty much work by introducing some bias in the model in order to lower the variability of how the model performs on test sets. This is especially useful when the coefficients
                            in your linear regression model are far apart. This is where Ridge Regression comes in. <br><br>
                            Ridge Regression uses something called <strong>L2 Regularization</strong>. Regularization is used to reduce overfitting. 
                            L2 Regularization reduces model complexity and helps bring the weights in our model closer to zero, in essence, decreasing variance and shifting us more left on the bias-variance curve.
                            If you're confused, take a look <a href="#biass"><u>here</u></a>. Linear Regression is prone to overfitting and high variance, what we can do to counter that is to introduce bias.
                            In return for adding bias, our model has decreased variance. <br><br>
                            A traditional linear regression model minimizes the sum of squared residuals, whereas in ridge regression, the sum of squared residuals <i>plus</i> a &lambda; times the sum of the coefficients squared is minimized. You can think of this as a penalty term. <br>
                            <img  src="images/ridgecost.png" alt="Ridge Cost Function" />
                            <br>
                            <br> If &lambda;=0, our ridge cost function equals our linear regression cost function. Let's implement a ridge regression model. First make sure you add <code>from sklearn.linear_model import Ridge</code> to get our library ready.<br><br>
                            <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);">rr = Ridge(alpha=<span class="hljs-number" style="color: rgb(136, 0, 0);">100</span>) 
rr.fit(X_train,y_train)
y_pred = rr.predict(X_test)
                                
<span class="hljs-function"><span class="hljs-title" style="font-weight: 700; color: navy;">print</span><span class="hljs-params">(<span class="hljs-string" style="color: rgb(0, 85, 0);">"R-Squared: "</span>, r2_score(y_test, y_pred)</span></span>)
<span class="hljs-function"><span class="hljs-title" style="font-weight: 700; color: navy;">print</span><span class="hljs-params">(<span class="hljs-string" style="color: rgb(0, 85, 0);">"Weights: "</span>,lr.coef_)</span></span>
<span class="hljs-function"><span class="hljs-title" style="font-weight: 700; color: navy;">print</span><span class="hljs-params">(<span class="hljs-string" style="color: rgb(0, 85, 0);">"Constant: "</span>,lr.intercept_)</span></span></pre><br><br>
                               
                                <img  src="images/alpha5.png" alt="alpha=100" /><br>
                                With an alpha of 5 we get:<br>
                                <img  src="images/alpha100.png" alt="alpha=5" />
                                </p>
                                <p class="lesson-text">
                                Let's first look at our alpha=100 model. The larger our alpha parameter is, the more bias we introduce and the higher our "penalty term" gets. You can think of alpha as a parameter that represents how much we want to <i>scale</i> our model's coefficients down.
                                A larger alpha helps reduce overfitting. 
                                In this case, an alpha of 100 is too large as it <i>underfits</i> our model. In trying to lower variance, we increased bias too much and are now on the opposite end of the bias-variance curve.
                                Using an alpha of 5, as you can see, yields a much higher correlation. We can see this because as we start with an alpha of 1 (linear regression) and increase it, we'll notice that after a certain point
                                our model performance starts diminishing.  <br>
                                The main benefit of this added bias is a decrease in variance. Our coefficients are much more evenly distributed which helps give our model
                                more stable results. This concept is L2 regularization. With an alpha of 5 our weights were somewhat more evenly distributed and this phenomenon is exaggerated
                                even more with an alpha of 100. Here's a visualiation provided by <a href="https://scikit-learn.org/stable/auto_examples/linear_model/plot_ridge_path.html"><u>scikit-learn.org</u></a>, showing how
                                as alpha values increase, weights for features tend towards zero.<br>
                                <img src="images/ridgealphas.png"><br><br>
                                Here are the coefficients from our model between the linear regression and two ridge regression models we fitted. This is another depiction of how L2 regularization moves weights closer to 0.
                                <br>
                                <img src="images/coefplot.png">
                                </p>

                                <p class="lesson-text">
                                <strong>LASSO Regression</strong><br><br>
                                    In this lecture we'll delve into LASSO (Least Absolute Shrinkage and Selection Operator) regression. In ridge regression, we improved on linear regression by adding a penalty term to help make our coefficients more stable.
                                    Ridge regression utilized L2 regularization while LASSO regression uses L1 regularization. LASSO regression adds <i>feature selection</i> in the mix,
                                    which means it can make certain coefficients or weights zero, thereby removing them and <i>selecting</i> the others. To see how this works, let's look at the LASSO cost function.
                                    <br>
                                    <img src="images/lassocost.png"><br><br>
                                    The difference in the penalty terms in ridge and LASSO regression shows how L1 and L2 regularization differ. Like ridge regression, a &lambda; value of 0 yields
                                    the linear regression cost function. By only looking at the magnitude of coefficients rather than the square, LASSO regression permits certain weights to reach 0 whereas in ridge regression,
                                    weights can only <i>approach</i> 0. This type of regularization is extremely useful when we have a lot of features in our data. L1 regularization uses feature selection to help reduce
                                    model complexity. <br><br>
                                    Let's quickly implement LASSO regression model. First, <code>from sklearn.linear_model import Lasso</code>.<br><br>
                                    <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);">lasso = Lasso() 
lasso.fit(X_train, y_train)
y_pred = lasso.predict(X_test)
                                            
<span class="hljs-function"><span class="hljs-title" style="font-weight: 700; color: navy;">print</span><span class="hljs-params">(<span class="hljs-string" style="color: rgb(0, 85, 0);">"R-Squared: "</span>, r2_score(y_test, y_pred)</span></span>)
<span class="hljs-function"><span class="hljs-title" style="font-weight: 700; color: navy;">print</span><span class="hljs-params">(<span class="hljs-string" style="color: rgb(0, 85, 0);">"Weights: "</span>,lasso.coef_)</span></span>
<span class="hljs-function"><span class="hljs-title" style="font-weight: 700; color: navy;">print</span><span class="hljs-params">(<span class="hljs-string" style="color: rgb(0, 85, 0);">"Constant: "</span>,lasso.intercept_)</span></span></pre>
                                <br> <img src="images/lassoresults.png">                                
</p>

                                <p class="lesson-text">
                                As you can see, LASSO regression gave a weight of 0 to our third feature, an example of feature selection.
                                    
                                
                            <br>
                            Using linear models on this dataset have clearly not been very effective. When using a linear model, ensure your data is <i>linearly seperable</i>, you can test this
                            by simply fitting a linear model and looking into performance. If your model has high error, don't worry! There are more regression models, some of which we'll be going into in future lessons.
                            Here's a helpful chart that shows
                            how linear, ridge, and LASSO regression compare. Before we move on to SVR, let's look into Elastic Net regularization, a combination of L1 and L2 regularization<br><br>
                            <img  src="images/ridgelassocompare.png" alt="Compared"/>
                            
                        </p>
                        <p class="lesson-text">
                        <strong>Elastic Net</strong><br><br>
                        In this lesson we've looked into linear, ridge, and LASSO regression and have learned how ridge and LASSO add bias into the linear regression model.
                        They do this by including a penalty term. The sum of the weights squared penalty term for ridge regression is L2 regularization while the sum of the magnitudes
                        of weights penalty term for LASSO regression is L1 regularization. Elastic Net combines these. If you remember our cost functions from the Ridge and LASSO, you'll
                        remember each one had a different penalty term boxed. The Elastic Net cost function is the linear regression cost function plus &alpha; x Ridge Penalty + (1-&alpha;) x LASSO Penalty.
                        In other words, it's a combination of L1 and L2. You can think of the &alpha; value as a ratio between L1 and L2 regularization.<br><br>
                        Let's implement Elastic Net regularization. First, <code>from sklearn.linear_model import ElasticNet</code>.<br><br>
                        <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);">elastic = ElasticNet(alpha=<span class="hljs-number" style="color: rgb(136, 0, 0);">0.1</span>) 
elastic.fit(X_train, y_train)
y_pred = elastic.predict(X_test)
                            
<span class="hljs-function"><span class="hljs-title" style="font-weight: 700; color: navy;">print</span><span class="hljs-params">(<span class="hljs-string" style="color: rgb(0, 85, 0);">"R-Squared: "</span>, r2_score(y_test, y_pred)</span></span>)
<span class="hljs-function"><span class="hljs-title" style="font-weight: 700; color: navy;">print</span><span class="hljs-params">(<span class="hljs-string" style="color: rgb(0, 85, 0);">"Weights: "</span>,elastic.coef_)</span></span>
<span class="hljs-function"><span class="hljs-title" style="font-weight: 700; color: navy;">print</span><span class="hljs-params">(<span class="hljs-string" style="color: rgb(0, 85, 0);">"Constant: "</span>,elastic.intercept_)</span></span></pre><br>
                            <img src="images/elasticresults.png">

                        
                    </p>
                    <p class="lesson-text">
                        The closer your &alpha; value is to 1, the more L1 regularization will be used and the closer your &alpha; value is to 0, the more L2 regularization will be used.

                    </p>
                        </div>

                        <div id="Logit">

                            <!-- Logistic Regression -->
                            <h1 class="post_title"> Logistic Regression (Logit)</h1>
                            <p class="lesson-text">
                                Here we'll get into one of the most commonly implemented classification models, <strong>Logistic Regression</strong>, We've already looked into a Linear Regression model
                                that can predict a continuous variable, but what if we need to classify our data? That's where a Logit model comes in. Let's first quickly review how we did linear regression.
                                We fit a line to our data and found the optimal coefficients for our linear model that took the form:<br><br>
                                <a href="https://www.codecogs.com/eqnedit.php?latex=y=a_{0}&space;&plus;&space;a_{1}x_{1}&space;&plus;&space;a_{2}x_{2}&space;...&space;&plus;&space;a_{n}x_{n}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y=a_{0}&space;&plus;&space;a_{1}x_{1}&space;&plus;&space;a_{2}x_{2}&space;...&space;&plus;&space;a_{n}x_{n}" title="y=a_{0} + a_{1}x_{1} + a_{2}x_{2} ... + a_{n}x_{n}" /></a>
                                </p>
                                <p class="lesson-text">
                                    Our logistic regression model will be very similar to our linear regression model. In fact, a logistic regression is really just a linear regression on the log-odds of your data. This will be easier to understand
                                    with an example. Let's say we want to figure out whether or not a student-athelete plays basketball or soccer solely based on his/her height. We can take our data in the form (height, sport) and encode our bins
                                    (sports) into numbers. We can say a soccer player is a '0' while a basketball player is a '1'. To read more on how encoding works, check out the Feature Engineering lesson
                                    <a href="#Feature-Engineering"><u>here</u></a>. Since we have two classes, this problem is known as a <strong>binary classification</strong> problem.  We can use logistic regression for multi-class classification but for now, let's start with a simple binary classification. Below is a side-by-side view of our data and plot:<br><br>
                                    <img  style="display: inline-block" src="images/logit-base.png" alt="Logit" /><br><br>
                                    In the above plot, you'll notice that there is an overlap in heights between the basketball players and soccer players, usually when working with data this will happen. Depending on your model and your loss/cost functions, you
                                    will need to take care of this one way or another. When we go into SVM's, you'll see how we can use this information to decide between hard and soft-margins as well as how we can add regularization
                                    to our models to help us generalize. For now, let's think of a shape that could fit this data well. A typical line doesn't work between we have no values between 0 and 1, but we could fit what's called a Sigmoid function. Here's a
                                    visualization of a line (red dotted) and a a sigmoid function (green) fitted onto our data.<br><br>
                                    <img style="display: inline-block" src="images/sig_vs_line.png" alt="Sigmoid_Line" /><br><br>
                                    As you can see from above, the 'S' shape (sigmoid) fit our data better than a line. Let's try to understand what the sigmoid graph in the above plot is <i>really</i> telling us. Our sigmoid function has a lower-bound of 0 and an upper-bound of 1 - similar to a probability or likelihood. That's because we can think of the output of our sigmoid function as the <i>probability</i> of the observation belonging to 
                                    class '1'. Why wouldn't linear regression work well here? Well, the probability that someone is a soccer player or a basketball player will be between 0 and 1 and the line of best fit will continue to both positive and negative infinity. Luckily, we can still make use of our linear regression model and modify it to create our Logit model. Let's think about how we can do this. A line with a non-zero slope will have a range from negative infinity to positive infinity, 
                                    while our data has a range of 0 to 1. What if we transformed our data from having a range of 0 to 1 to having a range of negative infinity to infinity? Instead of modeling the probability of our data, we could model the <i>log odds</i> of our data. This can be done using the formula below:<br><br>
                                    <img src="https://latex.codecogs.com/gif.latex?%5Cinline%20LogOdds%3Dlog%28%5Cfrac%7Bp%7D%7B1-p%7D%29" title="LogOdds=log(\frac{p}{1-p})" /><br><br>
                                    We just transformed our y-axis from being probability to being log-odds. What are log odds though? Recall that the odds of an event happening is the probability it will happen divided by the probability it won't happen. What are the odds you roll a die and see a number greater than 1?
                                    Well the probability of getting a 2,3,4, or 5 is 5/6 while the probability of getting a 1 is 1/6. The odds are then 5/6 divided by 1/6 which is 5. So we would say we have a 5 to 1 odds on that bet. As for why we take the logarithm of these odds, I recommend watching the StatQuest video on odds vs. log-odds <a href="https://www.youtube.com/watch?v=8nm0G-1uJzA&ab_channel=StatQuestwithJoshStarmer"><u>here</u></a>, though I'll try my best to briefly summarize why here.
                                    The issue with regular odds is that the when the probability of the event happening is less than 0.5, our p/(1-p) is between 0 and 1. If the probability of the event happening is greater than 0.5 however, the odds can take values from 1 to infinity. Since the ranges in both cases aren't symmetrical, this will cause our model to be biased towards probabilities greater than 0.5. We can take the logarithm of these odds and it will become symmetrical. Here's how that works. <br>
                                    The odds (p/(1-p)) can take on values from 0 to infinity when the probability (p) 
                                    approaches 0 and 1 respectively. When p is very small and close to 0, log(p/1-p) approaches log(0/1) which approaches negative infinity. When the probably of the event is almost certain and close to 1, log(p/(1-p)) approaches log(1/0) which equals log(1) - log(0) which equals
                                    0 - (negative infinity) or infinity. Let's recap this transformation. <br><br>
                                    <ul>
                                        <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1, Started with probabilities - Range:  0 to 1.</li>
                                        <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2. Converted the probabilities into odds - Range: 0 to infinity</li>
                                        <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3. Converted odds into log-odds - Range: -infinity to infinity</li>
                                    </ul></p>
                                    <p class="lesson-text">
                                    Now that our data has been transformed to be in the range of negative infinity to infinity we can perform a linear regression on our transformed data. This is what our transformed data looks like now:
                                    <br><br>
                                    <img  style="display: inline-block" src="images/log-odds-plot.png" alt="Log-Odds" />
                                </p>
                                <p class="lesson-text">
                                    One thing you'll notice right off the bat is that the y-values in our transformed data are either negative infinity or infinity. This is because our probabilities of a player being a soccer player or basketball player were 0 and 1 - we already knew those probabilities because we were given them! The issue is that
                                    we can't use our "typical" least-squares to minmize our error since our errors are technically infinite. What we can do is use a Maximum-Likelihood Estimation (read the lesson on MLE<a href="#MLE"><u>here</u></a>) to find our coefficients. Once we find our line of best fit, we'd have successfully created a model to
                                    related our input (heights of players) log odds of an event (the player being a basketball player). The thing is, this isn't what we set out to do. We need to find a way to relate the input to probabilities. Here's where the math comes in. Below you can see how we can use the equation of the line we just fit (expression in green)
                                    and transform it into a sigmoid function simply by converting the log odds (a natural log in this case) to probabilities by solving for p. I purposely highlighted the linear expression we get from fitting our line to the transformed data in green so you can see how it comes into play in our final equation.
                                    <br><br>
                                    <img  style="display: inline-block" src="images/logit-derivation.png" alt="Log-Odds" />
                                </p>
                                <p class="lesson-text">
                                    This expression is the form of our Logit model. If we want to <i>classify</i> our data, we will need to conver this probability to either a 0 or 1. One simple say to do this is by setting a threshold value, let's say 0.5 and saying any value above 0.5 will be classified as 1 while any value below 0.5 will be classified as a 0.
                                    It's important to note that the threshold will not always be 0.5. Sometimes we may use a lower or higher threshold function depending on whether we want to reduce our Type-I or Type-II error (we'll get into this in the next couple of lessons including ROC and AUC curves). Let's now look at how to do a logistic regression in Python
                                    using sklearn.<br><br>
                                     <i>As a reminder, this is very basic code and does not make use of any sort of hyperparameter tuning or reguliarization techniques. In practice, a logistic regression model will have to be optimized and a default model may not always perform best. You can read the previous lessons on optimizers and loss-functions to see the possibilities.
                                    In addition, we will later get into hyperparameter tuning (specifically with neural networks) later on and the overall procedure will be somewhat similar to hyperparameter tuning for other ML models like a Logit model. Nonetheless, the code is provided below for the classification of the Iris dataset we've been working on provided from the UC Irvine ML repository (<a href="https://archive.ics.uci.edu/ml/datasets/Iris">data</a>).</i> <br><br>
                                    </p>
                                    <div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #008800; font-weight: bold">import</span> <span style="color: #0e84b5; font-weight: bold">pandas</span> <span style="color: #008800; font-weight: bold">as</span> <span style="color: #0e84b5; font-weight: bold">pd</span>
<span style="color: #008800; font-weight: bold">from</span> <span style="color: #0e84b5; font-weight: bold">sklearn.linear_model</span> <span style="color: #008800; font-weight: bold">import</span> LogisticRegression
<span style="color: #008800; font-weight: bold">from</span> <span style="color: #0e84b5; font-weight: bold">sklearn.model_selection</span> <span style="color: #008800; font-weight: bold">import</span> train_test_split
<span style="color: #008800; font-weight: bold">from</span> <span style="color: #0e84b5; font-weight: bold">sklearn.metrics</span> <span style="color: #008800; font-weight: bold">import</span> accuracy_score
<span style="color: #008800; font-weight: bold">from</span> <span style="color: #0e84b5; font-weight: bold">sklearn.preprocessing</span> <span style="color: #008800; font-weight: bold">import</span> OneHotEncoder
                                        
<span style="color: #888888"># Reading in the Data</span>
iris <span style="color: #333333">=</span> pd<span style="color: #333333">.</span>read_csv(<span style="background-color: #fff0f0">'iris.csv'</span>)
iris<span style="color: #333333">.</span>columns<span style="color: #333333">=</span>[<span style="background-color: #fff0f0">'Sepal Length'</span>, <span style="background-color: #fff0f0">'Sepal Width'</span>, <span style="background-color: #fff0f0">'Petal Length'</span>, <span style="background-color: #fff0f0">'Petal Width'</span>, <span style="background-color: #fff0f0">'Class'</span>]
iris[<span style="background-color: #fff0f0">'Class'</span>] <span style="color: #333333">=</span> iris[<span style="background-color: #fff0f0">'Class'</span>]<span style="color: #333333">.</span>astype(<span style="background-color: #fff0f0">'category'</span>)
                                        
<span style="color: #888888"># Train-Test Split (80-20 Split)</span>
X <span style="color: #333333">=</span> iris<span style="color: #333333">.</span>drop(columns<span style="color: #333333">=</span>[<span style="background-color: #fff0f0">'Class'</span>])
Y <span style="color: #333333">=</span> iris[<span style="background-color: #fff0f0">'Class'</span>]<span style="color: #333333">.</span>cat<span style="color: #333333">.</span>codes
X_train, X_test, y_train, y_test <span style="color: #333333">=</span> train_test_split(X, Y, test_size<span style="color: #333333">=</span><span style="color: #6600EE; font-weight: bold">0.2</span>)
                                        
<span style="color: #888888"># Logistic Regression - Be Careful, it defaults to L2-Regularization</span>
logit <span style="color: #333333">=</span> LogisticRegression(penalty <span style="color: #333333">=</span> <span style="background-color: #fff0f0">'none'</span>) 
logit<span style="color: #333333">.</span>fit(X_train, y_train)
y_pred <span style="color: #333333">=</span> logit<span style="color: #333333">.</span>predict(X_test)
</pre></div>
                                         
                            </div>
                            <p class="lesson-text">
                                Please be aware that the Logistic Regression model from sklearn defaults to a L2 Reguliarzation. To remove this you will have to explicitly state: penalty = 'none' as a parameter. Next, let's look into how we can analyze the results of our classification model.
                                </p>
                           

                                <div id="Confusion">

                                    <!-- Confusion Matrix -->
                                    <h1 class="post_title"> Confusion Matrix</h1>
                                    <p class="lesson-text">
                                        Now that we've learned our first classification model. Let's look into how we can see how well it did. One way to see this is by using something called a <strong>Confusion Matrix</strong>. A confusion matrix
                                        can help us see how well our binary-classification model performed by comparing how the model classified both positive and negative examples. Below is an example of what a confusion matrix would look like. (<a href="https://miro.medium.com/max/2102/1*fxiTNIgOyvAombPJx5KGeA.png"><u>Source</u></a>)
                                        <br><br>
                                        <img  style="display: inline-block" src="images/confusion_matrix.png" alt="Confusion-Matrix" />
                                        
                                        </p>
                                        <p class="lesson-text">
                                        The top left square represents the number of examples that were positive and were predicted to be positive by the model - this is called the number of <strong>True Positives (TP)</strong>. The top right square represents the number of examples that were negative, but the model falsely classified as positive - the <strong>False Positives (FP)</strong>. Similarly, the bottom left square represents the number of examples the model classified as negative but were actually positive - <strong> False Negatives (FN)</strong>, and the bottom right square represents the number of negative examples that were correctly classified as negative - <strong>True Negatives (TN)</strong>. Next, let's look into the two main types of errors we will face with binary classification - Type I and Type II.
                                        </p>
                                        <br><br>
                                        <strong><u>Type I and Type II Errors</u></strong>
                                        <p class="lesson-text">
                                            The first type of error is known as a <strong>Type I error</strong>. A type I error occurs when we falsely reject a true null hypothesis, or in other words, when the model classifies an example as positive when it is actually negative (false positives). A <strong>Type II</strong> error is when we fail to reject a false null hypothesis, or in other words, when the model classifies an example as negative when it was really positive (false negative). 
                                            <br><br>
                                            The best way to think about this is if we have a test to diagnose cancer in a patient. Our false positive rate would be our Type I error and our false negative rate would be our Type II error. In this example, which error do you think is more important and should be minimized? Ideally, we want to be able to diagnose a patient who has cancer as often as possible so they can receive a timely treatment. Misclassifying someone who doesn't have cancer as having cancer would allow that individual to receive further tests in order to diagnose whether or not he/she truly does have cancer and decide what the best course of action is. On the other hand, misclassifying someone who has cancer and concluding they do not have cancer will be more costly since that individual will not be able to get the early care he/she needs which may even lead to his/her death. In this case, it is quite obvious that we want to reduce our false negative rate (Type II error). Can you think of an example where we would want to minimize our false positive rate instead?
                                            <br><br>
                                            What if we had to create a model to determine whether or not someone committed a serious crime that carries a lengthy sentence. If we falsely convict an innocent man, we could end up ruining his life. Ideally, we want to ensure that we minimize the number of false convictions, or <i>false positives</i> in our model. This is the equivalent of minimizing the Type I error. Before we get into how we can use an ROC and AUC to do that, let's first learn some terminology.
                                            </p>
                                            <br><br>
                                            <strong><u>Accuracy, Precision, Recall, F1 Score</u></strong>
                                            <p class="lesson-text">
                                            Below are the formulas for accuracy, predicion, recall, and an F1 score. <br><br>
                                            <img  style="display: inline-block" src="images/recall-formulas.png" alt="Formulas" />
                                            </p>
                                            <p class="lesson-text">
                                                As you can see above the accuracy is the proportion of examples our model predicted correctly, our precision is the proportion of examples our model classified as positive that were in fact positive, our recall shows us our true positive rate and our F1 Score is the harmonic mean of our precision and recall. The F1 score is derived from the F-score with the parameter beta equal to 1. You can read more about the F-score <a href="https://en.wikipedia.org/wiki/F-score"><u>here</u></a>. Now that we have an idea of how these values are calculated. Let's see how we can use ROC and AUC in order to analyze these metrics. 
                                                </p>
                                    </div>         

                                    <div id="AUC-ROC">

                                        <!-- AUC and ROC -->
                                        <h1 class="post_title"> Area Under Curve and Receiver Operating Characeristics (AUC & ROC)</h1>
                                        <p class="lesson-text">
                                            Now that we understand the idea behind a confusion matrix and the four main metrics, is there a way we can use them to figure out an optimal threshold for our classification problem? What about a way to compare which classification models work best? The answer is yes and yes. First, let's get familiar with two terms: <strong>sensitivity and specificity</strong>. These two metrics are similar to the metrics we discussed earlier, in fact, sensitivity (True Positive Rate) is just our recall from earlier. Specificity (True Negative Rate) is the proportion of negative examples that the model correctly classified as negative. I've summarized the two equations below: <br><br>
                                            <img  style="display: inline-block" src="images/specificity.png" alt="Specificity" />
                                            </p> <br><br>
                                            <strong><u>Receiver Operating Characteristics (ROC) Curve</u></strong>
                                            <p class="lesson-text">
                                                Let's quickly recap how we got our confusion matrix. We created a classification model, predicted probabilities using the classification model, and then converted the probabilities to classes based on if they were above or below a certain threshold. This means for every threshold, there exists a different confusion matrix. We usually think of the threshold as simply 0.5 since logically, if an example has more than a 50% chance of belonging to a class, we should put it in that class. The problem with using 0.5 is that it does not take into account whether or not we are trying to minimize our false positive or false negative rate and it may not work well depending on what data is fed in. Remember in our previous lesson we discussed two scenarios where we would want to minimize our Type I and Type II errors. Choosing a different threshold is one way we can minimize those errors. <br><br>
                                                The ROC curve plots the sensitivity (true positive rate) on the y-axis, and 1-specificity (false positive rate) on the x-axis. For each threshold we pick, we will get a new confusion matrix and can plot the TPR and FPR on the ROC curve. To summarize we will:<br><br>
                                                <ul>
                                                        <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1, Create a Classifier</li>
                                                        <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2. Computer Confusion Matricies for different thresholds.</li>
                                                        <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3. Computer the Sensitivity and Specificity of each Confusion Matrix</li>
                                                        <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4. Plot the (FPR, TPR) for each threshold.</li>
                                                    </ul>
                                            </p>
                                            <p class="lesson-text">
                                                    Here we can see what an ROC curve would look like:<br><br>
                                                    <img  style="display: inline-block" src="images/roc-curve.png" alt="ROC Curve" />
                                                </p>
                                            <p class="lesson-text">
                                                        In the above plot, each point represents a different threshold using in the classification model. A line connecting the points (0,0)and (1,1) show the points at which the false positive rate equals the true positive rate. Ideally, we want to maximize the true positive rate while minimizing the false positive rate. In the diagram above, the points highlighted in yellow show the best points according to our aforementioned criterion. These points also map to a different threshold value. Which threshold value we would use in this specific case would depend on whether or not we want to maximize our true positive rate or minimize our false positive rate. This decision varies depending on what the classification problem is.<br><br>
                                                        So now we know that given a classifier, we can map different threshold values to different confusion matricies and thus different points on the ROC curve, Like each threshold value had a seperate confusion matrix, each classifier has a seperate ROC curve. What if there was a way we could look at the ROC curves of different classification models and determine which classifier is better? Lukcily we can do that using the AUC. 
                                                    </p><br><br>
                                                    <strong><u>Area Under the Curve (AUC)</u></strong>
                                                    <p class="lesson-text">
                                                        The idea behind AUC is quite simple. Since ideally we want our false positive rate to be as low as possible and true positive rate to be as high as possible, we want to be as far left from the line that connects (0,0) and (1,1) as possible. THis is because the points closer to that line and to the right of that line, have a higher false positive rate than true positive rate. One thing we can do is plot the ROC curves for each classifier and then take the area under those curves to compare the models to each other. The model with the highest AUC will be better since it better optimizes for both a higher TPR and a lower FPR. This will come in handy later when we learn more classification models.
                                                        </p>
                                        </div>






























                    <div id="ARMA">

                            <!-- ARMA -->
                            <h1 class="post_title"> Autoregressive Moving-Average Model (ARMA)</h1>
                            <p class="lesson-text">
                                Now we'll start working with time-series data. When we refer to time-series data, we mean data that's indexed by a
                                time or date. We've worked with regression models before where we've regressed target variables on seperate features, but what if we wanted to regress
                                an observation with previous lagged observations? We can make use of an AR, or <strong>autoregressive</strong> model.
                                <br><br>
                                <strong><u>Autoregressive Models (AR)</u></strong><br><br>
                                We'll start off by going over autoregressive models. Autoregressive models are models that relates an observations value on its previous vaules.   
                                Here's a basic example of what I mean. <br><br>
                                <a href="https://www.codecogs.com/eqnedit.php?latex=y_{t}&space;=&space;\lambda&space;y_{t-1}&space;&plus;c" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y_{t}&space;=&space;\lambda&space;y_{t-1}&space;&plus;c" title="y_{t} = \lambda y_{t-1} +c" /></a>                                
                                <br><br>
                                The model above is an example of an AR(1) model. This means its an autoregressive model that utilizes one autoregressive lag. We'll learn more about this soon. 
                                You should notice that the formula above looks very similar to our equation for linear regression. In the model above, we're saying that the value of y, our target variable, <i>at time t</i> is related to its value
                                y at <i>time t-1</i>. Lags are how many previous "time steps" or "intervals" our model will use as features in our predictive model.
                                y<sub>t-1</sub> is known as the first <strong>autoregressive lag</strong>.
                                Determining how many autoregressive lags our model should use is very important. We'll be going through
                                how to do this using ACF and PACF plots. Let's first take our equation above 
                                and rewrite it to take into account 'n' autoregressive lags. Since our input into our model is a lag of our ouput, we can
                                denote the output as X. The general form of an AR model would then look like this: <br><br>
                                <a href="https://www.codecogs.com/eqnedit.php?latex=X_{t}&space;=\sum_{i=1}^{n}&space;\lambda_{i}&space;X_{t-i}&space;&plus;\varepsilon_{i}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?X_{t}&space;=\sum_{i=1}^{n}&space;\lambda_{i}&space;X_{t-i}&space;&plus;\varepsilon_{i}" title="X_{t} =\sum_{i=1}^{n} \lambda_{i} X_{t-i} +\varepsilon_{i}" /></a><br><br>
                                In summary, an autoregressive model is a model where the output <i>depends on its previous values.</i> An AR(2) model means the 
                                output variable's value is going to be dependent on two <i>autoregressive lags</i>, or in other words, its two previous values.
                                We'll be implementing these models near the end of the lesson. The reason for this is because once we learn
                                ARMA, ARIMA, and SARIMA models, the parameters can be adjusted to give us simple AR or MA models. It's important to understand the underlying intuition behind these models first.
                                <br><br>
                                <strong><u>Moving Average Models (MA)</u></strong><br><br>
                                The next piece of the puzzle is moving averages. Let's first understand what a moving average is. A moving average simply calculates
                                the average of our time-series over a certain fixed n lags. So a 30-day simple moving average would be the average of our time-series over the past 30 days.
                                So what's the advantage of even using a moving average? Why can't we just look at the latest value and make predictions off that? Well with time-series data,
                                there tends to be a lot of noise, or random fluctuations. It's important that our model does not overreact to these random movements and more importantly, 
                                looks into the overall trend of the series before reacting to a fluctuation. This is a reason why a moving average is known as a lagging indicator. Now let's look into what an MA
                                model looks like. <br><br>
                                <a href="https://www.codecogs.com/eqnedit.php?latex=X_{t}&space;=&space;\mu&space;&plus;&space;\sum_{i=0}^{q}\theta_i\epsilon_{t-i}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?X_{t}&space;=&space;\mu&space;&plus;&space;\sum_{i=0}^{q}\theta_i\epsilon_{t-i}" title="X_{t} = \mu + \sum_{i=0}^{q}\theta_i\epsilon_{t-i}" /></a><br><br>
                                Where &theta;<sub>0</sub> is 1, &mu; represents our mean, and &epsilon; represents our error. This might look a little complicated so let's start off by looking at an MA(2) model. A model that uses
                                two moving average lags. This can be written like so: <br><br>
                                <a href="https://www.codecogs.com/eqnedit.php?latex=X_{t}&space;=&space;\mu&space;&plus;&space;\theta_0&space;&plus;&space;\theta_1\epsilon_{t-1}&space;&plus;&space;\theta_2\epsilon_{t-2}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?X_{t}&space;=&space;\mu&space;&plus;&space;\theta_0&space;&plus;&space;\theta_1\epsilon_{t-1}&space;&plus;&space;\theta_2\epsilon_{t-2}" title="X_{t} = \mu + \theta_0 + \theta_1\epsilon_{t-1} + \theta_2\epsilon_{t-2}" /></a><br><br>
                                Let's make sense of what's going on here. The &epsilon; term represents how far off our <i>previous</i> prediction was. 
                                That means our model is saying the predicted value is going to equal our average value plus some coefficient times our last lag's error, plus another coefficient times
                                the error from the lag <i>before</i>. You might've noticed by now that our MA model uses how far off our previous values were in order to more accurately predict our next value.
                                So what is &epsilon;'s value? 
                                &epsilon; is white noise. A white noise sequence has a mean of 0, a finite variance, and no correlation amongst the terms.
                                We usually write this as such: <br><br>
                                <a href="https://www.codecogs.com/eqnedit.php?latex=\epsilon_{t}&space;\sim&space;WN(0,\sigma&space;^{2})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\epsilon_{t}&space;\sim&space;WN(0,\sigma&space;^{2})" title="\epsilon_{t} \sim WN(0,\sigma ^{2})" /></a>
                                <br><br>
                                If you remember our linear regression equation from the <a href="#Linear-Regression"><u>Linear Regression</u></a> module, you might
                                be thinking that this looks similar. You're right. A moving average value is really a sort of linear regression between our series' current value and lagging noise terms.
                                One thing that's interesting is that since our noise terms are independent and distributed with a mean of 0, our predicted value
                                will be averaged around &mu;. 
                                <br><br>
                                <strong><u>ARMA Models</u></strong><br><br>
                                Now that we understand conceptually how both AR and MA models work, let's learn how we can combine these into an ARMA model.


                                

                                


                                
                                </p>
                                </div>
                                
                <div id="Model-Resources">

                        <!-- Useful-Resources -->
                        <h1 class="post_title"> Useful Resources</h1>
                        <p class="lesson-text">
                            In this series, we covered various models used for analyzing our data. We looked into how the models worked, as well as how to implement each model. If you're looking for datasets to practice with,
                            the <code>sklearn</code> library has <a href="https://scikit-learn.org/stable/datasets/index.html"><u>datasets</u></a> built-in to get started. In addition, <a href="https://www.kaggle.com/datasets"><u>Kaggle</u></a> and
                            the <a href="https://archive.ics.uci.edu/ml/datasets.php"><u>UC Irving Machine Learning Repository</u></a> have datasets as well. On Reddit, <a href="https://www.reddit.com/r/datasets/"><u>r/datasets</u></a> is a good source too. 
                            Here are some guides and documentation relevant to what we covered in this series. <br>
                            <ul class = "bullets">
                                    <li>  Documentations: <a href="https://scikit-learn.org/stable/"><u>sklearn</u></a> | <a href="https://pypi.org/project/stats/"><u>stats</u></a> | <a href="https://www.statsmodels.org/dev/tsa.html"><u>statsmodels.tsa</u></a> </li>

                                    <li>  Here's an  
                                        <a href="https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Scikit_Learn_Cheat_Sheet_Python.pdf"><u>sklearn Cheatsheet</u></a>.</li>
                
                                </ul>
                                <br>
                            <a href="lessons.html"><u>Back to Top</u></a>
                            </p>

                        </div>


<!------------------------------------------------------------------------------------------------------------ END -------------------------------------------------------------------------------------------------------------------------------->






                    <br><br>
            </div>
            <footer>
                <div class="footer content-1170 center-relative">
                </div>
            </footer>
   
    
            <!--Load JavaScript-->
            <script type="text/javascript" src="js/jquery.js"></script>
            <script type='text/javascript' src='js/jquery.sticky-kit.min.js'></script>
            <script type='text/javascript' src='js/jquery.smartmenus.min.js'></script>
            <script type='text/javascript' src='js/jquery.sticky.js'></script>
            <script type='text/javascript' src='js/jquery.dryMenu.js'></script>
            <script type='text/javascript' src='js/isotope.pkgd.js'></script>
            <script type='text/javascript' src='js/jquery.carouFredSel-6.2.0-packed.js'></script>
            <script type='text/javascript' src='js/jquery.mousewheel.min.js'></script>
            <script type='text/javascript' src='js/jquery.touchSwipe.min.js'></script>
            <script type='text/javascript' src='js/jquery.easing.1.3.js'></script>
            <script type='text/javascript' src='js/imagesloaded.pkgd.js'></script>
            <script type='text/javascript' src='js/jquery.prettyPhoto.js'></script>
            <script type='text/javascript' src='js/main.js'></script>
            <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
            <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/prism.min.js"></script>
            
        </body>
</html>
