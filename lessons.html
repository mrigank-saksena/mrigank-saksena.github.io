<!DOCTYPE HTML>
<html lang="en-US">
    <head>
            
        <title>TWD - Thinking With Data</title>
        <meta https-equiv="Content-Type" content="text/html; charset=UTF-8">
        <meta name="description" content="Template by Colorlib" />
        <meta name="keywords" content="HTML, CSS, JavaScript, PHP" />
        <meta name="author" content="Colorlib" />
        <!--<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">-->
        <meta name="viewport" content="width=1024">
        <link rel="shortcut icon" href="images/favicon.png" />
        <link href='https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700,700i,900|Montserrat:400,700|PT+Serif' rel='stylesheet' type='text/css'>
        <link rel="stylesheet" type="text/css"  href='css/clear.css' />
        <link rel="stylesheet" type="text/css"  href='css/common.css' />
        <link rel="stylesheet" type="text/css"  href='css/font-awesome.min.css' />
        <link rel="stylesheet" type="text/css"  href='css/carouFredSel.css' />
        <link rel="stylesheet" type="text/css"  href='css/prettyPhoto.css' />
        <link rel="stylesheet" type="text/css"  href='css/sm-clean.css' />
        <link rel="stylesheet" type="text/css"  href='style.css' />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.10/styles/agate.min.css"/>

        <!--[if lt IE 9]>
                <script src="js/html5.js"></script>
        <![endif]-->

        <style>
            body {
                background-color: #19134E;
                
            }
            
        </style>
       
        <script type="text/javascript">
            function toggleLessonbar(){
             document.getElementById("lessonbar").classList.toggle('active');
            }
        </script>    
    </head>

        <title>TWD - Thinking With Data</title>
        <body> 
           
           
            <div class="menu-wrapper center-relative">
                <nav id="header-main-menu">
                    <div class="mob-menu">MENU</div>
                    <ul class="main-menu sm sm-clean">
                        <li><a href="index.html#home">Home</a></li>
                        <li><a href="index.html#services">Outline</a></li>
                        <li><a href="#lessonbanner">Lessons</a></li>                    
                    </ul>
                </nav>
            </div>
           
                
            
            <div id="lessonbanner" class="section lessons-intro-page">
                <div class="block content-1170 center-relative center-text">
                    <img class="top-logo" src="images/logo.png" alt="TWD" />
                    <br>
                    <h1 class="big-title">Lessons</h1>
                    <!--<button class="lesson-button" onclick="toggleLessonbar()"> View Lessons </button>-->
                </div>
            </div>
            <div></div>
            <div id="lessonbar">
                <div class="menu-button" onclick="toggleLessonbar()">
                    <a>
                    <span></span>
                    <span></span>
                    <span></span>
                    </a>
                </div>    
                <ul class="toc">
                        <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>
                    <li><h1 class="lesson-topic"><a class="lesson-item" href="#queries">SQL Queries</a></h1></li>
                    <li><a class="lesson-item" href="#SQL-Setup">Setup and Introduction</a></li>
                    <li><a class="lesson-item" href="#Creating-Tables">Creating Tables</a></li>
                    <li><a class="lesson-item" href="#SQL-Queries">Writing SQL Queries</a></li>
                    <li><a class="lesson-item" href="#SQL-Queries">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Select Statements</a></li>
                    <li><a class="lesson-item" href="#Functions & Wildcards">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Functions and Wildcards</a></li>
                    <li><a class="lesson-item" href="#GroupBy">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Group By</a></li>
                    <li><a class="lesson-item" href="#Joins">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Unions and Joins</a></li>
                    <li><a class="lesson-item" href="#Nested">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Nested Queries</a></li>
                    <li><a class="lesson-item" href="#SQL-Resources">Useful Resources</a></li>
                    <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>
                    
                    <li><h1 class="lesson-topic"><a class="lesson-item" href="#DP">Data Pre-Processing</a></h1></li>
                    <li><a class="lesson-item" href="#Configuring-Pandas">Setup and Introduction</a></li>
                    <li><a class="lesson-item" href="#Structuring-Data">Structuring Data</a></li>
                    <li><a class="lesson-item" href="#Missing-Values">Interpolating Missing Data</a></li>
                    <li><a class="lesson-item" href="#Pandas-Resources">Useful Resources</a></li>
                    <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>
                    <li><h1 class="lesson-topic"><a class="lesson-item" href="#DE">Data Exploration</a></h1></li>
                    <li><a class="lesson-item" href="#Explore-Setup">Setup and Introduction</a></li>
                    <li><a class="lesson-item" href="#Univariate">Univariate Analysis</a></li>
                    <li><a class="lesson-item" href="#Out">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Removing Outliers</a></li>
                    <li><a class="lesson-item" href="#Bivariate">Bivariate Exploration</a></li>
                    <li><a class="lesson-item" href="#Feature-Engineering">Feature Engineering</a></li>
                    <li><a class="lesson-item" href="#PCA">Principal Components</a></li>
                    <li><a class="lesson-item" href="#Explore-Resources">Useful Resources</a></li>
                    <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>

                    <li><h1 class="lesson-topic"><a class="lesson-item" href="#DM">Data Modeling</a></h1></li>
                    <li><a class="lesson-item" href="#Model-Setup">Setup and Introduction</a></li>
                    <li><a class="lesson-item" href="#Train-Test">Train-Test </a></li>
                    <li><a class="lesson-item" href="#Cross-Validation">Cross-Validation</a></li>
                    <li><a class="lesson-item" href="#Bias-Variance">Bias-Variance Tradeoff</a></li>   
                    <li><a class="lesson-item" href="#Loss-Functions">Loss Functions</a></li>
                    <li><a class="lesson-item" href="#MAE">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MAE (L1) and MSE (L2)</a></li>  
                    <li><a class="lesson-item" href="#Huber">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Huber</a></li> 
                    <li><a class="lesson-item" href="#Log">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Binary Cross-Entropy</a></li>  
                    <li><a class="lesson-item" href="#Optimizers">Optimization Algorithms</a></li> 
                    <li><a class="lesson-item" href="#GD">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Gradient Descent (GD)</a></li>
                    <li><a class="lesson-item" href="#SGD">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Stochastic GD (SGD)</a></li>
                    <li><a class="lesson-item" href="#Adam">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Adam</a></li>   
                    <li><a class="lesson-item" href="#MLE">MLE for Parameterization</a></li>  
                    <li><a class="lesson-item" href="#Linear-Regression">Machine Learning Models</a></li>  
                    <li><a class="lesson-item" href="#Linear-Regression">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Linear Regression</a></li>
                    <li><a class="lesson-item" href="#Ridge">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Ridge, LASSO, Elastic Net</a></li> 
                    <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Logistic Regression</li>
                    <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Confusion Matrix</li>
                    <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Decision Trees</li>
                    <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Random Forest</li>
                    <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;K-Nearest Neighbors</li>             
                    <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Support Vector Machines</li>
                    <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Naive Bayes</li>
                    <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;K-Means Clustering</li>
                    <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;DBSCAN</li>
                    <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Multilayer Perceptron</li>
                    <li>Time-Series Models</li>
                    <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;AR, MA, ARMA</li>
                    <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ARIMA and SARIMA</li>
                    <li><a class="lesson-item" href="#Model-Resources">Useful Resources</a></li>
                    <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>

                </ul>
            </div>



















            <!-- POSTS -->

            <div id="lesson-post" class="post_background">
                
                <!-- QUERIES TITLE -->
                <h1 id = "queries" class="topic_title">SQL Queries </h1>
                <!-- QUERIES TITLE -->


                <div id="SQL-Setup">
                <!-- SETUP AND INSTRUCTIONS -->
                <h1 class="post_title"> Setup and Introduction </h1>
                <p class="lesson-text">    In order to get started, make sure you have your environment set up. To do this, I recommend referencing these links:</p>
                
                <ul class = "bullets">
                    <li>  You can download MySQL <a href="https://dev.mysql.com/downloads/mysql/"><u>here</u></a>.</li>

                    <li>  One free interface I find very useful in writing SQL commands is PopSQL. Which you can download 

                        <a href="https://popsql.com/"><u>here</u></a>.</li>
                </ul>
                <p class="lesson-text">     Before we dive into writing SQL queries, it's important to understand what SQL and MySQL are.
                        SQL represents the syntax used in manipulating relational databases, whereas MYSQL is a RDBMS (Relational Database Management System).
                        In other words MySQL uses Structured Query Language to manipulate databases.
                </p>
                </div>
                    
                <div id="RD">
               
                <p class="lesson-text">  
                    Shown below are two tables within a database. The table on the left shows the Players trying out for sports teams
                    while the table on the right shows the coaches for each sport.<br><br>
                    <img  style="display: inline-block" src="images/rd1.png" alt="Tables" />
                    
                </p>
                <p class="lesson-text"> 
                    The database shown above is a <i> relational database</i> because the two tables 'Players', and 'Coaches' are related
                    by a common field 'Sport'. Often times databases will have tables that are related to each other and through SQL queries
                    we can retrieve information from these tables. 
                    </p>
                </div>
                <div id="Creating-Tables">
                     <!-- Basic SQL Commands -->
                    <h1 class="post_title"> Creating Tables </h1>
                    <p class="lesson-text"> 
                            Let's start off by creating the two tables we'll be querying in this lesson. The two tables will be
                            called '' and ''. We can make use of the <code>CREATE TABLE</code> and <code>INSERT</code> commands
                            to do this. <br><br>
                            <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-keyword" style="font-weight: 700; color: navy;">CREATE</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">TABLE</span> Players(
<span class="hljs-keyword" style="font-weight: 700; color: navy;">Name</span> <span class="hljs-built_in" style="font-weight: 700; color: navy;">VARCHAR</span>(<span class="hljs-number" style="color: rgb(136, 0, 0);">20</span>) PRIMARY <span class="hljs-keyword" style="font-weight: 700; color: navy;">KEY</span>,
Age <span class="hljs-built_in" style="font-weight: 700; color: navy;">INT</span>,
Experience <span class="hljs-built_in" style="font-weight: 700; color: navy;">INT</span>,
Sport <span class="hljs-built_in" style="font-weight: 700; color: navy;">VARCHAR</span>(<span class="hljs-number" style="color: rgb(136, 0, 0);">20</span>)
);
                            
<span class="hljs-keyword" style="font-weight: 700; color: navy;">CREATE</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">TABLE</span> Coaches(
<span class="hljs-keyword" style="font-weight: 700; color: navy;">Name</span> <span class="hljs-built_in" style="font-weight: 700; color: navy;">VARCHAR</span>(<span class="hljs-number" style="color: rgb(136, 0, 0);">20</span>) PRIMARY <span class="hljs-keyword" style="font-weight: 700; color: navy;">KEY</span>,
Age <span class="hljs-built_in" style="font-weight: 700; color: navy;">INT</span>,
Sport <span class="hljs-built_in" style="font-weight: 700; color: navy;">VARCHAR</span>(<span class="hljs-number" style="color: rgb(136, 0, 0);">20</span>)
);
                            
<span class="hljs-keyword" style="font-weight: 700; color: navy;">INSERT</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">INTO</span> Players <span class="hljs-keyword" style="font-weight: 700; color: navy;">VALUES</span>(<span class="hljs-string" style="color: rgb(0, 85, 0);">'Derek'</span>,<span class="hljs-number" style="color: rgb(136, 0, 0);">18</span>,<span class="hljs-number" style="color: rgb(136, 0, 0);">2</span>,<span class="hljs-string" style="color: rgb(0, 85, 0);">'Soccer'</span>);
<span class="hljs-keyword" style="font-weight: 700; color: navy;">INSERT</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">INTO</span> Players <span class="hljs-keyword" style="font-weight: 700; color: navy;">VALUES</span>(<span class="hljs-string" style="color: rgb(0, 85, 0);">'Bob'</span>,<span class="hljs-number" style="color: rgb(136, 0, 0);">19</span>,<span class="hljs-number" style="color: rgb(136, 0, 0);">1</span>,<span class="hljs-string" style="color: rgb(0, 85, 0);">'Basketball'</span>);
<span class="hljs-keyword" style="font-weight: 700; color: navy;">INSERT</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">INTO</span> Players <span class="hljs-keyword" style="font-weight: 700; color: navy;">VALUES</span>(<span class="hljs-string" style="color: rgb(0, 85, 0);">'Michael'</span>,<span class="hljs-number" style="color: rgb(136, 0, 0);">18</span>,<span class="hljs-number" style="color: rgb(136, 0, 0);">4</span>,<span class="hljs-string" style="color: rgb(0, 85, 0);">'Basketball'</span>);
<span class="hljs-keyword" style="font-weight: 700; color: navy;">INSERT</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">INTO</span> Players <span class="hljs-keyword" style="font-weight: 700; color: navy;">VALUES</span>(<span class="hljs-string" style="color: rgb(0, 85, 0);">'Tom'</span>,<span class="hljs-number" style="color: rgb(136, 0, 0);">16</span>,<span class="hljs-number" style="color: rgb(136, 0, 0);">3</span>,<span class="hljs-string" style="color: rgb(0, 85, 0);">'Golf'</span>);
<span class="hljs-keyword" style="font-weight: 700; color: navy;">INSERT</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">INTO</span> Players <span class="hljs-keyword" style="font-weight: 700; color: navy;">VALUES</span>(<span class="hljs-string" style="color: rgb(0, 85, 0);">'Fred'</span>,<span class="hljs-number" style="color: rgb(136, 0, 0);">14</span>,<span class="hljs-number" style="color: rgb(136, 0, 0);">1</span>,<span class="hljs-string" style="color: rgb(0, 85, 0);">'Soccer'</span>);
<span class="hljs-keyword" style="font-weight: 700; color: navy;">INSERT</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">INTO</span> Players <span class="hljs-keyword" style="font-weight: 700; color: navy;">VALUES</span>(<span class="hljs-string" style="color: rgb(0, 85, 0);">'Carl'</span>,<span class="hljs-number" style="color: rgb(136, 0, 0);">18</span>,<span class="hljs-number" style="color: rgb(136, 0, 0);">0</span>,<span class="hljs-string" style="color: rgb(0, 85, 0);">'Golf'</span>);
<span class="hljs-keyword" style="font-weight: 700; color: navy;">INSERT</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">INTO</span> Players <span class="hljs-keyword" style="font-weight: 700; color: navy;">VALUES</span>(<span class="hljs-string" style="color: rgb(0, 85, 0);">'Ivan'</span>,<span class="hljs-number" style="color: rgb(136, 0, 0);">21</span>,<span class="hljs-number" style="color: rgb(136, 0, 0);">0</span>,<span class="hljs-string" style="color: rgb(0, 85, 0);">'Basketball'</span>);
<span class="hljs-keyword" style="font-weight: 700; color: navy;">INSERT</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">INTO</span> Players <span class="hljs-keyword" style="font-weight: 700; color: navy;">VALUES</span>(<span class="hljs-string" style="color: rgb(0, 85, 0);">'Brian'</span>,<span class="hljs-number" style="color: rgb(136, 0, 0);">13</span>,<span class="hljs-number" style="color: rgb(136, 0, 0);">1</span>,<span class="hljs-string" style="color: rgb(0, 85, 0);">'Basketball'</span>);
                            
<span class="hljs-keyword" style="font-weight: 700; color: navy;">INSERT</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">INTO</span> Coaches <span class="hljs-keyword" style="font-weight: 700; color: navy;">VALUES</span>(<span class="hljs-string" style="color: rgb(0, 85, 0);">"Leo"</span>,<span class="hljs-number" style="color: rgb(136, 0, 0);">34</span>,<span class="hljs-string" style="color: rgb(0, 85, 0);">'Soccer'</span>);
<span class="hljs-keyword" style="font-weight: 700; color: navy;">INSERT</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">INTO</span> Coaches <span class="hljs-keyword" style="font-weight: 700; color: navy;">VALUES</span>(<span class="hljs-string" style="color: rgb(0, 85, 0);">"Brian"</span>,<span class="hljs-number" style="color: rgb(136, 0, 0);">36</span>, <span class="hljs-string" style="color: rgb(0, 85, 0);">'Basketball'</span>);
<span class="hljs-keyword" style="font-weight: 700; color: navy;">INSERT</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">INTO</span> Coaches <span class="hljs-keyword" style="font-weight: 700; color: navy;">VALUES</span>(<span class="hljs-string" style="color: rgb(0, 85, 0);">"Gary"</span>,<span class="hljs-number" style="color: rgb(136, 0, 0);">49</span>,<span class="hljs-string" style="color: rgb(0, 85, 0);">'Golf'</span>);</pre>
                    </p>
                    <p class="lesson-text"> 
                    <strong>NOTE: You will have to run each query seperately.</strong> <br>
                    We first created two tabels 'Players' and 'Coaches'. Inside each <code>CREATE TABLE</code> block
                    are the names and datatypes of each column of each table. The <code>PRIMARY KEY</code> signifies the
                    ID of each entry, similar to an index. Relational databases may also have a <code>FOREIGN KEY</code>  
                    that signifies a column or attribute that is the <code>PRIMARY KEY</code> of <i>another</i> table. After creating
                    the tables we then <code>INSERT INTO</code> each table specific <code>VALUES</code>, this is how you populate
                    a SQL table.<br>
                    There are many other commands we can use to manipulate our tables before querying them. I've outlined some of them below.<br><br>
                    <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);">DESCRIBE Players; <span class="hljs-keyword" style="color: rgb(119, 119, 119);"># Summarizes the Players</span>
DROP <span class="hljs-keyword" style="font-weight: 700; color: navy;">TABLE</span> Players; <span class="hljs-comment" style="color: rgb(119, 119, 119);"># Deletes the Players table</span>
ALTER <span class="hljs-keyword" style="font-weight: 700; color: navy;">TABLE</span> Players <span class="hljs-comment" style="color: rgb(0, 0, 0);">ADD Height INT</span>;<span class="hljs-comment" style="color: rgb(119, 119, 119);"> # Adds a</span> <span class="hljs-comment" style="color: rgb(119, 119, 119);">"Height"</span><span class="hljs-comment" style="color: rgb(119, 119, 119);"> attribute to Players</span>
ALTER <span class="hljs-keyword" style="font-weight: 700; color: navy;">TABLE</span> Players <span class="hljs-comment" style="color: rgb(0, 0, 0);">DROP COLUMN Height</span>;<span class="hljs-comment" style="color: rgb(119, 119, 119);"> # Deletes the</span> <span class="hljs-comment" style="color: rgb(119, 119, 119);">"Height"</span><span class="hljs-comment" style="color: rgb(119, 119, 119);"> attribute from Players</span>
UPDATE Coaches SET Age = <span class="hljs-number" style="color: rgb(136, 0, 0);">40</span>; <span class="hljs-number" style="color: rgb(119, 119, 119);"># Updates the ages of coaches to 40</span>
DELETE FROM Players WHERE Name = <span class="hljs-string" style="color: rgb(0, 85, 0);">'Tom'</span>; <span class="hljs-keyword" style="color: rgb(119, 119, 119);"># Deletes Tom from Players</span></pre>    
                    </p>
                </div>

                <div id="SQL-Queries">
                        <!-- SQL QUERIES -->
                        <h1 class="post_title"> Writing SQL Queries </h1>
                        
                        <p class="lesson-text"> <a href="https://dev.mysql.com/doc/refman/8.0/en/select.html"><u><strong><code>SELECT</code></strong></u></a><br>Querying a database is a fancy way of saying you want to retrieve information
                            from a database. SQL Queries make use of a powerful command called <code>SELECT</code>, it allows
                        us to retrieve specific information from our database. Let's first start off by selecting all the data from
                        our Players tables. We can do this using <code>SELECT * FROM Players;</code>. After running this, you should 
                        see the Players table. The <code>SELECT</code> command is where we put the attributes you're hoping to retrieve.
                        the <code>FROM</code> command is used to signify which table you plan on selecting your attributes from, and a <code>WHERE</code> command
                        is used to provide a condition to determine which specific entries from those columns you hope to retrieve. Here's an example of one of these
                        queries. <br><br>
                        <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-keyword" style="font-weight: 700; color: navy;">SELECT</span> Name, Sport <span class="hljs-keyword" style="font-weight: 700; color: navy;">FROM</span> Players <span class="hljs-keyword" style="font-weight: 700; color: navy;">WHERE</span> Age &gt;= <span class="hljs-number" style="color: rgb(136, 0, 0);">18</span>;</pre>
                        <br> 
                        <img  src="images/query1.png" alt="Query #1 Output" /> 
                    </p>   
                    <p class="lesson-text">
                        In the above query, we selected the Name and Sport of each person in the Players table whose age 
                        is at least 18. We could also add a <code>ORDER BY Sport</code> after the <code>WHERE</code> command
                        in order to see our derived table sorted by Sport. <br>
                        If we were instead only interested in the sports that have players over 18, we could take out the <code>Name</code> attribute
                        from our <code>SELECT</code> command. However, when running this query, you'll notice sports come up multiple times.
                        We can get around this by using the <code>DISTINCT</code> keyword. <br><br>
                        <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-keyword" style="font-weight: 700; color: navy;">SELECT</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">DISTINCT</span> Sport <span class="hljs-keyword" style="font-weight: 700; color: navy;">FROM</span> Players <span class="hljs-keyword" style="font-weight: 700; color: navy;">WHERE</span> Age &gt;= <span class="hljs-number" style="color: rgb(136, 0, 0);">18</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">ORDER</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">BY</span> Sport;</pre><br>
                        <img  src="images/query2.png" alt="Query #2 Output" />

                    </p>
                    <p class="lesson-text">
                        Let's do one more example. What if we wanted to see all the players who play either Soccer or Golf who are
                        between 14 and 17? We can do this using the following query. <br><br>
                        <pre id = "Functions & Wildcards" class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-keyword" style="font-weight: 700; color: navy;">SELECT</span> Name
<span class="hljs-keyword" style="font-weight: 700; color: navy;">FROM</span> Players 
<span class="hljs-keyword" style="font-weight: 700; color: navy;">WHERE</span> (age <span class="hljs-keyword" style="font-weight: 700; color: navy;">BETWEEN</span> <span class="hljs-number" style="color: rgb(136, 0, 0);">14</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">AND</span> <span class="hljs-number" style="color: rgb(136, 0, 0);">17</span>) <span class="hljs-keyword" style="font-weight: 700; color: navy;">AND</span> (Sport <span class="hljs-keyword" style="font-weight: 700; color: navy;">IN</span> (<span class="hljs-string" style="color: rgb(0, 85, 0);">'Soccer'</span>, <span class="hljs-string" style="color: rgb(0, 85, 0);">'Golf'</span>));</pre>
                    </p>
                    <p  class="lesson-text"> <a href="https://dev.mysql.com/doc/refman/8.0/en/select.html"><u><strong><code>Functions</code></strong></u></a><br>
                    <br>
                    Although it's useful to be able to retrieve entries from our database, sometimes we may be interested in 
                    characteristics of our data. SQL keywords such as <code>AVG, COUNT, SUM, MAX, MIN</code> can be used to 
                    return characteristics of a column. Here we find some summary statistics of the 
                    basketball players' age. We use the <code>AS</code> keyword to rename the columns in our derived table.
                    <br><br>
                    <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-keyword" style="font-weight: 700; color: navy;">SELECT</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">AVG</span>(Age) <span class="hljs-keyword" style="font-weight: 700; color: navy;">AS</span> Average, <span class="hljs-keyword" style="font-weight: 700; color: navy;">MAX</span>(Age) <span class="hljs-keyword" style="font-weight: 700; color: navy;">AS</span> Maximum, <span class="hljs-keyword" style="font-weight: 700; color: navy;">MIN</span>(Age) <span class="hljs-keyword" style="font-weight: 700; color: navy;">AS</span> Minimum, <span class="hljs-keyword" style="font-weight: 700; color: navy;">COUNT</span>(AGE) <span class="hljs-keyword" style="font-weight: 700; color: navy;">AS</span> <span class="hljs-string" style="color: rgb(0, 85, 0);">'Count'</span>, <span class="hljs-keyword" style="font-weight: 700; color: navy;">SUM</span>(AGE) <span class="hljs-keyword" style="font-weight: 700; color: navy;">AS</span> <span class="hljs-string" style="color: rgb(0, 85, 0);">'Sum'</span> 
<span class="hljs-keyword" style="font-weight: 700; color: navy;">FROM</span> Players 
<span class="hljs-keyword" style="font-weight: 700; color: navy;">WHERE</span> Sport = <span class="hljs-string" style="color: rgb(0, 85, 0);">'Basketball'</span>;</pre>
                    </p>   
                    <p  class="lesson-text">
                        What if we wanted to filter our data based on the characters within a <code>VARCHAR</code> attribute? If we wanted
                        to find the average age of the players whose name ends with the letter l we can use SQL Wildcards which
                        make use of the <code>LIKE</code> keyword.<br><br>
                        <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-keyword" style="font-weight: 700; color: navy;">SELECT</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">AVG</span>(Age) <span class="hljs-keyword" style="font-weight: 700; color: navy;">AS</span> Average <span class="hljs-keyword" style="font-weight: 700; color: navy;">FROM</span> Players <span class="hljs-keyword" style="font-weight: 700; color: navy;">WHERE</span> Name <span class="hljs-keyword" style="font-weight: 700; color: navy;">LIKE</span> <span class="hljs-string" style="color: rgb(0, 85, 0);">"%l"</span>;</pre>
                    </p>
                    <p  class="lesson-text">
                        The <code>LIKE</code> keyword compares a <code>VARCHAR</code> to a specified pattern. The <code>%</code>
                        in <code>"%l"</code> means and sequence of characters that ends with l. The <code>%</code> is what's called
                        a <i>Wildcard Character</i>. These are often helpful when looking for specific patters within non-numerical
                        data. A name starting with M would use the wildcard <code>"M%"</code>, while a name whose second letter is r would
                        use <code>"_r%"</code>. You can read more about wildcards <a id = "GroupBy" href="https://www.w3schools.com/sql/sql_wildcards.asp"><u>here</u></a>.
                    </p>
                    <p  class="lesson-text"> <a href="https://dev.mysql.com/doc/refman/8.0/en/select.html"><u><strong><code>GROUP BY</code></strong></u></a><br>
                        <br>
                        Sometimes we're interested in querying data by categories or <i>groups</i>. To do this, we can make use
                        of the <code>GROUP BY</code> command. Let's say we were interested in the number of players for each sport.
                        We would have to count the number of players <i>grouped</i> by Sport. In this example, we ordered
                        the derived table in reverse-alphabetical order.<br><br>
                        <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-keyword" style="font-weight: 700; color: navy;">SELECT</span> Sport, <span class="hljs-keyword" style="font-weight: 700; color: navy;">COUNT</span>(Name) <span class="hljs-keyword" style="font-weight: 700; color: navy;">AS</span> Total 
<span class="hljs-keyword" style="font-weight: 700; color: navy;">FROM</span> Players 
<span class="hljs-keyword" style="font-weight: 700; color: navy;">GROUP</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">BY</span> Sport 
<span class="hljs-keyword" style="font-weight: 700; color: navy;">ORDER</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">BY</span> Sport <span class="hljs-keyword" style="font-weight: 700; color: navy;">DESC</span>;</pre>
                        <br><img  src="images/groupby.png" alt="GROUP BY" />
                        </p>
                        <p  class="lesson-text">
                            One confusing part of the <code>GROUP BY</code> command is knowing when to use <code>WHERE</code> versus
                            <code>HAVING</code> for conditional queries. <code>WHERE</code> is used <i>before</i> the <code>GROUP BY</code>
                            command whereas <code>HAVING</code> is used <i>after</i> the GROUP BY clause. Here are a couple examples to help.<br><br>
                            <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-keyword" style="font-weight: 700; color: navy;">SELECT</span> Sport, <span class="hljs-keyword" style="font-weight: 700; color: navy;">COUNT</span>(Name) <span class="hljs-keyword" style="font-weight: 700; color: navy;">AS</span> Total 
<span class="hljs-keyword" style="font-weight: 700; color: navy;">FROM</span> Players 
<span class="hljs-keyword" style="font-weight: 700; color: navy;">WHERE</span> Age &gt;= <span class="hljs-number" style="color: rgb(136, 0, 0);">18</span> 
<span class="hljs-keyword" style="font-weight: 700; color: navy;">GROUP</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">BY</span> Sport 
<span class="hljs-keyword" style="font-weight: 700; color: navy;">ORDER</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">BY</span> Sport <span class="hljs-keyword" style="font-weight: 700; color: navy;">DESC</span>;
                                
<span class="hljs-keyword" style="font-weight: 700; color: navy;">SELECT</span> Sport, <span class="hljs-keyword" style="font-weight: 700; color: navy;">AVG</span>(Age) <span class="hljs-keyword" style="font-weight: 700; color: navy;">AS</span> <span class="hljs-string" style="color: rgb(0, 85, 0);">'Average Age'</span>
<span class="hljs-keyword" style="font-weight: 700; color: navy;">FROM</span> Players 
<span class="hljs-keyword" style="font-weight: 700; color: navy;">GROUP</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">BY</span> Sport 
<span class="hljs-keyword" style="font-weight: 700; color: navy;">HAVING</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">COUNT</span>(Name) &gt; <span class="hljs-number" style="color: rgb(136, 0, 0);">2</span>
<span class="hljs-keyword" style="font-weight: 700; color: navy;">ORDER</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">BY</span> Sport <span class="hljs-keyword" style="font-weight: 700; color: navy;">DESC</span>;</pre>

                        </p>
                        <p  class="lesson-text">
                                In our first query, we're filtering entries based on the number of players who are at least 18.
                                This means we're filtering out players <i>before</i> they're grouped. Whether or not a player
                                is 18 years old is irrelevant to what sport he/she plays and thus we can make use of the
                                <code>WHERE</code> keyword. However, in our second query, we're filtering our results based on
                                how many players there are in each sport, or in other words, <i>after</i> they're grouped, which is why we use
                                the <code id = "Joins">HAVING</code> clause.
                        </p>
                        <p  class="lesson-text"><a href="https://dev.mysql.com/doc/refman/8.0/en/select.html"><u><strong><code>UNIONS & JOINS</code></strong></u></a><br>
                            Unions and joins are incredibly useful in querying data from tables that
                            can be combined. A <code>UNION</code> is an easy way to combine multiple <code>SELECT</code> statements.
                            Here's an example. <br><br>
                            <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-keyword" style="font-weight: 700; color: navy;">SELECT</span> Name, Age
<span class="hljs-keyword" style="font-weight: 700; color: navy;">FROM</span> Players
<span class="hljs-keyword" style="font-weight: 700; color: navy;">WHERE</span> Age &gt; <span class="hljs-number" style="color: rgb(136, 0, 0);">18</span>
<span class="hljs-keyword" style="font-weight: 700; color: navy;">UNION</span>
<span class="hljs-keyword" style="font-weight: 700; color: navy;">SELECT</span> Name, Age
<span class="hljs-keyword" style="font-weight: 700; color: navy;">FROM</span> Coaches
<span class="hljs-keyword" style="font-weight: 700; color: navy;">WHERE</span> Age &gt; <span class="hljs-number" style="color: rgb(136, 0, 0);">40</span>;</pre>
                            <br><img  src="images/union.png" alt="Union Output" />
                        </p>
                        <p  class="lesson-text">
                            In this example, we selected both the players who are over 18 <i>and</i> the coaches who are over 40.
                            Unions are convenient when you want to create new rows in your table. The query result is a derived table
                            that simply stacks the table with players over 18 on top of the table of coaches over 40. If instead of this
                            we were interested in combining <i>columns</i> of data. We could make use of the <code>JOIN</code> command.<br><br>
                            <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-keyword" style="font-weight: 700; color: navy;">SELECT</span> Players.Name <span class="hljs-keyword" style="font-weight: 700; color: navy;">as</span> <span class="hljs-string" style="color: rgb(0, 85, 0);">'Player Name'</span>, Coaches.Name <span class="hljs-keyword" style="font-weight: 700; color: navy;">as</span> <span class="hljs-string" style="color: rgb(0, 85, 0);">'Coach Name'</span>
<span class="hljs-keyword" style="font-weight: 700; color: navy;">FROM</span> Players
<span class="hljs-keyword" style="font-weight: 700; color: navy;">LEFT</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">JOIN</span> Coaches
<span class="hljs-keyword" style="font-weight: 700; color: navy;">ON</span> Players.Sport = Coaches.Sport;</pre><br>
                            <img src="images/leftjoin.png" alt="Left Join" />
                        </p>
                        <p  class="lesson-text"> In the query above, we're retrieving tha names of each player along with their coaches.
                            Their coaches are determined by their sport which is why we joined <code>ON Players.Sport = Coaches.Sport</code>.
                            However, I included a <code>LEFT JOIN</code> in my query when combining both tables, what is this? A <code>LEFT JOIN</code>
                            is a method of joining two tables. It starts with each entry in the left table (Players) and then <i>matches</i> it to a value on the right
                            table (Coaches) based on their shared attribute (Sport), a <code>LEFT JOIN</code> is the same as as
                            <code>LEFT OUTER JOIN</code>. The best way to understand the various joins is through venn diagrams. Here's a helpful
                            and easy diagram I found on <a href="https://www.reddit.com/r/SQL/comments/108xpy/visual_guide_to_sql_joins/"><u>Reddit</u></a> that shows the different types of joins. <br><br>
                            
                        </p>
                        <p  class="lesson-text">
                            To know when to use each type of <code>join</code>, it's important you understand how they work conceptually. The Venn Diagram will help
                            but the best practice is to try each <code>join</code> out yourself.
                        </p>
                </div>

                <div id="Nested">
                        <!-- NESTED QUERIES -->
                        <h1 class="post_title"> Nested Queries </h1>
                        <p class="lesson-text">
                            Sometimes the queries we write will be <i>nested</i>. A nested query, sometimes called a subquery, is a
                            <code>SELECT</code> statement <i>within</i> the <code>WHERE</code> clause of <i>another</i> query. You can
                            think of it as a query that's based on another query. We'll start off with a simple example. Let's say we wanted
                            a list of players with the same name as a coach. We could this simply by: <br><br>
                            <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-keyword" style="font-weight: 700; color: navy;">SELECT</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">DISTINCT </span> Name 
<span class="hljs-keyword" style="font-weight: 700; color: navy;">FROM</span> Players 
<span class="hljs-keyword" style="font-weight: 700; color: navy;">WHERE</span> Name <span class="hljs-keyword" style="font-weight: 700; color: navy;">IN</span> (<span class="hljs-keyword" style="font-weight: 700; color: navy;">SELECT</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">DISTINCT</span> Name <span class="hljs-keyword" style="font-weight: 700; color: navy;">FROM</span> Coaches);</pre>
                        </p>
                        <p class="lesson-text">
                            After running this query, we get <code>Brian</code> as an output, the only common name between the two tables. In this example 
                            the subquery is <code>"SELECT DISTINCT Name FROM Coaches"</code> within the <code>"SELECT DISTINCT Name FROM Players WHERE..."</code> query.
                            Now let's do a slightly more complicated nested query. How could we query the Names and Sports of the coaches who coach teams
                            where the average age is at least 17 or that has more than two players. Although this sounds complex, its often easiest
                            to break down the problem into smaller parts. The first part being how to query the Name and Sport of Coaches within the Coach table. 
                            The next part is figuring out how to query the sports whose players meet the conditions. The last part requires the use of the
                            <code>GROUP BY</code> clause, as illustrated below.<br><br>
                            <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-keyword" style="font-weight: 700; color: navy;">SELECT</span> Name, Sport
<span class="hljs-keyword" style="font-weight: 700; color: navy;">FROM</span> Coaches
<span class="hljs-keyword" style="font-weight: 700; color: navy;">WHERE</span> Sport <span class="hljs-keyword" style="font-weight: 700; color: navy;">IN</span> 
   (<span class="hljs-keyword" style="font-weight: 700; color: navy;">SELECT</span> Sport
    <span class="hljs-keyword" style="font-weight: 700; color: navy;">FROM</span> Players 
    <span class="hljs-keyword" style="font-weight: 700; color: navy;">GROUP</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">BY</span> Sport 
    <span class="hljs-keyword" style="font-weight: 700; color: navy;">HAVING</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">COUNT</span>(Name) &gt; <span class="hljs-number" style="color: rgb(136, 0, 0);">2</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">OR</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">AVG</span>(Age)&gt;=<span class="hljs-number" style="color: rgb(136, 0, 0);">17</span>);</pre>
                        </p>
                        <p class="lesson-text">
                            Subqueries are extremely useful when using SQL and can get complicated very quickly. My strategy is to take the query
                            you're trying to write, and break it down into its queries and subqueries individually if possible. From there you can
                            combine your queries and subqueries to reach your final query. Remember, practice is key.
                        </p>
                </div>







                <div id="SQL-Resources">
                        <!-- SQL RESOURCES -->
                        <h1 class="post_title"> Useful Resources </h1>
                        <p class="lesson-text">    Even though we've learned the most important and "fundamental" SQL commands, we haven't
                            worked with large datasets, complex nested queries, or certain topics such as table aliases and triggers. It's helpful to make your own tables and try to create queries yourself
                            in order to practice. Nevertheless, here are some references I think are particularly useful.
                        </p>
                        
                        <ul class = "bullets">
                            <li>  Here are a couple of places you can practice SQL commands. I personally recommend
                                Hackerrank though have found all three helpful. <a href="https://www.hackerrank.com/domains/sql"><u>Hackerrank</u></a>, 
                                <a href="https://www.w3schools.com/sql/exercise.asp"><u>W3Schools</u></a>, <a href="https://sqlzoo.net/"><u>SQL Zoo</u></a>.
                            
                            </li>
          
                            <li>  Here's a downloadable cheatsheet that goes over the main SQL commands.  
                                <a href="https://www.sqltutorial.org/sql-cheat-sheet/"><u>Cheatsheet</u></a>.
                            </li>
                        </ul>
                            </p>   
                </div>























                <!-- DATA PREPERATION TITLE -->
                <h1 id = "DP" class="topic_title">Data Pre-Processing </h1>
                <!-- DATA PREPERATION TITLE -->


                <div id="Configuring-Pandas">
                <!-- SETUP AND INSTRUCTIONS -->
                <h1 class="post_title"> Setup and Introduction </h1>
                <p class="lesson-text">    In order to get started, make sure you have your environment set up. In order to do this, I recommend referencing these links:</p>
                
                <ul class = "bullets">
                    <li>  You can download Python <a href="https://www.python.org/downloads/"><u>here</u></a>.</li>

                    <li>  Most Python code written here will be written on a Jupyter Notebook. You can reference 
                        <a href="https://jupyter.org/install"><u>this guide</u></a> on Jupyter's site for installation.</li>
  
                    <li>  Some important libraries we'll be using for data preprocessing include pandas, numpy, matplotlib, and seaborn. All of which can be installed
                        following these <a href="https://docs.python.org/3/installing/index.html"><u>guidelines</u></a>.
                    </li>
                    <li> You can download the CSV file we'll be referencing <a href="supplements/Sample_Stocks.csv" download><u>here</u></a>.</li>
                </ul>
                </div>


                <!-- Structuring Data -->
                <div id="Structuring-Data">
                <h1 class="post_title"> Structuring Data </h1>
                <p class="lesson-text">    The first thing we have to do is load our data into our Jupyter Notebook. To open up your Jupyter Notebook, 
                    simply open up terminal and type <code>jupyter notebook</code>. A full guide on the interface can be found 
                    <a href="https://jupyter-notebook.readthedocs.io/en/latest/notebook.html"><u>here</u></a>.
                    Let's first import pandas into our notebook. We'll reference it as "pd" for simplicity.
                    <br><br>
                    <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-keyword" style="font-weight: 700; color: navy;">import</span> pandas <span class="hljs-keyword" style="font-weight: 700; color: navy;">as</span> pd</pre>
                </p>
                <p class="lesson-text"> Pandas is a powerful tool used for manipulating data providing functioanlities to make working with out data easier. The CSV file referenced in this part of the tutorial can be downloaded 
                    <a href="supplements/Sample_Stocks.csv" download><u>here</u></a>. The first powerful tool in pandas we'll make use of is a dataframe.
                    You can think of a dataframe like a spreadsheet or data table. It's a data structure representing a two-dimensional table. Let's read in our data from CSV into a dataframe object and look at the first few rows of our data.
                    <br><br>
                    <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-attribute" style="color: rgb(0, 85, 0);">stock_data</span> = pd.read_csv(<span class="hljs-string" style="color: rgb(0, 85, 0);">'Sample_Stocks.csv'</span>) <span class="hljs-comment" style="color: rgb(136, 0, 0);"></span>
stock_data.head()   </pre>
                    <br>
                    <img  src="images/headfunction.png" alt="Head Output" />
                </p>
                <p class="lesson-text"> 
                    
                    The <code>head(n)</code> function displays the first few rows of our table. You can specify the last few rows using tail(). Placing a integer parameter (<code>head(n) or tail(n)</code>) displays the first or last <code>n</code> rows. We can get summary statistics of our data as well through the <code>describe()</code> function. 
                    <br><br>
                    <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);">stock_data[<span class="hljs-string" style="color: rgb(0, 85, 0);">'HINDEX'</span>].describe() <span class="hljs-comment" style="color: rgb(136, 0, 0);"># Describes the HINDEX attribute</span>
stock_data.groupby(<span class="hljs-keyword" style="font-weight: 700; color: navy;">by</span>=<span class="hljs-string" style="color: rgb(0, 85, 0);">'Stock'</span>).describe() <span class="hljs-comment" style="color: rgb(136, 0, 0);"># Describes all attributes grouped by 'Stock'</span>
stock_data.groupby(<span class="hljs-keyword" style="font-weight: 700; color: navy;">by</span>=<span class="hljs-string" style="color: rgb(0, 85, 0);">'Stock'</span>)[<span class="hljs-string" style="color: rgb(0, 85, 0);">'Volume'</span>].describe() <span class="hljs-comment" style="color: rgb(136, 0, 0);"># Output shown below</span></pre>    
                    <br>
                    <img  src="images/volumedescribe.png" alt="Volume describe() Output" />        
                </p>
                <p class="lesson-text"> 
                    After looking through the columns, you may realize "HINDEX" is relatively useless as its always zero. We can delete 
                    this column using: 
                    <br><br><pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);">stock_data.drop(columns = [<span class="hljs-string" style="color: rgb(0, 85, 0);">'HINDEX'</span>])</pre> 
                </p>
                <p class="lesson-text"> 
                    Using SQL, we were able to query a database in order to retrieve specific information. Thankfully, pandas provides a similar functionality. Let's
                    say we were interested in finding all the entries of stock A with a price of over $46.00. We can do this by using conditionals within
                    the pandas <code>loc</code> function. Let's try this out. 
                        <br><br><pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);">stock_data.loc[(stock_data[<span class="hljs-string" style="color: rgb(0, 85, 0);">'Stock'</span>]==<span class="hljs-string" style="color: rgb(0, 85, 0);">'A'</span>) &amp; (stock_data[<span class="hljs-string" style="color: rgb(0, 85, 0);">'Price'</span>]&gt;<span class="hljs-number" style="color: rgb(136, 0, 0);">46</span>)]</pre>
                        
                </p>
                <p class="lesson-text"> 
                    When we run this, we get an error message saying <code>TypeError: '>' not supported between instances of 'str' and 'int'</code>
                    What this message is telling us is we're trying to compare a string type variable to an integer. We'll have to 
                    convert the 'Price' column into floating point numbers before running our query. We can make use of the <code>replace()</code> and <code>astype()</code> functions.<br><br><pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);">stock_data[<span class="hljs-string" style="color: rgb(0, 85, 0);">'Price'</span>] = stock_data[<span class="hljs-string" style="color: rgb(0, 85, 0);">'Price'</span>].replace(<span class="hljs-string" style="color: rgb(0, 85, 0);">'[\$,]'</span>, <span class="hljs-string" style="color: rgb(0, 85, 0);">''</span>, regex=<span class="hljs-symbol" style="color: rgb(0, 85, 0);">True</span>).astype(float)
stock_data.loc[(stock_data[<span class="hljs-string" style="color: rgb(0, 85, 0);">'Stock'</span>]==<span class="hljs-string" style="color: rgb(0, 85, 0);">'A'</span>) &amp; (stock_data[<span class="hljs-string" style="color: rgb(0, 85, 0);">'Price'</span>]&gt;<span class="hljs-number" style="color: rgb(136, 0, 0);">46</span>)]</pre>      
                <br>
                <img  src="images/pandasquery.png" alt="Query Result" />
                </p>
                <p class="lesson-text"> 
                        So far, we've looked into the very basics of pandas. We'll continue to make use of this library
                        in future lessons which is why I highly recommend getting more comfortable with the pandas library and its
                        various functionalities. This will make understanding future lessons much easier.
                        Feel free to utilize the references provided in the <a href="#1Resources"><u>Useful Resources</u></a> section.
                    </p>

                </div>
               

                <div id="Missing-Values">
                        <h1 class="post_title"> Missing Data </h1>
                        <p class="lesson-text">    Part of cleaning your data is dealing with missing values. One way to detect if your dataset has missing values
                            is to use the <code>.info()</code> function. Using the dataset from the <a href="#Structuring-Data"><u>previous lesson</u></a>,  <code>stock_data.info()</code> outputs the following:
                            <br><br>
                            <img  src="images/nulldetection.png" alt="info() output" />
                            <br><br>
                            <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html"><u><strong><code>dropna()</code></strong></u></a><br>
                            We can see our dataset only has 35 non-null price values and 37 non-null volume values out of 40 total values.
                            One way to deal with missing values is to delete the rows that have at least one value missing. Using pandas,
                            this can be done through the <code>dropna()</code> function, which will return a dataframe with all rows containing
                            missing values deleted. <br>
                            <br>
                            <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html"><u><strong><code>fillna()</code></strong></u></a><br>
                            However, deleted entries may not always be the best option, especially when 
                            the entry has other important information we might want to keep. This leads us to our first method of handling
                            missing data using the <code>fillna()</code> function. The <code>fillna()</code> function
                            allows us to quickly find all NaN values and replace them using a specified method. You can learn more about 
                            <code>fillna()</code> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html"><u>here</u></a>.
                            One way to use <code>fillna()</code> is with the parameter <code>method = 'ffill' </code>. This parameter
                            propogates all non-null values forward, replacing each null value with the previous non-null value.
                            Similarly <code>method = 'bfil'</code> does the opposite. <br><br>
                            <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);">stock_data.drop(columns = [<span class="hljs-string" style="color: rgb(0, 85, 0);">'HINDEX'</span>], inplace = <span class="hljs-keyword" style="font-weight: 700; color: navy;">True</span>)
stock_data</pre>
                            <br>
                            <img  src="images/fillna2.png" alt="FFill" />
                            </p>
                            <p class="lesson-text"> Previously the values for 'Price' and 'Volume' for the entry at index 12
                                were both NaN. After using the <code>ffill</code> method of <code>fillna()</code> the values for 'Price'
                                and 'Volume' were replaced with the 'Price' and 'Volume' values for the entry at index 11. A snippet showing this change is
                                shown above. The inplace parameter in the above function call signifies that the outputted dataframe
                                should replace our current dataframe.
                                <br>
                                <br>
                                <a href="https://pandas.pydata.org/pandas-docs/version/0.16/generated/pandas.DataFrame.interpolate.html"><u><strong><code>interpolate()</code></strong></u></a><br>
                                Another way to fill missing values, especially useful for time-series data, is the interpolate method. Interpolating data
                                is very helpful and there are many ways to do it. One way of the simplest ways that does not involve the fitting
                                of a complex model to our data is using the <code>interpolate()</code> function. It defaults to a linear method
                                which replaces all missing values with the <i>linear average</i> of the non-null values surrounding it. It can be called on
                                an entire dataframe object like shown below, however, is often useful for individual serieses as well (like our 'Price' column alone). Read more about that
                                <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.interpolate.html"><u>here</u></a>.
                                <br><br>
                                <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);">stock_data.interpolate(<span class="hljs-function"><span class="hljs-keyword" style="font-weight: 700; color: navy;">method</span>='<span class="hljs-title" style="font-weight: 700; color: navy;">linear</span>', <span class="hljs-title" style="font-weight: 700; color: navy;">inplace</span>=<span class="hljs-title" style="font-weight: 700; color: navy;">True</span>) </span></pre>
                                <br>
                                <img  src="images/interpolate.png" alt="interpolate()" />

                </div>

                <div id="Pandas-Resources">
                    <h1 class="post_title"> Useful Resources </h1>
                    <p class="lesson-text">    So far, we've looked into the basics of manipulating our data with the pandas library. The following links and guidelines
                        are extremely useful and I highly recommend looking into them to get a better idea of what we've learned:</p>
                
                <ul class = "bullets">
                    <li>  The full pandas documentation that outlines the various functions and required parameters
                        within the pandas library can be found <a href="https://pandas.pydata.org/pandas-docs/stable/"><u>here</u></a>.</li>

                    <li>  Another important library to look into is the numpy library (we'll start using this soon), which makes working with arrays
                        much faster and easier. See documentation  
                        <a href="https://docs.scipy.org/doc/numpy/user/basics.html"><u>here</u></a>.</li>
  
                    <li>  Here's a useful "cheatsheet" I used to use that goes over commonly used pandas functionalities.  
                        <a href="https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html"><u>Cheatsheet</u></a>.
                    </li>
                </ul>
                    </p>
                </div>
                




















                <!-- DATA EXPLORATION TITLE -->
                <h1 id = "DE" class="topic_title">Data Exploration </h1>
                <!-- DATA EXPLORATION TITLE -->
                <div id="Explore-Setup">
                        <!-- SETUP AND INSTRUCTIONS -->
                        <h1 class="post_title"> Setup and Introduction</h1>
                        <p class="lesson-text">
                            If you don't have your environment set up yet. Please reference <a href="#Configuring-Pandas"><u>this guide</u></a>. Throughout this lesson
                            we'll make use of multiple Python libraries. If you see a library you haven't previously used, simply install the library 
                            using <code>pip install</code>. For reference, here's the <code>pip install</code> <a href="https://pip.pypa.io/en/stable/reference/pip_install/"><u>documentation</u></a>.
                            Download the CSV file we'll be working with <a href="supplements/iris.csv" download><u>here</u></a>.
                            <br><br>
                            Before we get started, let's recap what we've gone through so far. We've learned how to query a database, import data,
                            manipulate data tables, and other techniques involved in cleaning and aggregating data. Now that we have our DataFrames
                            setup, we can begin exploring our variables. This initial "exploration" will help us choose and create a model later on as well
                            as give us insight into our data.
                        </p>
                </div>
                <div id="Univariate">
                        <!-- UNIVARIATE -->
                        <h1 class="post_title"> Univariate Exploration </h1>
                        <p class="lesson-text"> The first step to looking into our now "cleaned" data is
                            to perform a univariate analysis. A univariate analysis is when we look at the characteristics
                            and plots of each variable <i>individually</i> in order to gain insight into our variables. We'll
                            be making use of a few new libraries like Matplotlib and Seaborn, which you should install now if you plan
                            on following along. (See <a href="#Explore-Setup"><u>setup</u></a> for more information.) The best way
                            to start looking into your data is to visualize your data. For this exercise we'll use the
                            following <a href="#Explore-Setup"><u>CSV</u></a> file. The dataset we're using is
                            the Iris dataset (read more <a href="https://archive.ics.uci.edu/ml/datasets/Iris"><u>here</u></a>). 
                            Even though this dataset is preloaded in <code>sklearn</code> library, it's best we practice importing
                            our own datasets. Here is the code requried
                            to read in our data and get it ready for exploration. Feel free to copy and paste. <br><br>
                            <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-keyword" style="font-weight: 700; color: navy;">import</span> pandas <span class="hljs-keyword" style="font-weight: 700; color: navy;">as</span> pd
<span class="hljs-keyword" style="font-weight: 700; color: navy;">import</span> numpy <span class="hljs-keyword" style="font-weight: 700; color: navy;">as</span> np
<span class="hljs-keyword" style="font-weight: 700; color: navy;">import</span> matplotlib.pyplot <span class="hljs-keyword" style="font-weight: 700; color: navy;">as</span> plt
<span class="hljs-keyword" style="font-weight: 700; color: navy;">import</span> seaborn <span class="hljs-keyword" style="font-weight: 700; color: navy;">as</span> sns
                                
iris = pd.read_csv(<span class="hljs-string" style="color: rgb(0, 85, 0);">"iris.csv"</span>)
iris.columns=[<span class="hljs-string" style="color: rgb(0, 85, 0);">'Sepal Length'</span>, <span class="hljs-string" style="color: rgb(0, 85, 0);">'Sepal Width'</span>, <span class="hljs-string" style="color: rgb(0, 85, 0);">'Petal Length'</span>, <span class="hljs-string" style="color: rgb(0, 85, 0);">'Petal Width'</span>, <span class="hljs-string" style="color: rgb(0, 85, 0);">'Class'</span>]</pre>

                </p>
                <p class="lesson-text">
                    If you remember from the Data Preperation lessons, we learned that the <code>describe()</code> function
                    of dataframes outputs summary statistics of our data. We can do that now and also develop a couple
                    visualizations. <br><br>
                    <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-symbol" style="color: rgb(0, 85, 0);"># Defining subplots</span> 
fig,axes=plt.subplots(<span class="hljs-number" style="color: rgb(136, 0, 0);">2</span>,<span class="hljs-number" style="color: rgb(136, 0, 0);">2</span>, figsize=(<span class="hljs-number" style="color: rgb(136, 0, 0);">12</span>, <span class="hljs-number" style="color: rgb(136, 0, 0);">9</span>))
                        
<span class="hljs-symbol" style="color: rgb(0, 85, 0);"># Visualizing each numeric attribute</span> 
sns.distplot(iris[<span class="hljs-string" style="color: rgb(0, 85, 0);">'Sepal Length'</span>], ax=axes[<span class="hljs-number" style="color: rgb(136, 0, 0);">0</span>,<span class="hljs-number" style="color: rgb(136, 0, 0);">0</span>])
sns.distplot(iris[<span class="hljs-string" style="color: rgb(0, 85, 0);">'Sepal Width'</span>], ax=axes[<span class="hljs-number" style="color: rgb(136, 0, 0);">0</span>,<span class="hljs-number" style="color: rgb(136, 0, 0);">1</span>])
sns.distplot(iris[<span class="hljs-string" style="color: rgb(0, 85, 0);">'Petal Length'</span>], ax=axes[<span class="hljs-number" style="color: rgb(136, 0, 0);">1</span>,<span class="hljs-number" style="color: rgb(136, 0, 0);">0</span>])
sns.distplot(iris[<span class="hljs-string" style="color: rgb(0, 85, 0);">'Petal Width'</span>], ax=axes[<span class="hljs-number" style="color: rgb(136, 0, 0);">1</span>,<span class="hljs-number" style="color: rgb(136, 0, 0);">1</span>])
                        
<span class="hljs-symbol" style="color: rgb(0, 85, 0);"># Displaying figures</span> 
plt.show()</pre><br>
                    <img  src="images/histo.png" alt="Histograms" /> <br><br></p>
                    <p class="lesson-text">
                    These histograms give us insight into our data by showing us each distribution. For instance,
                    we can see that sepal length and sepal width display unimodality (one mode/peak), whereas petal length
                    and petal width seem more bimodal. We can go one step furthur and look into the kurtosis and skew our each attribute.
                    The kurtosis measures how close our values are to our mean, you can think of it as how "steep" our distribution slopes around
                    the its mean. Skew on the otherhand shows us if we have more observations on the tails of our data, in other words
                    it shows us if our distribution favors the left or right side. <br><br>
                    <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-function"><span class="hljs-title" style="font-weight: 700; color: navy;">print</span><span class="hljs-params">(<span class="hljs-string" style="color: rgb(0, 85, 0);">"Kurtosis"</span>)</span></span>
<span class="hljs-function"><span class="hljs-title" style="font-weight: 700; color: navy;">print</span><span class="hljs-params">(iris.kurtosis()</span></span>)
<span class="hljs-function"><span class="hljs-title" style="font-weight: 700; color: navy;">print</span><span class="hljs-params">(<span class="hljs-string" style="color: rgb(0, 85, 0);">"Skew"</span>)</span></span>
<span class="hljs-function"><span class="hljs-title" style="font-weight: 700; color: navy;">print</span><span class="hljs-params">(iris.skew()</span></span>)</pre> <br>
                    <img  src="images/kurtosis.png" alt="Kurtosis and Skew" /></p>
                    <p class="lesson-text">
                    Great! We have our kurtosis and skew values, but what to they mean? A negative skew value indicates that
                    our data has more observations to the right of the mean, also known as left-skewed. If our value is positive,
                    our data is considered right-skewed. However, the magnitude of the skewness values also matter. Our skewness
                    values are between -0.5 and 0.5 which means our data is relatively symmetric. A larger skewness value tends to mean
                    your data is more heavily skewed. In general a skewness value between -1 and 1 is considered acceptable (however
                    these values aren't set in stone). In terms of kurtosis, a positive kurtosis shows that our data is steep
                    near the mean, while a negative kurtosis means our "hump" is relatively flat. Like skewness, the magnitude of the
                    kurtosis value is important. The higher the magnitude, the more observations are clustered near the mean. In general, 
                    kurtosis values between -2 and 2 are considered acceptable. <br><br>
                    One more thing we can do is look at a box-plot of our data. A box-plot will show us our interquartile range (IQR)
                    which represents the range of values between the 25th and 75th percentile. In addition, a box-plot will show us if we have any outliers, 
                    something we'll learn how to deal with next.<br><br> 
                    <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-title" style="font-weight: 700; color: navy;">sns</span>.boxplot(<span class="hljs-class"><span class="hljs-keyword" style="font-weight: 700; color: navy;">data</span>=iris)</span></pre><br>
                    <img  id="Out" src="images/boxplot.png" alt="Box-plot" /></p>
                    <p class="lesson-text">
                    As you can see, we potentially may have outliers in our sepal width attribute. The outliers are denoted by the points
                    that fall outside the box-plot. There are many ways to deal with outliers, one of which is removing all entries where outliers
                    exist. However, determining what should be done with outliers depends on our model. We can train and test our model with
                    and without outliers to see performance. In general, it is a good idea to deal with outliers early to prevent
                    our model from being trained on observations that aren't representative of our dataset as a whole. In order to deal with
                    outliers, we'll first have to import stats from the scipy library and then remove outlier observations. <br><br>
                    <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-keyword" style="font-weight: 700; color: navy;">from</span> scipy <span class="hljs-keyword" style="font-weight: 700; color: navy;">import</span> stats
<span class="hljs-function"><span class="hljs-title" style="font-weight: 700; color: navy;">print</span><span class="hljs-params">(iris.size)</span></span>
<span class="hljs-keyword" style="font-weight: 700; color: navy;">for</span> column <span class="hljs-keyword" style="font-weight: 700; color: navy;">in</span> iris<span class="hljs-selector-class">.columns</span>:
    <span class="hljs-keyword" style="font-weight: 700; color: navy;">if</span> column != <span class="hljs-string" style="color: rgb(0, 85, 0);">'Class'</span>:
        iris = iris[(np.abs(stats.zscore(iris[column])))&lt;<span class="hljs-number" style="color: rgb(136, 0, 0);">3</span>]
<span class="hljs-function"><span class="hljs-title" style="font-weight: 700; color: navy;">print</span><span class="hljs-params">(iris.size)</span></span>   </pre></p>
                    <p class="lesson-text">    
                    What we've done here is removed all the outliers from our data. The way we did this was by looking at something called a z-score, which 
                    represents how many standard deviations an observation is from the mean. Generally, any observation with a z-score over 3 (3 standard deviations
                    from the mean), is considered an outlier. In this code snippet, we iterated through each column and updated our dataframe
                    with only observations which had a z-score of less than 3. What's interested is when looking at the size of our dataframe before and after,
                    no observations were deleted, in other words, out data had no outliers.<br><br>
                    Something you may have wondering is that why our box-plot displayed outliers while our z-score checker did not. The reason for the
                    difference is how outliers are calculated. The z-score looks at the standard deviations an observation is from the mean (read more about z-scores <a href="https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/z-score/"><u>here</u></a>.)
                    With box-plots, outliers are usually determined by looking at how many <i>IQRs</i> our data is from our mean. Data outside <code>k x IQR</code> (where <code>k</code> is usually 1.5)
                    from the quartiles denote outliers in a box-whisker plot. 
                </p>
                </div>


                <div id="Bivariate">
                        <!-- BIVARIATE -->
                        <h1 class="post_title"> Bivariate Exploration </h1>
                        <p class="lesson-text"> In our previous lesson, we learned how to look into the attributes of our dataset and
                            utilize visualizations to explore our data. Often times, we're interested in how variables relate to each other, leading to
                            bivariate and multivariate data exploration. <br><br>
                            To start off, we'll look at how our variables are correlated with each other through a <i>pairs plot</i>, a pairs plot
                            is a scatterplot of two specific attributes. We can use <code>seaborn</code> to create these.<br><br>
                            <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);">sns.<span class="hljs-title" style="font-weight: 700; color: navy;">pairplot</span>(iris, hue = <span class="hljs-string" style="color: rgb(0, 85, 0);">'Class'</span>);</pre><br>
                            <img  src="images/pairsplot.png" alt="Pairs Plot" /></p>
                            <p class="lesson-text">
                        I know the plot may look daunting, but it's very easy to understand. The points are colored based on the 'Class'
                        of each observation. We did this by passing in the Class attribute for the hue paramater. On either axis of the overall figure
                        are each of our attributes. Each individual figure corresponds to a scatter plot of those attributes. For instance, in the figure on the top row
                        second column, attribute 'Sepal Width' is on the x-axis while attribute 'Sepal Length' is on the y-axis. On the diagonal you'll notice
                        rather than scatterplots, distributions are shown. Each distribution of each figure along the diagonal, shows the distribution of that specific
                        attribute for each Class. <br><br>
                        The next step is to see what insight we can get from these figures. Looking at the figures, it seems that iris-virginica Class 
                        tends to be the largest in terms of dimensions while the iris-setosa seems to be the smallest. Another thing you can notice is that on the
                        distributions along the diagonal, the peaks and steepnesses aren't the same across classes. This is due to different kurtosis and skew values. If you
                        look at our kurtosis and skew calculations from the previous lecture, you'll be able to visualize how these metrics translate to a distribution. Something else
                        you'll notice is that our plot for petal length and petal width seem to follow a strong linear correlation. With pair plots, we can look into how correlated
                        variables are with each other. 
                        <br><br>
                        However, there's an even better way to quantify and visualize the correlation between out attributes using Heatmaps and correlation matricies. 
                        Let's start off by getting the correlation matrix of our attributes.<br><br>
                        <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-name" style="font-weight: 700; color: navy;">print(</span>iris.corr())</pre><br>
                        <img  src="images/corrmat.png" alt="Correlation Matrix" /></p>    
                        <p class="lesson-text">           
                        Like in our pairs plot, the x and y axis show our attributes while the values show the correlation between those attributes. Obviously,
                        variables will be 100% correlated with themselves, which is why our diagonal is 1. After looking at the values you'll notice we were correct
                        in saying petal length and petal width are correlated, with a correlation of 0.96. We'll notice other variables are also strongly correlated with each other
                        like sepal length and petal length. 
                        As useful as the correlation matrix is, it'll be helpful to actually <i>visualize</i> the matrix. We can do this with a Heat Map. <br><br>
                        <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);">f, <span class="hljs-attr">ax</span> = plt.subplots(<span class="hljs-attr">figsize=(12,</span> <span class="hljs-number" style="color: rgb(136, 0, 0);">9</span>))
sns.heatmap(corr_mat, <span class="hljs-attr">vmax=1,</span> <span class="hljs-attr">square=True,</span> <span class="hljs-attr">annot=True);</span></pre><br>
                        <img  src="images/heatmap.png" alt="Heat Map" />   
                        </p>
                        <p class="lesson-text"> 
                        Our heat map shows us a visual version of our correlation matrix. our <code>vmax</code> parameter sets the maximum value scale for our correlation. The <code>square</code> parameter
                        gives us both sides of our matrix rather than a "staircase" version. The <code>annot</code> parameter displays the numeric value for the correlation within each square.                     
                        </p>

                    </div>
                    <div id="Feature-Engineering">
                            <!-- feature-engineering -->
                            <h1 class="post_title"> Feature Engineering </h1>

                            <p class="lesson-text"> Now that we've explored our data, it's time for us to develop the features for our model. We need to alter the features we'll use in our model as well as potentially transform
                                these features in order to help develop stronger predictive models. What concepts are considered part of feature engineering and what aren't is subjective. Think of feature engineering as the final step
                            in figuring out the best way to change our features to better be able to predict our target. When working with our dataset in this lesson, we'll be using a lot of the concepts from the Data Pre-Processing series. 
                            Instead of continuing our use of the Iris dataset, let's look into a dataset that has more features. This will help make understanding certain
                            feature engineering concepts easier (we'll go back to the Iris dataset for the next lesson). For this lesson, we'll use the Capital Bikeshare Service dataset, a dataset about a bikesharing service in the Metro DC area (download <a href="supplements/bikeshare.csv" download><u>here</u></a> data from Kaggle). Our goal is to create a model
                            to predict bike traffic (out 'cnt' column).</p>
                            Let's first take a look at our data.<br><br>
                            <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-keyword" style="font-weight: 700; color: navy;">import</span> pandas <span class="hljs-keyword" style="font-weight: 700; color: navy;">as</span> pd

bikedata = pd.read_csv(<span class="hljs-string" style="color: rgb(0, 85, 0);">'bikeshare.csv'</span>)
bikedata.head()</pre>
                                <br>
                                <img src="images/bikehead.png">
                            </p>
                            <p class="lesson-text"> 
                                You'll notice a couple things right off the bat. First, our column "instant" is just an indexing column and won't be a feature in our regression model. Since we already have dataframe indicies, we can get rid of this column.
                                Next, you'll notice that our 'dteday', 'season', 'yr', 'mnth', 'weekday', and 'workingday' attributes all give us information about the date. In our model, we should try to use as few features as possible while keeping our model accurate.
                                Given that the information from these attributes is similar, let's see which ones are the most useful. Since we're not doing a time-series analysis, the "dteday" feature can be removed. Likewise, 'yr' can also be removed.
                                The 'season' and 'mnth' attribute are related to each other since the season is determined by the month. Bike usage would probably change the most season to season rather than month to month, since seasons have much more distinguishable
                                weather factors. Thus, we can remove the 'mnth' feature as well. The 'weekday' column is related to the 'workingday' column by distinguishing between weekdays and weekends. Since traffic is more likely affected by weekday vs. weekend rather than
                                two arbitrary days of the week, we'l keep the 'workingday' feature and remove the 'weekday' feature. One feature we glossed over is the 'holiday' feature. The holiday feature has two values, 0 for not a holiday or 1 for a holiday. Let's quickly look into
                                how many holidays we have. We can do this by running.<br><br>
                                <code>bikedata[bikedata['holiday']==1].count()[0]</code>
                                <br><br>
                                You'll notice out of our 731 total entries, only 21 are holidays - thats roughly 2.5% of our dataset. One thing we haven't learned yet is the concept of training and testing sets. That's when we divide our dataset into two sets to fit our model and test its performance.
                                Having a binary feature with very few "true" instances could throw our model off during training/testing. This is because we're not sure that the holiday feature we'll show up evenly. Although we can get around this using stratified k-fold cross-validation (we'll also get into that later),
                                it's better for us to remove the feature altogether as it may introduce unnecessary variance.
                                <br><br>
                                Often times, you won't be lucky enough to have a dataset that already has the dates split into weekdays, seasons, etc. This dataset has clearly already been pre-processed. If you're only given dates and want to create new features. Simply create a new column
                                using conditional statements based on the date. 
                                Now let's look at our non-date data. To see a reference as to what each column signifies, go <a href="https://www.kaggle.com/marklvl/bike-sharing-dataset"><u>here</u></a>. Our 'atemp' and 'temp' features are obviously very correlated and will have 
                                <i>multicollinearity</i>. We can deal with this multicollinearity through dimensionality reduction (we'll get more into that in the next lecture), or pure feature selection. In this case, we'll just choose one of them. 'atemp' represents the temperature
                                <i>felt</i> rather than the actual temperature. Since people are more likely to base their judgements on their <i>perception</i> of the weather, we'll choose to only look at 'atemp'. We also have two features that split up the total count of bike traffic into
                                'casual', and 'registered'. Since at this point it's difficult to logically determine whether or not our features would be more correlated to one over the other, we'll choose to keep both. So now after pure logical feature selection our dataframe looks like this:<br><br>
                                <img src="images/updatedbikehead.png">
                                <br><br>
                                One more thing we can do is categorize our numerical data. Another term used for this is <i>encoding</i>. In school, 90-100 is an A, 80-89 is a B, 70-79 is a C, etc. This is an example of a <i>numerical mapping</i>,
                                where certain ranges correspond to certain categorical variables or bins. When looking at our data, one feature I think could benefit from this sort of encoding is windspeed. Most people split days into either windy or not windy when making decisions so why shouldn't we?
                                We can now bin our 'windspeed' feature. It's important to make sure each of your bins has enough representation in your total dataset, this will help reduce variance (we'll get more into that later) within your model's results.
                                <br>
                                Let's first get a summary of our 'windspeed' feature using the <code>describe()</code> function.<br><br>
                                <img src="images/windhead.png"><br><br>
                                We can <i>bin</i> our 'windspeed' data assigning it to a category mapped to its quartile. Let's create a list of [min, 25th percentile, 50th percentile, 75th percentile, max]. Then we'll encode our 'windspeed'
                                into a categorical bin. <strong>NOTE: 'windspeed' will now become a categorical variable.</strong> <br><br>
                                <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);">bins = [bikedata[<span class="hljs-string" style="color: rgb(0, 85, 0);">'windspeed'</span>].describe()[<span class="hljs-number" style="color: rgb(136, 0, 0);">3</span>],bikedata[<span class="hljs-string" style="color: rgb(0, 85, 0);">'windspeed'</span>].describe()[<span class="hljs-number" style="color: rgb(136, 0, 0);">4</span>],
        bikedata[<span class="hljs-string" style="color: rgb(0, 85, 0);">'windspeed'</span>].describe()[<span class="hljs-number" style="color: rgb(136, 0, 0);">5</span>],bikedata[<span class="hljs-string" style="color: rgb(0, 85, 0);">'windspeed'</span>].describe()[<span class="hljs-number" style="color: rgb(136, 0, 0);">6</span>],bikedata[<span class="hljs-string" style="color: rgb(0, 85, 0);">'windspeed'</span>].describe()[<span class="hljs-number" style="color: rgb(136, 0, 0);">7</span>]]
bikedata[<span class="hljs-string" style="color: rgb(0, 85, 0);">'windspeed'</span>] = pd.cut(bikedata[<span class="hljs-string" style="color: rgb(0, 85, 0);">'windspeed'</span>], bins)
bikedata[<span class="hljs-string" style="color: rgb(0, 85, 0);">'windspeed'</span>].head(<span class="hljs-number" style="color: rgb(136, 0, 0);">10</span>)</pre><br>
                                <img src="images/windbin.png">
                            </p>
                            <p class="lesson-text"> 
                                As you can see in our 'windspeed' column, our speeds have been put into categories. This can help with categorical analysis in looking at regressions across various 'windspeed' bins. Something else we can do is engineer our own features
                                through something called <i>feature interaction</i>. This means we can combine columns. For instance, let's say 'weathersit' was not given to us as a feature. We could calculate it by creating a new column called 'weathersit', and
                                assigning values based on conditional statements around windspeed, humidity, temperature, etc. We could say if the temperature is within some range, the wind is below some value, and the humidity is between a certain range, that the 'weathersit' is "2" or "okay".
                                Like I mentioned earlier, the dataset we imported has already been preprocessed by the uploader and no features like this need to be created.
                                <br><br>
                                One important concept we skimmed over is <i>multicollinearity</i>. Multicollinearity is a fancy way of saying our features are correlated with each other. For instance, the season and temperature features may be correlated since temperatures vary according to season.
                                Likewise, the weather situation ('weathersit') would be correlated with one of our weather features. One method we'll go into in our LASSO lesson is using L1 regularization for feature selection to address this multicollinearity. In the next lesson,
                                we'll go through a common model used for dimensionality reduction (we'll learn this in the next lesson), called a Principal Component Analysis or PCA.


                            </p>
                        
                        
                        </div>

                    <div id="PCA">
                                <!-- pca -->
                                <h1 class="post_title"> Principal Component Analysis with Singular Value Decomposition</h1>

                                <p class="lesson-text"> In the previous lesson, we engineered our features in our bikeshare dataset and made it more ready for modeling. In this lesson we'll use a concept called
                                    Principal Component Analysis (PCA). A PCA can help us visualize our dataset when we have many features and can also help us with dimensionality reduction. Before we get into how to implement a PCA.
                                    Let's understand how it works. Before I get started I highly recommend checking out StatQuest's video on PCA. 
                                    It's only 20 minutes and is extremely useful. Much of what I'll go over is expanded on in in-depth his video (view video <a href="https://www.youtube.com/watch?v=FgakZw6K1QQ"><u>here</u></a>).<br>
                                    <br>
                                    Shown below is a scatterplot of height and weight of various men (blue) and women (pink). <br>
                                    <img src="images/pca1.png" ><br>
                                    What we'll do next is find the overall average weight and height and shift our axis so that
                                    (Average Weight, Average Height) is our <i>new origin</i>. Our plot now looks something like this: <br><br>
                                    <img src="images/pca2.png" ><img src="images/pca3.png" ><br>
                                    Our next step will be to find a line of best fit for our shifted plot. This will be called the first <strong>principal component</strong> (PC1).
                                    The unit vector of this principal component is known as the <strong>eigenvector</strong> or <strong>singular vector</strong> for PC<sub>1</sub>.<br>
                                    How do we find that line? We'll use this diagram to help:<br><br>
                                    <img src="images/pca4.png" ><br>
                                    In this diagram, the blue circle represents the data point while the green line represents PC<sub>1</sub>. The
                                    red line represents the distance from the point to the origin, we'll call this distance 'c'. It's important to note that 
                                    this distance never changes, regardless of where the line is. The purple line represents the distance the point is from the 
                                    principal component. One way to find the line of best fit is to try to minimize 'a'. This is usually done by minimizing the sum of the distances
                                    squared. That value is known as the <strong>eigenvalue</strong> for that principal component and the square root of the eigenvalue is called the <strong>singular value</strong> for that 
                                    principal component.<br>
                                    One thing you'll see is that side lengths a,b, and c form a right triangle. 'c', as we mentioned earlier, is constant. According to the Pythagorean Theorem
                                    a<sup>2</sup> + b<sup>2</sup> = c <sup>2</sup>. This means that minimizing 'a' is the same thing as maximizing 'b'. A PCA maximizes 'b' by maximizing
                                    the sum squared of 'b' across all datapoints.<br><br>
                                    Once the PC<sub>1</sub> is found, PC<sub>2</sub> is simply the best-fit line perpendicular to PC<sub>1</sub>. If we had more features,
                                    we could add even more principal components, all of which would be the best-fit line orthogonal to all the others. We'll stick with two dimensions for the sake of
                                    simplicity. Our PC<sub>1</sub> and PC<sub>2</sub> look something like this.<br><br>
                                    <img src="images/pca5.png" ><br>
                                    Now we'll rotate our plot so that PC<sub>1</sub> becomes our horizontal axis. Our plot now looks something like this:<br><br>
                                    <img src="images/pca6.png" ><br>
                                    If we had more than two features, to create the 2-D plot, we simply project the points from the other principal components on to PC<sub>1</sub>
                                    and PC<sub>2</sub>. <br><br>
                                    If you take the eigenvalue and divide it by (n-1), you get the variation from that principal component. You can compare the varations between different
                                    components to see which ones account for more variability within the dataset. A graph of these variations is called a <strong>scree plot</strong>
                                    and can be useful to determine how many features to keep in our final model.
                                    <br><br>
                                    Now that we understand the process of principal component analysis using singular value decomposition (PCA using SVD), let's look into how we can implement it.
                                    First, make sure you have your iris dataset properly loaded into a dataframe using the <code>read_csv</code> function. (To see the full
                                    instructions on setting up go <a href="#Univariate"><u>here</u></a>. The CSV file
                                    can be downloaded <a href="supplements/iris.csv" download><u>here</u></a>.<br><br>
                                    We'll start by standardizing our features. This means setting the mean and variance to 0 and 1 respectively. We can do this by subtracting the mean from each point (something we did above), and then dividing by the
                                    standard deviation. Luckily, sklearn makes this easy for us. <br><br>
                                    <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);">x = iris<span class="hljs-selector-class">.loc</span>[:, [<span class="hljs-string" style="color: rgb(0, 85, 0);">'Sepal Length'</span>, <span class="hljs-string" style="color: rgb(0, 85, 0);">'Sepal Width'</span>, <span class="hljs-string" style="color: rgb(0, 85, 0);">'Petal Length'</span>, <span class="hljs-string" style="color: rgb(0, 85, 0);">'Petal Width'</span>]]<span class="hljs-selector-class">.values</span>
y = iris<span class="hljs-selector-class">.loc</span>[:,[<span class="hljs-string" style="color: rgb(0, 85, 0);">'Class'</span>]]<span class="hljs-selector-class">.values</span>
x = StandardScaler().fit_transform(x)</pre> </p>
                                        <p class="lesson-text">
                                        All we're doing here is seperating our features from our target variable ('Class'), and standardizing our features (subtracting means and dividing by standard deviations).
                                        After we do this we can start our PCA. <br><br>
                                        <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-attr">pca</span> = PCA(<span class="hljs-number" style="color: rgb(136, 0, 0);">2</span>)
<span class="hljs-attr">projected</span> = pca.fit_transform(x)
<span class="hljs-attr">pca_x</span> = projected[:, <span class="hljs-number" style="color: rgb(136, 0, 0);">0</span>]
<span class="hljs-attr">pca_y</span> = projected[:, <span class="hljs-number" style="color: rgb(136, 0, 0);">1</span>]</pre>


                                </p> 
                                <p class="lesson-text">
                                        pca_x holds our PC<sub>1</sub> values while pca_y holds our PC<sub>2</sub> values. Remember, we originally had four features and now
                                        we only have two. Now let's visualize these components. <br><br>
                                        <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);">color_dict = {<span class="hljs-string" style="color: rgb(0, 85, 0);">'Iris-setosa'</span>:<span class="hljs-string" style="color: rgb(0, 85, 0);">'red'</span>,<span class="hljs-string" style="color: rgb(0, 85, 0);">'Iris-virginica'</span>:<span class="hljs-string" style="color: rgb(0, 85, 0);">'green'</span>,<span class="hljs-string" style="color: rgb(0, 85, 0);">'Iris-versicolor'</span>:<span class="hljs-string" style="color: rgb(0, 85, 0);">'blue'</span>}
color_list = [color_dict[label] <span class="hljs-keyword" style="font-weight: 700; color: navy;">for</span> <span class="hljs-selector-tag" style="font-weight: 700; color: navy;">label</span> <span class="hljs-keyword" style="font-weight: 700; color: navy;">in</span> iris[<span class="hljs-string" style="color: rgb(0, 85, 0);">'Class'</span>]]
plt.scatter(pca_x, pca_y,c=color_list, edgecolor=<span class="hljs-string" style="color: rgb(0, 85, 0);">''</span>, alpha=<span class="hljs-number" style="color: rgb(136, 0, 0);">0.5</span>)
plt.xlabel(<span class="hljs-string" style="color: rgb(0, 85, 0);">'Principal Component 1'</span>)
plt.ylabel(<span class="hljs-string" style="color: rgb(0, 85, 0);">'Principal Component 2'</span>)
plt.show()</pre><br>
                                            <img src="images/pca7.png">
                            
                                </p>
                                <p class="lesson-text">
                                    PCA is very useful for visualizing data as well as reducing the number of features we have to explore. One more thing we can do is use the line
                                    <code>pca.explained_variance_ratio_</code>. This returns a list showing the variance ratio (as we discussed earlier), for each principal component.
                                    This can help show the importance of different components and help determine how many features to include. 

                                </p>

                            </div>
                                
                    <div id="Explore-Resources">
                                <!-- USEFUL RESOURCES -->
                                <h1 class="post_title"> Useful Resources </h1>

                                <p class="lesson-text"> In this lesson, we learned how to explore our data and understand how our variables relate
                                    to each other. We went through a few functionalities of the seaborn and pandas libraries and a highly recommend learning how to
                                    tweak paramaters for various visualizations. 
                                </p>
                                
                                <ul class = "bullets">
                                    <li>  The following are the documentations for the functionalities we learned about. <br> <a href="https://seaborn.pydata.org/api.html"><u>Seaborn API</u></a> | 
                                    <a href="https://seaborn.pydata.org/generated/seaborn.distplot.html#seaborn.distplot"><u>distplot()</u></a> | <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.kurtosis.html"><u>kurtosis()</u></a> | 
                                    <a href="https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.DataFrame.skew.html"><u>skew()</u></a> | <a href="https://pythonbasics.org/seaborn_boxplot/"><u>boxplot()</u> | <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.zscore.html"><u>zscore()</u> | 
                                    <a href="https://seaborn.pydata.org/generated/seaborn.pairplot.html"><u>pairplot()</u></a> | <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html"><u>corr()</u></a> | <a href="https://seaborn.pydata.org/generated/seaborn.heatmap.html"><u>heatmap()</u></a>.            
                                    </li>
                
                                    <li>  Here's a helpful guide provided by the University of Minnesota on covariance and correlation. 
                                        <a href="http://users.stat.umn.edu/~helwig/notes/datamat-Notes.pdf"><u>Link</u></a>.</li>
                
                                </ul>

                                    </p>

                                    </div>
                                











                                    









                <!-- DATA MODELING TITLE -->
                <h1 id = "DM" class="topic_title">Data Modeling </h1>
                <!-- DATA MODELING TITLE -->
                <div id="Model-Setup">
                        <!-- SETUP AND INSTRUCTIONS -->
                        <h1 class="post_title"> Setup and Introduction</h1>
                        <p class="lesson-text">
                            If you don't have your environment set up yet. Please reference <a href="#Configuring-Pandas"><u>this guide</u></a>. Throughout this lesson
                            we'll make use of multiple Python libraries. If you see a library you haven't previously used, simply install the library 
                            using <code>pip install</code>. For reference, here's the <code>pip install</code> <a href="https://pip.pypa.io/en/stable/reference/pip_install/"><u>documentation</u></a>.
                            <br><br>
                            The next concept we'll be going over is modeling our data. Even though modeling data is extremely important, it's important to understand
                            that a majority of your time will be spent cleaning and exploring your data and thus, should be practiced heavily. In this series, we'll go over
                            some commonly used models, how they work, how to implement them, and when to use them. Feel free to use your own datasets when following along.
                        </p>
                </div>

                        <div id="Train-Test">
                                <!-- TRAIN AND TEST -->
                                <h1 class="post_title"> Training and Testing Sets</h1>
                                <p class="lesson-text">
                                    Before we start learning how to use models, it's important to understand what it means to "train" and "test" your data.
                                    When you "train" your data, you are fitting your model to a part of your dataset. "Testing" your data is when you feed the other part of your dataset
                                    into your newly trained model in order to see how it performs. You can see how it performs by comparing your model's forecasts with the actual values in your dataset.
                                    We'll be going much more in-depth on how to do all this in later lectures. Figuring out your train/test ratio is important to prevent under or overfitting.
                                
                                    Training your model too much on your training set may cause the model to be <i>overfitted</i> while not training your model enough may cause the model to be <i>underfitted</i>. A technique
                                    used to avoid overfitting is using another part of your data called a <i>validation set</i>. The validation set is really part of your training set - we'll get more into this in the next lesson. <br><br>
                                    You'll hear a lot of recommended train/test ratios like 70/30, 80/20, or 75/25 and train/test/validation ratios of 70/15/15 or 60/20/20. If your dataset is smaller, 
                                    a validation set may not be helpful and a 70/30 train/test split could work. With larger datasets, using cross-validation can help with fitting the model. We'll split our iris dataset from the previous lessons below.
                                    <br><br>
                                    <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-built_in" style="font-weight: 700; color: navy;">from</span> sklearn.model_selection <span class="hljs-built_in" style="font-weight: 700; color: navy;">import</span> train_test_split
X_train, X_test, y_train, y_test = <span class="hljs-built_in" style="font-weight: 700; color: navy;">train_test_split</span>(iris, iris[<span class="hljs-string" style="color: rgb(0, 85, 0);">'Class'</span>], <span class="hljs-built_in" style="font-weight: 700; color: navy;">test_size</span>=0.3)</pre>
                                    </p>
                                    <p class="lesson-text">
                                        <code>X_train</code> represents the inputs we'll be training our model on, while <code>X_test</code> 
                                        represents the inputs we'll be using for testing our model. Similarly, <code>y_test</code> and <code>y_train</code>
                                        are both the outputs we'll be using for training and testing respectively. The <code>train_test_split</code> function splits our <code>iris</code> dataset, with <code>iris['Class']</code>  being the target variable, into a training and testing set according to the ratio in the <code>test_size</code> paramater.
                                        <br><br>
                                        One of the reasons <code>train_test_split</code> is more useful than just manually splitting a Series is because it defaults to <i>shuffling</i> our data. This means instead of just taking the first 70% of your dataset as your training set, it'll shuffle your dataset first to prevent
                                        potential bias based on how the data was prepared or inputted into your database. The <code>train_test_split</code> function can also <i>stratify</i> our data. <br><br>
                                        <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);">X_train, X_test, y_train, y_test = <span class="hljs-built_in" style="font-weight: 700; color: navy;">train_test_split</span>(iris, iris[<span class="hljs-string" style="color: rgb(0, 85, 0);">'Class'</span>], <span class="hljs-built_in" style="font-weight: 700; color: navy;">stratify</span>=iris[<span class="hljs-string" style="color: rgb(0, 85, 0);">'Class'</span>], <span class="hljs-built_in" style="font-weight: 700; color: navy;">test_size</span>=0.3)</pre>
                                        
                                    </p>
                                    <p class="lesson-text">
                                        Stratifying our training sets allows the model to be trained on a subset more representative of the dataset as a whole. In our case, our target variable is 'Class'. There are four iris 'Class' categories
                                        each with a certain share of the total dataset. When a stratified training set is created, it <i>maintains</i> that share of each 'Class' when making the training set. This is especially useful in smaller datasets.
                                    </p>                         
                                    
                </div>
                <div id="Cross-Validation">
                        <!-- CROSS-VALIDATION -->
                        <h1 class="post_title"> Cross Validation</h1>
                        <p class="lesson-text">
                            Cross-Validation is a method to reduce overall bias in our model's predictions. We'll be focusong on <code>k-fold cross-validation</code>. Let's say we took 70% of our data for training and 30% for testing.
                            We would only be training our model on one specific training and testing set, this may not be the best option because our model might perform better on specific subsets of our data than other subsets. How can we 
                            get around this? One solution is to use <i>multiple</i> training and testing sets and then looking at the <i>overall</i> result across <i>all</i> sets. This makes it much easier to compare different machine learning models
                            because we'll have a better idea of how each model would perform across multiple testing sets. Luckily, this is one of the main benefits of <code>k-fold cross-validation</code>. <br><br>
                            K-fold cross validation works by dividing out dataset into k subsets, and running our learning algorithm k times, with each bin being the testing set for a different iteration. I'll give you an example.
                            Let's say our dataset had 1000 observations. We could use a k-fold cross-validation method with k=10 that would divide our dataset into k=10 bins, each of size 100. Good so far? Instead of then training our model in one iteration,
                            we'll use 9 of those bins to train our data and the last bin left over to test our data. Going back to basic combinatorics, there are only 10 ways to choose 9 training subsets and 1 testing subset from a set of
                            10 bins, this is why it runs 10 times.
                            <br><br>
                            When k=10, like in the example above, it is known as <i>10-fold cross-validation</i>. Usually we use k=10 for cross-validation, however, like when we divided our data into training and testing sets, it all depends on how much data you have. Here's a useful visualization
                            from <a href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)#/media/File:K-fold_cross_validation_EN.svg"><u>Wikipedia</u></a> that shows how a k-fold validation would split our dataset.<br><br>
                            <img  src="images/kfoldval.png" alt="K-Fold Cross-Validation" /><br><br>
                            Now let's look into how we can implement a <code>k-fold cross-validation</code>.<br><br>
                            <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-keyword" style="font-weight: 700; color: navy;">from</span> sklearn<span class="hljs-selector-class">.model_selection</span><span class="hljs-keyword" style="font-weight: 700; color: navy;"> import </span>KFold
folds = <span class="hljs-keyword" style="font-weight: 700; color: navy;">KFold</span>(n_splits=<span class="hljs-number" style="color: rgb(136, 0, 0);">10</span>)
<span class="hljs-keyword" style="font-weight: 700; color: navy;">for</span> train_index, test_index <span class="hljs-keyword" style="font-weight: 700; color: navy;">in</span> folds.split(iris):
    X_train, X_test, y_train, y_test = iris<span class="hljs-selector-class">.iloc</span>[train_index], iris<span class="hljs-selector-class">.iloc</span>[test_index], iris[<span class="hljs-string" style="color: rgb(0, 85, 0);">'Class'</span>]<span class="hljs-selector-class">.iloc</span>[train_index], iris[<span class="hljs-string" style="color: rgb(0, 85, 0);">'Class'</span>]<span class="hljs-selector-class">.iloc</span>[test_index]</pre><br><br>
                                Now we'll have our training and testing sets ready. Another common type of cross-validation is Leave-One-Out Cross Validation (LOOCV). LOOCV is a type of k-fold cross-validation where k = n, or the total number of data points in our dataset. This is when <i>each</i> of our observations will
                                individually be a testing set when training our model.
                                Obviously, as k increases, the computation time increases as well, since more training and testing will have to occur. This is why LOOCV is only recommended for small datasets. When choosing your k, it's important to keep in mind the size of your dataset
                                as well as the complexity of your model. One other reason a high k may not be advisable is due to the variance-bias tradeoff. Something we'll get into in our next lesson.
                            </p>
                            </div>
                <div id="Bias-Variance">
                        <!-- Bias-Variance -->
                        <h1 class="post_title"> Bias-Variance Tradeoff</h1>
                        <p class="lesson-text">
                            When choosing your model, it's important to keep the bias-variance tradeoff in mind. Let's look into what bias and variance are.
                            <br><br>
                            <strong>Bias</strong><br>
                            Bias is how well the average or our model's predictions compares to the actual value. One way to think of it is like throwing darts at a dartboard where the center is your expected output. 
                            If your model had low bias, the darts would be around the center - meaning we did a pretty good job <i>on average</i> in predicting our output. However, we're also interested in decreasing the <i>variability</i>
                            in those predictions. In the dart analogy, that's like making the darts we threw closer together. This brings us to <i>variance</i>.<br><br>
                            <strong>Variance</strong><br>
                            Whent training a model, if the model becomes overfitted on our training set, it may perform very well when fitting that specific set, however when testing it on
                            a different set of data, the model may not perform as well. You can think of the variation in how our model performs on these sets of data as the <i>variance</i> of our model.
                            Ideally, we want to reduce the variance in order to have more precise outputs. When throwing darts, if the darts are close together, you have low variance whereas if the darts we threw
                            are far apart, we have high variance. Here's a <a href="https://www.pinterest.com/pin/514043744937666589/?lp=true"><u>visualization</u></a> of the dart analogy.<br>
                            <img  src="images/biasvariability.png" alt="Dart Analogy" /><br><br>
                            When making out model, if we train our model to much on our training set it becomes <i>overfitted</i>. This means it will perform very well on the training set but not perform well when tested. The variation in how it performs
                            between these two sets is why we say it has <i>high variance</i>. If our model is <i>underfitted</i> that means we haven't fit our model well and we have <i>high bias</i>. Here's a figure to help out. <br><br>
                            <img  src="images/underoverfit.png" alt="Under and Overfitting" /><br><br>
                            Now that we know what bias and variance are and how they relate to our model's fit, we can look into the trade-off. When using an overly-simplistic model, we'll have high bias since our model doesn't fit our data well. To lower this bias, we can
                            try using a more complex model. However, the more complex our model is and the more it fits our training data, the more likely it is to have high variance when tested on a testing set. A popular <a id='biass' href="https://www.dataquest.io/blog/learning-curves-machine-learning/"><u>curve</u></a> you'll see that depicts this relationship is shown below. <br><br>
                            <img src="images/biascurve.png" alt="Bias-Variance Tradeoff" /><br><br>
                            Like the figure shows, the goal is to find the optimal balance between bias and variance. This may be a bit confusing since we haven't learned any models yet, but keep this in mind 
                            as you go through the series.

                            </p>
                        </div>

                <div id="Loss-Functions">
                        <!-- Loss Functions and Optimizers -->
                        <h1 class="post_title"> Loss Functions and Optimizers</h1>
                        <p class="lesson-text">
                            You might be wondering why we're learning all these concepts <i>before</i> we've even started using any models.
                            The reason is because learning how to fit various models is just one part of data analysis. The important part
                            is understanding how the models work and how to optimize and gain insight from them. So bear with me, we'll get to ML models soon. <br>
                            Let's first learn what a loss function and optimizer are, and then look into some that are commonly used. Let's start with the loss function.<br>
                            <strong id="MAE"> NOTE: We will not be going through the code required for these loss functions as it would be much easier to understand the code when implementing them into a model (we'll do this in our modeling series).</strong>
                            <br><br>
                            <strong><u>Regression Loss Functions - MAE (L1), MSE (L2) and Huber</u></strong><br><br>
                            As you remember from our train-test lesson. When we use a supervised machine learning model, we split our data into a training set and a testing set.
                            We then talked about how a model "trains" on the training set and then tests itself on the testing set. How does the model know if it guessed correctly?
                            The model determines its error using a <i>loss function</i>. Let's see some examples. <br><br>

                            <strong><u>Regression Loss Functions: MAE (L1) and MSE (L2) Loss Functions</u></strong><br><br>
                            For regression, two commonly used loss functions are Mean Squared Error (MSE) or L2 and Mean Absolute Error (MAE) or L1. MSE works by taking the average squared difference
                            between the predicted values and expected values (we kinda went over this in our PCA with SVD lesson). MAE works similarly except only looks at the average absolute value, or magnitude, of the difference between predicted
                            and expected values. One major difference between these two loss functions is that MSE tends to punish values that are far very harshly, since the
                            error is proportional to the difference squared. MAE on the otherhand does not punish outliers more since the error is proportional to the distance. We'll be revisiting these topics
                            when we get into <a id = "Huber" href="#Ridge"><u>Ridge and LASSO</u></a> regression, as they use MSE and MAE for regularization (we'll learn about that later).<br><br>
                            <strong><u>Regression Loss Functions: Huber Loss Function</u></strong><br><br>
                            <br>Another loss function is the <strong>Huber Loss Function</strong>. You can think of a Huber loss function as a sort of combination of L1 (MAE) and L2 (MSE). 
                            When using a Huber loss function, you define a &delta;. This &delta; represents a <i>tolerance</i> in terms of error in our predictions. If our error is less than this &delta;
                            we use a <i>quadratic</i> loss function. This just means our loss function has a degree of 2 - like MSE. However, like we learned earlier, MSE punishes outliers heavily since
                            loss is dependent on the error squared. Huber counters this by using a different loss function of degree 1 - or <i>linear</i>, when our error is greater than &delta;. This is called a <i>piecewise function</i>.
                            Here's the equation from <a href="https://en.wikipedia.org/wiki/Huber_loss"><u>Wikipedia</u></a>, where <code>f(x)</code> represents our expected value.<br><br>
                            <img src="images/huber.png"><br><br>
                            This concept is similar to how Elastic Net regularization builds on L1 and L2 regularization, something we'll get into in our Regression lesson series.<br>
                            Here are some popular regression loss functions:<br><br>
                            <img  id="Log" src="images/lossfunctions.png"><br><br><br>
                            <strong><u>Classification Loss Functions: Binary Cross-Entropy (Logarithmic)</u></strong><br><br>
                            When working with classification data, we have to use different loss functions that work with our "classes".
                            We'll be working with other classification loss functions like <strong>Hinge Loss</strong> later on. We'll skip it for now because we'll cover it once we learn SVMs.<br><br>
                            When we have a binary classification problem, our goal is to create a model that 
                            takes in our features and outputs either 0 or 1, representing our two classes. We'll learn more about these in our classification section, but it's a good idea to understand the basic premise.
                            When we feed our features into our model, let's say we use a logistic regression model, we can get an output between 0 and 1 for each observation that corresponds to the class. For example, when feeding in our features for a specific 
                            observation, our model may output 0.94. If our expected value was 1, we weren't too far off, but if our expected value was 0, we have some work to do. Binary Cross-Entropy is useful because it treats these two cases differently ("binary" because there are two classes).
                            When the error is large, like if our expected out was 0 in our previous example, binary cross-entropy would penalize these predictions <i>heavily</i>. To better illustrate this, take a look at the log loss function
                            <a href="https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html"><u>graph</u></a>. when the true value is 1. The x-axis represents our model's predictions, and the y-axis represents the corresponding log error associated with that prediction. <br><br>
                            <img src="images/crossentropy.png"><br><br>
                            
                            As you can see, the log loss function penalizes predictions that are way off <i>much more</i> than predictions that are slightly off. Intuitively, this should make sense. Let's take a look at the <strong>cost function</strong>. <br><br>
                            <img src="images/binaryeqn.png"><br><br>
                            An important distinction to make is the difference between a <i>loss</i> function and a <i>cost</i> function. A <strong>loss function</strong> is the the error we calculate for a <i>single observation</i>, whereas
                            a <strong>cost function</strong> is the <i>average</i> of our loss function across our entire dataset. Knowing that the average is just the sum divided by the total number of observations, we can see that the loss function for cross-entropy
                            is everything inside the summation. Let's make sense of what each variable represents. y<sub>i</sub> represents the observation's target class, either 0 or 1. The p(y<sub>i</sub>) represents our model's probability <i>prediction</i> of
                            that observation. Remember that all our probabilities are between 0 and 1 and the logarithm of a value between 0 and 1 goes from 0 to negative infinity. In practice, it's common to maximize the negative log rather minimize the log loss function.
                            Minimizing the log of a function is the same as maximizing the negative log of that function. <br><br>
                            One thing you'll notice aboutis our coefficients are y<sub>i</sub> and (1-y<sub>i</sub>), and as we mentioned earlier, y<sub>i</sub> can only have values 0 or 1. This means that only one of these terms is used when calculating the cross-entropy loss for an
                            observation. Why is that so? Imagine we have an observation whose target is 0. Our error would simply be our model's output (y<sub>i</sub>) minus zero, or, y<sub>i</sub>. If our target was 1, our error would be 1 minus our model's output (y<sub>i</sub>), or
                            , (1-y<sub>i</sub>). The cross-entropy loss function will prove extremely useful when we start using neural networks and other classification models. <br><br>
                            
                            Now that we know what a loss function is, we can look into how to <i>minimize</i> it. We can minimize our loss function by using
                            certain optimizers.
                            </p>

                            </div>
                <div id="Optimizers">
                        <!-- Optimizers -->
                        <h1 class="post_title">Optimizers</h1>
                        <p class="lesson-text">
                            Like our last lesson, this lesson will focus on the conceptual and mathematical understanding of certain optimizers rather than
                            their programatic implementation. We'll be implementing these optimizers throughout our machine learning models series.<br><br>
                            Optimization functions are commonly used in conjunction with loss functions. If you remember from our loss functions lesson, a loss function represents the error
                            of a model's prediction. Obviously, when we fit our model to our training set, we're interested in trying to minimize our error. This is where optimizers come in.
                            Optimizers are functions whose primary purpose is to minimize or maximize another function - in our case, a <i id="GD">loss function.</i> <br>
                            <br><br>
                            <strong><u>(Vanilla) Gradient Descent (GD)</u></strong><br><br>
                            An optimization algorithm you'll definitely hear about and use is gradient descent. To understand gradient descent, we'll go through
                            how it works. Let's say we're trying to use gradient descent for the linear model: <br><br>
                            <a href="https://www.codecogs.com/eqnedit.php?latex=y&space;=&space;\Theta&space;_{0}&space;&plus;&space;\Theta&space;_1x" target="_blank"><img src="https://latex.codecogs.com/png.latex?y_{i}&space;=&space;\Theta&space;_{0}&space;&plus;&space;\Theta&space;_1x_{i}" title="y = \Theta _{0} + \Theta _1x" /></a><br><br>
                            Our loss function (RSS) is:<br><br>
                            <a href="https://www.codecogs.com/eqnedit.php?latex=RSS&space;=&space;\sum_{i=1}^{n}&space;(\widehat{y}_{i}-y_{i})^2" target="_blank"><img src="https://latex.codecogs.com/png.latex?RSS&space;=&space;\sum_{i=1}^{n}&space;(\widehat{y}_{i}-y_{i})^2" title="RSS = \sum_{i=1}^{n} (\widehat{y}_{i}-y_{i})^2" /></a><br><br>
                            Let's imagine that we substituted two random values for &theta;<sub>0</sub> and &theta;<sub>1</sub> into our linear model 
                            and then calculated our RSS.
                            We could graph the point (&theta;<sub>0</sub>, &theta;<sub>1</sub>, RSS) on a 3-D plot. Here's an example of what that plot might look like:<br><br>
                            <img src="images/gd.png"><br><br>
                            You can imagine the plane the "bowl" sits on as our (&theta;<sub>0</sub>, &theta;<sub>1</sub>) plane, making out third axis (the one coming up) 
                            our RSS axis. Remember that our goal is to minimize our loss function, graphically, that means we want to find the "bottom" of our bowl. This is called a minima.
                            The two types of minima are local and absolute. A local minimum is a minimum relative to surrounding points whereas
                            an absolute minimum is the minimum for the entire plot. You can think of it like your local star basketball player, a <i>local</i> legend, and Kobe Bryant,
                            an <i>absolute</i> legend (RIP Kobe). In the diagram above, our local minimum is our absolute minimum.<br><br>
                            Finding a minimum means 
                            we have to find where our plot bottoms out, or in other words, where the slope and deriviative <i>are zero</i>. We can take our RSS loss function
                            and take the partial derivative with respect to each of our &theta;'s to see how the deviations
                            in any specific parameter affect the RSS keeping while <i>all else constant</i>. These expressions are the rates of change of our RSS function with respect to our paramaters,
                            in this case &theta;<sub>0</sub> and &theta;<sub>1</sub>. Remember that a derivative of a summation
                            is simply the derivative of each term added together, and when taking the partial with respect to &theta;<sub>0 or 1</sub>, the other &theta; can be treated as a constant. <br>
                            Okay, so now what?
                            <br><br>
                            We'll start off by giving arbitrary values for &theta;<sub>0</sub> and &theta;<sub>1</sub>. You can imagine our gradient descent algorithm plotting that point
                            on a graph (&theta;<sub>0</sub>, &theta;<sub>1</sub>, RSS) and then jumping to another &theta;<sub>0</sub>, &theta;<sub>1</sub>, calculating the model's RSS with 
                            the new paramaters, and plotting that point before repeating the process. Our gradient descent algorithm tries to take steps or jumps towards the minimum
                            by iteratively adjusting &theta;<sub>0</sub> and &theta;<sub>1</sub> until the step sizes are as close to 0 as possible, or a certain step limit 
                            has been reached. 
                            It might be hard to visualize the three-dimensional plot so here's a helpful labeled diagram of gradient descent in two-dimensions:<br><br>
                            
                            <img src="images/gdjumps.png">
                            <br><br>
                            Here is how we will mathematically update our weights.<br><br>
                            <a href="https://www.codecogs.com/eqnedit.php?latex=\Theta&space;_{1}'&space;=\Theta&space;_{1}-\alpha&space;\frac{\partial&space;}{\partial&space;\Theta&space;_{1}}RSS(\Theta&space;_{1},&space;\Theta&space;_{2})" target="_blank"><img src="https://latex.codecogs.com/png.latex?\Theta&space;_{0}'&space;=\Theta&space;_{0}-\alpha&space;\frac{\partial&space;}{\partial&space;\Theta&space;_{0}}RSS(\Theta&space;_{0},&space;\Theta&space;_{1})" title="\Theta _{1}' =\Theta _{1}-\alpha \frac{\partial }{\partial \Theta _{1}}RSS(\Theta _{1}, \Theta _{2})" /></a><br>
                            <a href="https://www.codecogs.com/eqnedit.php?latex=\Theta&space;_{1}'&space;=\Theta&space;_{1}-\alpha&space;\frac{\partial&space;}{\partial&space;\Theta&space;_{1}}RSS(\Theta&space;_{1},&space;\Theta&space;_{2})" target="_blank"><img src="https://latex.codecogs.com/png.latex?\Theta&space;_{1}'&space;=\Theta&space;_{1}-\alpha&space;\frac{\partial&space;}{\partial&space;\Theta&space;_{1}}RSS(\Theta&space;_{0},&space;\Theta&space;_{1})" title="\Theta _{1}' =\Theta _{1}-\alpha \frac{\partial }{\partial \Theta _{1}}RSS(\Theta _{1}, \Theta _{2})" /></a>
                            <br><br>The equation pretty much says that our paramater's updated value should be its previous value <i>minus</i> how much a change in our parameter would affect our RSS, scaled by our learning rate &alpha;.
                            &theta;' represents our updated value and &alpha; represents our <i>learning rate</i>. Our learning rate decides how meticulous our gradient descent algorithm should be in terms of step size.
                            If our learning rate is too small, it will take a long time for our algorithm to converge, if our rate is too large, we may overshoot and end with infinite error. This is why determining the learning rate is vital. 
                            Trying out different rates and using a fixed value is a good way to start. Another common practice is to start with a larger learning rate and then decrease it as your model approaches its optimal solution. 
                            This is helpful because it allows our model to work quickly in the beginning with larger steps while being more precise around the minima.<br><br>
                            Gradient descent is very useful but has some drawbacks. One major drawback is working with Big Data. When we have a lot of features and a lot of observations, our gradient descent algorithm will have to calculate the partial deriviatves
                            for <i>each parameter</i> in <i id = "SGD">each observation</i>, this can be very time-costly. Luckily, SGD helps with this problem.
                            
                            
                            
                            <br><br>
                            <strong><u>Stochastic Gradient Descent (SGD) and Mini Batch Gradient Descent</u></strong><br><br>
                            Stochastic Gradient Descent (SGD), is a variation of the <a href="#GD"><u>Gradient Descent</u></a> algorithm. In our gradient descent algorithm, we
                            used <i>all</i> our observations when finding our errors before changing our weights. With a lot of data, this becomes too time-costly. 
                            SGD randomly chooses a <strong>single</strong> sample in each iteration to calculate our error and partial derivatives. This makes our computation n times faster, n being our
                            dataset size.<br><br>
                            SGD minimizes our loss much faster but has a lot more noise and can oscillate around the minimum. If we wanted to get the best of both worlds, we could use
                            <strong>Mini Batch Gradient Descent</strong>. Our vanilla gradient descent algorithm took our entire dataset as a "batch," while SGD took one sample as a "batch". 
                            Mini Batch Gradient Descent takes a <i>subset</i> of our dataset as a batch in each iteration to adjust our model's weights. This improves the computational complexity of vanilla gradient descent
                            while <i id="Adam">decreasing</i> the variance from stochastic gradient descent.
                            <br><br>
                            <strong><u>Adaptive Moment Estimation (Adam)</u></strong><br><br>
                            Now we'll learn about the <strong>Adam</strong> optimization algorithm. The Adam algorithm is a combination of two other algorithms
                            called AdaGrad and RMSProp, both of which we'll be going over. 
                            We'll start with the <strong>Adaptive Gradient Algorithm (AdaGrad)</strong>. To understand AdaGrad, let's revisit our gradient descent update function. This time, instead of writing "RSS" we'll use "L" (Loss).<br><br>
                            <a href="https://www.codecogs.com/eqnedit.php?latex=\Theta&space;_{j}'&space;=\Theta&space;_{j}-\alpha&space;\frac{\partial&space;L&space;}{\partial&space;\Theta&space;_{j}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Theta&space;_{j}'&space;=\Theta&space;_{j}-\alpha&space;\frac{\partial&space;L&space;}{\partial&space;\Theta&space;_{j}}" title="\Theta _{j}' =\Theta _{j}-\alpha \frac{\partial L }{\partial \Theta _{j}}" /></a><br>
                            <br>This can be rewritten as: <br><br>
                            <a href="https://www.codecogs.com/eqnedit.php?latex=\Delta&space;\Theta&space;_{j}&space;=-\alpha&space;\frac{\partial&space;L&space;}{\partial&space;\Theta&space;_{j}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Delta&space;\Theta&space;_{j}&space;=-\alpha&space;\frac{\partial&space;L&space;}{\partial&space;\Theta&space;_{j}}" title="\Delta \Theta _{j} =-\alpha \frac{\partial L }{\partial \Theta _{j}}" /></a><br><br>
                            You'll notice that our &alpha; is constant and doesn't depend on our parameter estimates. AdaGrad improves on gradient descent by <i>decreasing</i> the learning rate as
                            the model gets closer to the solution. It does this by taking the &alpha; from the equation above and dividing it by an expression every epoch to yield: <br><br>
                            <a href="https://www.codecogs.com/eqnedit.php?latex=\Delta&space;\Theta&space;_{j}&space;=-\frac{\alpha}{\sqrt{G_{j}}&plus;\epsilon&space;}&space;\cdot&space;\frac{\partial&space;L&space;}{\partial&space;\Theta&space;_{j}}" target="_blank"><img src="https://latex.codecogs.com/png.latex?\Delta&space;\Theta&space;_{j}&space;=-\frac{\alpha}{\sqrt{G_{j}}&plus;\epsilon&space;}&space;\cdot&space;\frac{\partial&space;L&space;}{\partial&space;\Theta&space;_{j}}" title="\Delta \Theta _{j} =-\frac{\alpha}{\sqrt{G_{j}}+\epsilon } \cdot \frac{\partial L }{\partial \Theta _{j}}" /></a><br><br>
                            The only difference between this and our previous equation is the denominator of the square root of G<sub>j</sub> plus some epsilon. The epsilon isn't very important as its
                            main function is to prevent division by 0. One thing you should take note of is that each parameter will have its own adaptive learning rate, which is very useful
                            in minimizing our overall time-cost. So anyway, what is G<sub>j</sub>? You can think of G<sub>j</sub> as our scaling factor for our learning rates. The larger our value for G<sub>j</sub>,
                            the smaller our learning rate and vice versa. G<sub>j</sub> is calculated as follows: <br><br>
                            <a href="https://www.codecogs.com/eqnedit.php?latex=G_{j}'&space;=G_{j}&space;&plus;&space;(\frac{\partial&space;L&space;}{\partial&space;\Theta&space;_{j}})^2" target="_blank"><img src="https://latex.codecogs.com/png.latex?G_{j}'&space;=G_{j}&space;&plus;&space;(\frac{\partial&space;L&space;}{\partial&space;\Theta&space;_{j}})^2" title="G_{j}' =G_{j} + (\frac{\partial L }{\partial \Theta _{j}})^2" /></a><br><br>
                            When we start, our G<sub>j</sub> is 0 so our &alpha;'s denominator is simply &epsilon;. There are some important things to notice. The first thing is that since G<sub>j</sub> starts at 0 and a squared value (positive) is being
                            added, our G<sub>j</sub> value will increase every epoch as more and more positive terms are added. This means our denominator for our &alpha; will continually decrease and after a certain point, our learning rate will be so small our model
                            will barely change our parameters. <br><br>
                            Now you can see why these are called <i>adaptive</i> algorithms. Our learning rates <i>adapt</i> according to our dataset.
                             

                            </p>
                        </div>
                            
                <div id="Linear-Regression">
                        <!-- Linear-Regression -->
                        <h1 class="post_title"> Linear Regression</h1>
                        <p class="lesson-text">
                            The first model we'll look into is linear regression. Linear regression works by trying to find the <i>line of best fit</i>. Or, the line that minimizes the error between
                            our predictions and what the values actually are. Here's an animation to help out.<br><br>
                            <img  src="images/linreg.gif" alt="Linear Regression Animation" /><br><br>
                            The best way to understand linear regression is to understand what a linear regression algorithm is trying to do. A linear equation takes the form: <br><br>
                            <a href="https://www.codecogs.com/eqnedit.php?latex=y=a_{0}&space;&plus;&space;a_{1}x_{1}&space;&plus;&space;a_{2}x_{2}&space;...&space;&plus;&space;a_{n}x_{n}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y=a_{0}&space;&plus;&space;a_{1}x_{1}&space;&plus;&space;a_{2}x_{2}&space;...&space;&plus;&space;a_{n}x_{n}" title="y=a_{0} + a_{1}x_{1} + a_{2}x_{2} ... + a_{n}x_{n}" /></a><br><br>
                            Where a<sub>0</sub> represents the y-intercept, or when our linear equation crosses the y-axis. This is usually refered to as the <i>constant</i> or <i>shift</i>. The coefficients
                            a<sub>1,2...,n</sub> represent the weights given to each of our features (independent variables) x<sub>1,2,...,n</sub>. The larger the weight, the more important
                            that feature is in predicting our output. Our overall goal is to minimize the <i>cost function</i>, how we go about optimizing our model, whether through OLS or gradient descent, will be covered
                            in the next lesson. Our cost function looks something like this: <br><br>
                            <a href="https://www.codecogs.com/eqnedit.php?latex=J&space;=&space;\frac{1}{n}\sum_{i=1}^{n}(p_{i}&space;-&space;y_{i})^{2}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?J&space;=&space;\frac{1}{n}\sum_{i=1}^{n}(p_{i}&space;-&space;y_{i})^{2}" title="J = \frac{1}{n}\sum_{i=1}^{n}(p_{i} - y_{i})^{2}" /></a><br><br>
                            This equation is what we call the <strong>Mean Squared Error (MSE)</strong>. <code>J</code> represents the cost, what we're trying to minimize. While <code>p<sub>i</sub></code> is our prediction and <code>y<sub>i</sub></code> is the actual value. What we're doing is looking at the
                            difference in what our model predicted and what the actual output was, squaring it, and averaging it across our <code>n</code> observations. No wonder it's called Mean Squared Error! Minimzing our cost function (<code>J</code>), is how we determine the coefficients
                            a<sub>0</sub>, a<sub>1</sub>, ..., a<sub>n</sub>, in our final model. Like I said earlier, we'll be going over how to minimize the cost-function in our next lesson.<br><br>
                            Now let's look into how we can implement a linear regression model. The dataset we'll be using is based on data collected from the Capital Bikeshare Service, a bikesharing service in the Metro DC area (download <a href="supplements/bikeshare.csv" download><u>here</u></a> data from Kaggle). Our goal will be to build a regression
                            model that relates the features we choose to the total bikes being used on a given day. We'll start by importing our libraries and loading/preparing our training and testing sets. <br><br>
                            <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);"><span class="hljs-keyword" style="font-weight: 700; color: navy;">import</span> pandas <span class="hljs-keyword" style="font-weight: 700; color: navy;">as</span> pd
from sklearn.model_selection <span class="hljs-keyword" style="font-weight: 700; color: navy;">import</span> train_test_split
from sklearn.linear_model <span class="hljs-keyword" style="font-weight: 700; color: navy;">import</span> LinearRegression 
from sklearn.metrics <span class="hljs-keyword" style="font-weight: 700; color: navy;">import</span> r2_score
                                
bikedata = pd.read_csv(<span class="hljs-string" style="color: rgb(0, 85, 0);">'bikeshare.csv'</span>)[[<span class="hljs-string" style="color: rgb(0, 85, 0);">'workingday'</span>,<span class="hljs-string" style="color: rgb(0, 85, 0);">'weathersit'</span>, <span class="hljs-string" style="color: rgb(0, 85, 0);">'temp'</span>, <span class="hljs-string" style="color: rgb(0, 85, 0);">'atemp'</span>, <span class="hljs-string" style="color: rgb(0, 85, 0);">'hum'</span>, <span class="hljs-string" style="color: rgb(0, 85, 0);">'windspeed'</span>, <span class="hljs-string" style="color: rgb(0, 85, 0);">'cnt'</span>]]
X_train, X_test, y_train, y_test = train_test_split(bikedata.iloc[:,:-<span class="hljs-number" style="color: rgb(136, 0, 0);">1</span>], bikedata[<span class="hljs-string" style="color: rgb(0, 85, 0);">'cnt'</span>], test_size=<span class="hljs-number" style="color: rgb(136, 0, 0);">0.3</span>)</pre>
                        </p>
                        <p class="lesson-text">
                            Instead of choosing all attributes, we've decided to only look at a few (this is an arbitrary decision, to know how to deal with features, I recommend looking at the
                            Feature Engineering lesson). We split out data
                            into a training and testing set. If you'd like to use cross-validation, you can look into <a href="#Cross-Validation"><u>this lesson</u></a>.
                            Next, we'll fit our linear regression model, look at how it performs, and develop our linear equation. <br><br>
                            <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);">lr = LinearRegression()
lr.fit(X_train,y_train)
y_pred = lr.predict(X_test)
                                    
<span class="hljs-function"><span class="hljs-title" style="font-weight: 700; color: navy;">print</span><span class="hljs-params">(<span class="hljs-string" style="color: rgb(0, 85, 0);">"R-Squared: "</span>, r2_score(y_test, y_pred)</span></span>)
<span class="hljs-function"><span class="hljs-title" style="font-weight: 700; color: navy;">print</span><span class="hljs-params">(<span class="hljs-string" style="color: rgb(0, 85, 0);">"Weights: "</span>,lr.coef_)</span></span>
<span class="hljs-function"><span class="hljs-title" style="font-weight: 700; color: navy;">print</span><span class="hljs-params">(<span class="hljs-string" style="color: rgb(0, 85, 0);">"Constant: "</span>,lr.intercept_)</span></span></pre><br>
                            <img  src="images/rsquared.png" alt="R-Squared" />
                        </p>
                        <p class="lesson-text">
                            The first line shows our R-Squared value. An R-Squared value is a correlation coefficient that tells us how well our model
                            performed when predicting during the testing phase. When looking at the error of our model, we can call the regression error 
                            the difference between our prediction and our expected output, and call total error the difference between our prediction and the average output.
                            When you subtract the sum of your regression error squared divided by the sum of your total error squared from 1, you arrive at your R-Squared value. Our next outputs
                            are the coefficients and intercepts of our model. Remember a linear model takes the form: <br>
                            <a href="https://www.codecogs.com/eqnedit.php?latex=y=a_{0}&space;&plus;&space;a_{1}x_{1}&space;&plus;&space;a_{2}x_{2}&space;...&space;&plus;&space;a_{n}x_{n}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y=a_{0}&space;&plus;&space;a_{1}x_{1}&space;&plus;&space;a_{2}x_{2}&space;...&space;&plus;&space;a_{n}x_{n}" title="y=a_{0} + a_{1}x_{1} + a_{2}x_{2} ... + a_{n}x_{n}" /></a><br>
                            The coefficients outputted correspond to the weights for each of our features. One thing you'll notice is that certain weights are higher than others, for instance, 'weathersit', 'temp', 'atemp', 'windspeed' seem to have the highest weights which means our model
                            predicts they have model the target variable better, or in other words have more "sway", in the outcome. If we were to run the regression with just these four variables,
                            our R-Squared wouldn't be too much different - it may even be worth it. This idea is the basis of Ridge, LASSO, and Elastic Net regression, models we'll get into
                            in the next lesson. 
                            </p>


                        </div>


                <div id="Ridge">
                        <!-- Ridge, LASSO, Elastic Net -->
                        <h1 class="post_title"> Ridge, LASSO, and Elastic Net</h1>
                        <p class="lesson-text">
                            <strong>Ridge Regression</strong><br><br>
                            When using linear regression, especially with small datasets, the chance of overfitting your model is high. This is because although there is low bias, with a smaller training set,
                            the variability in how your model performs on testing sets will increase. In order to counter overfitting, we can use various regularization techniques used by Ridge and LASSO regression.
                            They pretty much work by introducing some bias in the model in order to lower the variability of how the model performs on test sets. This is especially useful when the coefficients
                            in your linear regression model are far apart. This is where Ridge Regression comes in. <br><br>
                            Ridge Regression uses something called <strong>L2 Regularization</strong>. Regularization is used to reduce overfitting. 
                            L2 Regularization reduces model complexity and helps bring the weights in our model closer to zero, in essence, decreasing variance and shifting us more left on the bias-variance curve.
                            If you're confused, take a look <a href="#biass"><u>here</u></a>. Linear Regression is prone to overfitting and high variance, what we can do to counter that is to introduce bias.
                            In return for adding bias, our model has decreased variance. <br><br>
                            A traditional linear regression model minimizes the sum of squared residuals, whereas in ridge regression, the sum of squared residuals <i>plus</i> a &lambda; times the sum of the coefficients squared is minimized. You can think of this as a penalty term. <br>
                            <img  src="images/ridgecost.png" alt="Ridge Cost Function" />
                            <br>
                            <br> If &lambda;=0, our ridge cost function equals our linear regression cost function. Let's implement a ridge regression model. First make sure you add <code>from sklearn.linear_model import Ridge</code> to get our library ready.<br><br>
                            <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);">rr = Ridge(alpha=<span class="hljs-number" style="color: rgb(136, 0, 0);">100</span>) 
rr.fit(X_train,y_train)
y_pred = rr.predict(X_test)
                                
<span class="hljs-function"><span class="hljs-title" style="font-weight: 700; color: navy;">print</span><span class="hljs-params">(<span class="hljs-string" style="color: rgb(0, 85, 0);">"R-Squared: "</span>, r2_score(y_test, y_pred)</span></span>)
<span class="hljs-function"><span class="hljs-title" style="font-weight: 700; color: navy;">print</span><span class="hljs-params">(<span class="hljs-string" style="color: rgb(0, 85, 0);">"Weights: "</span>,lr.coef_)</span></span>
<span class="hljs-function"><span class="hljs-title" style="font-weight: 700; color: navy;">print</span><span class="hljs-params">(<span class="hljs-string" style="color: rgb(0, 85, 0);">"Constant: "</span>,lr.intercept_)</span></span></pre><br><br>
                               
                                <img  src="images/alpha5.png" alt="alpha=100" /><br>
                                With an alpha of 5 we get:<br>
                                <img  src="images/alpha100.png" alt="alpha=5" />
                                </p>
                                <p class="lesson-text">
                                Let's first look at our alpha=100 model. The larger our alpha parameter is, the more bias we introduce and the higher our "penalty term" gets. You can think of alpha as a parameter that represents how much we want to <i>scale</i> our model's coefficients down.
                                A larger alpha helps reduce overfitting. 
                                In this case, an alpha of 100 is too large as it <i>underfits</i> our model. In trying to lower variance, we increased bias too much and are now on the opposite end of the bias-variance curve.
                                Using an alpha of 5, as you can see, yields a much higher correlation. We can see this because as we start with an alpha of 1 (linear regression) and increase it, we'll notice that after a certain point
                                our model performance starts diminishing.  <br>
                                The main benefit of this added bias is a decrease in variance. Our coefficients are much more evenly distributed which helps give our model
                                more stable results. This concept is L2 regularization. With an alpha of 5 our weights were somewhat more evenly distributed and this phenomenon is exaggerated
                                even more with an alpha of 100. Here's a visualiation provided by <a href="https://scikit-learn.org/stable/auto_examples/linear_model/plot_ridge_path.html"><u>scikit-learn.org</u></a>, showing how
                                as alpha values increase, weights for features tend towards zero.<br>
                                <img src="images/ridgealphas.png"><br><br>
                                Here are the coefficients from our model between the linear regression and two ridge regression models we fitted. This is another depiction of how L2 regularization moves weights closer to 0.
                                <br>
                                <img src="images/coefplot.png">
                                </p>

                                <p class="lesson-text">
                                <strong>LASSO Regression</strong><br><br>
                                    In this lecture we'll delve into LASSO (Least Absolute Shrinkage and Selection Operator) regression. In ridge regression, we improved on linear regression by adding a penalty term to help make our coefficients more stable.
                                    Ridge regression utilized L2 regularization while LASSO regression uses L1 regularization. LASSO regression adds <i>feature selection</i> in the mix,
                                    which means it can make certain coefficients or weights zero, thereby removing them and <i>selecting</i> the others. To see how this works, let's look at the LASSO cost function.
                                    <br>
                                    <img src="images/lassocost.png"><br><br>
                                    The difference in the penalty terms in ridge and LASSO regression shows how L1 and L2 regularization differ. Like ridge regression, a &lambda; value of 0 yields
                                    the linear regression cost function. By only looking at the magnitude of coefficients rather than the square, LASSO regression permits certain weights to reach 0 whereas in ridge regression,
                                    weights can only <i>approach</i> 0. This type of regularization is extremely useful when we have a lot of features in our data. L1 regularization uses feature selection to help reduce
                                    model complexity. <br><br>
                                    Let's quickly implement LASSO regression model. First, <code>from sklearn.linear_model import Lasso</code>.<br><br>
                                    <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);">lasso = Lasso() 
lasso.fit(X_train, y_train)
y_pred = lasso.predict(X_test)
                                            
<span class="hljs-function"><span class="hljs-title" style="font-weight: 700; color: navy;">print</span><span class="hljs-params">(<span class="hljs-string" style="color: rgb(0, 85, 0);">"R-Squared: "</span>, r2_score(y_test, y_pred)</span></span>)
<span class="hljs-function"><span class="hljs-title" style="font-weight: 700; color: navy;">print</span><span class="hljs-params">(<span class="hljs-string" style="color: rgb(0, 85, 0);">"Weights: "</span>,lasso.coef_)</span></span>
<span class="hljs-function"><span class="hljs-title" style="font-weight: 700; color: navy;">print</span><span class="hljs-params">(<span class="hljs-string" style="color: rgb(0, 85, 0);">"Constant: "</span>,lasso.intercept_)</span></span></pre>
                                <br> <img src="images/lassoresults.png">                                
</p>

                                <p class="lesson-text">
                                As you can see, LASSO regression gave a weight of 0 to our third feature, an example of feature selection.
                                    
                                
                            <br>
                            Using linear models on this dataset have clearly not been very effective. When using a linear model, ensure your data is <i>linearly seperable</i>, you can test this
                            by simply fitting a linear model and looking into performance. If your model has high error, don't worry! There are more regression models, some of which we'll be going into in future lessons.
                            Here's a helpful chart that shows
                            how linear, ridge, and LASSO regression compare. Before we move on to SVR, let's look into Elastic Net regularization, a combination of L1 and L2 regularization<br><br>
                            <img  src="images/ridgelassocompare.png" alt="Compared"/>
                            
                        </p>
                        <p class="lesson-text">
                        <strong>Elastic Net</strong><br><br>
                        In this lesson we've looked into linear, ridge, and LASSO regression and have learned how ridge and LASSO add bias into the linear regression model.
                        They do this by including a penalty term. The sum of the weights squared penalty term for ridge regression is L2 regularization while the sum of the magnitudes
                        of weights penalty term for LASSO regression is L1 regularization. Elastic Net combines these. If you remember our cost functions from the Ridge and LASSO, you'll
                        remember each one had a different penalty term boxed. The Elastic Net cost function is the linear regression cost function plus &alpha; x Ridge Penalty + (1-&alpha;) x LASSO Penalty.
                        In other words, it's a combination of L1 and L2. You can think of the &alpha; value as a ratio between L1 and L2 regularization.<br><br>
                        Let's implement Elastic Net regularization. First, <code>from sklearn.linear_model import ElasticNet</code>.<br><br>
                        <pre class="hljs" style="display: block; overflow-x: auto; padding: 0.5em; background-color: rgb(244, 244, 244); color: rgb(0, 0, 0);">elastic = ElasticNet(alpha=<span class="hljs-number" style="color: rgb(136, 0, 0);">0.1</span>) 
elastic.fit(X_train, y_train)
y_pred = elastic.predict(X_test)
                            
<span class="hljs-function"><span class="hljs-title" style="font-weight: 700; color: navy;">print</span><span class="hljs-params">(<span class="hljs-string" style="color: rgb(0, 85, 0);">"R-Squared: "</span>, r2_score(y_test, y_pred)</span></span>)
<span class="hljs-function"><span class="hljs-title" style="font-weight: 700; color: navy;">print</span><span class="hljs-params">(<span class="hljs-string" style="color: rgb(0, 85, 0);">"Weights: "</span>,elastic.coef_)</span></span>
<span class="hljs-function"><span class="hljs-title" style="font-weight: 700; color: navy;">print</span><span class="hljs-params">(<span class="hljs-string" style="color: rgb(0, 85, 0);">"Constant: "</span>,elastic.intercept_)</span></span></pre><br>
                            <img src="images/elasticresults.png">

                        
                    </p>
                    <p class="lesson-text">
                        The closer your &alpha; value is to 1, the more L1 regularization will be used and the closer your &alpha; value is to 0, the more L2 regularization will be used.

                    </p>

                <div id="Model-Resources">

                        <!-- Useful-Resources -->
                        <h1 class="post_title"> Useful Resources</h1>
                        <p class="lesson-text">
                            In this series, we covered various models used for analyzing our data. We looked into how the models worked, as well as how to implement each model. If you're looking for datasets to practice with,
                            the <code>sklearn</code> library has <a href="https://scikit-learn.org/stable/datasets/index.html"><u>datasets</u></a> built-in to get started. In addition, <a href="https://www.kaggle.com/datasets"><u>Kaggle</u></a> and
                            the <a href="https://archive.ics.uci.edu/ml/datasets.php"><u>UC Irving Machine Learning Repository</u></a> have datasets as well. On Reddit, <a href="https://www.reddit.com/r/datasets/"><u>r/datasets</u></a> is a good source too. 
                            Here are some guides and documentation relevant to what we covered in this series. <br>
                            <ul class = "bullets">
                                    <li>  Documentations: <a href="https://scikit-learn.org/stable/"><u>sklearn</u></a> | <a href="https://pypi.org/project/stats/"><u>stats</u></a> | <a href="https://www.statsmodels.org/dev/tsa.html"><u>statsmodels.tsa</u></a> </li>

                                    <li>  Here's an  
                                        <a href="https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Scikit_Learn_Cheat_Sheet_Python.pdf"><u>sklearn Cheatsheet</u></a>.</li>
                
                                </ul>
                                <br>
                            <a href="lessons.html"><u>Back to Top</u></a>
                            </p>

                        </div>


<!------------------------------------------------------------------------------------------------------------ END -------------------------------------------------------------------------------------------------------------------------------->






                    <br><br>
            </div>
            <footer>
                <div class="footer content-1170 center-relative">
                </div>
            </footer>
   
    
            <!--Load JavaScript-->
            <script type="text/javascript" src="js/jquery.js"></script>
            <script type='text/javascript' src='js/jquery.sticky-kit.min.js'></script>
            <script type='text/javascript' src='js/jquery.smartmenus.min.js'></script>
            <script type='text/javascript' src='js/jquery.sticky.js'></script>
            <script type='text/javascript' src='js/jquery.dryMenu.js'></script>
            <script type='text/javascript' src='js/isotope.pkgd.js'></script>
            <script type='text/javascript' src='js/jquery.carouFredSel-6.2.0-packed.js'></script>
            <script type='text/javascript' src='js/jquery.mousewheel.min.js'></script>
            <script type='text/javascript' src='js/jquery.touchSwipe.min.js'></script>
            <script type='text/javascript' src='js/jquery.easing.1.3.js'></script>
            <script type='text/javascript' src='js/imagesloaded.pkgd.js'></script>
            <script type='text/javascript' src='js/jquery.prettyPhoto.js'></script>
            <script type='text/javascript' src='js/main.js'></script>
            <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
            <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.0/prism.min.js"></script>
            
        </body>
</html>